/* automatically generated by rust-bindgen 0.69.4 */

pub const __BIONIC__: u32 = 1;
pub const __WORDSIZE: u32 = 64;
pub const __bos_level: u32 = 0;
pub const __ANDROID_API_FUTURE__: u32 = 10000;
pub const __ANDROID_API__: u32 = 10000;
pub const __ANDROID_API_G__: u32 = 9;
pub const __ANDROID_API_I__: u32 = 14;
pub const __ANDROID_API_J__: u32 = 16;
pub const __ANDROID_API_J_MR1__: u32 = 17;
pub const __ANDROID_API_J_MR2__: u32 = 18;
pub const __ANDROID_API_K__: u32 = 19;
pub const __ANDROID_API_L__: u32 = 21;
pub const __ANDROID_API_L_MR1__: u32 = 22;
pub const __ANDROID_API_M__: u32 = 23;
pub const __ANDROID_API_N__: u32 = 24;
pub const __ANDROID_API_N_MR1__: u32 = 25;
pub const __ANDROID_API_O__: u32 = 26;
pub const __ANDROID_API_O_MR1__: u32 = 27;
pub const __ANDROID_API_P__: u32 = 28;
pub const __ANDROID_API_Q__: u32 = 29;
pub const __ANDROID_API_R__: u32 = 30;
pub const __ANDROID_API_S__: u32 = 31;
pub const __ANDROID_API_T__: u32 = 33;
pub const __ANDROID_API_U__: u32 = 34;
pub const __ANDROID_API_V__: u32 = 35;
pub const __ANDROID_NDK__: u32 = 1;
pub const __NDK_MAJOR__: u32 = 26;
pub const __NDK_MINOR__: u32 = 0;
pub const __NDK_BETA__: u32 = 0;
pub const __NDK_BUILD__: u32 = 10792818;
pub const __NDK_CANARY__: u32 = 0;
pub const INT8_MIN: i32 = -128;
pub const INT8_MAX: u32 = 127;
pub const INT_LEAST8_MIN: i32 = -128;
pub const INT_LEAST8_MAX: u32 = 127;
pub const INT_FAST8_MIN: i32 = -128;
pub const INT_FAST8_MAX: u32 = 127;
pub const UINT8_MAX: u32 = 255;
pub const UINT_LEAST8_MAX: u32 = 255;
pub const UINT_FAST8_MAX: u32 = 255;
pub const INT16_MIN: i32 = -32768;
pub const INT16_MAX: u32 = 32767;
pub const INT_LEAST16_MIN: i32 = -32768;
pub const INT_LEAST16_MAX: u32 = 32767;
pub const UINT16_MAX: u32 = 65535;
pub const UINT_LEAST16_MAX: u32 = 65535;
pub const INT32_MIN: i32 = -2147483648;
pub const INT32_MAX: u32 = 2147483647;
pub const INT_LEAST32_MIN: i32 = -2147483648;
pub const INT_LEAST32_MAX: u32 = 2147483647;
pub const INT_FAST32_MIN: i32 = -2147483648;
pub const INT_FAST32_MAX: u32 = 2147483647;
pub const UINT32_MAX: u32 = 4294967295;
pub const UINT_LEAST32_MAX: u32 = 4294967295;
pub const UINT_FAST32_MAX: u32 = 4294967295;
pub const SIG_ATOMIC_MAX: u32 = 2147483647;
pub const SIG_ATOMIC_MIN: i32 = -2147483648;
pub const WINT_MAX: u32 = 4294967295;
pub const WINT_MIN: u32 = 0;
pub const __PRI_64_prefix: &[u8; 2] = b"l\0";
pub const __PRI_PTR_prefix: &[u8; 2] = b"l\0";
pub const __PRI_FAST_prefix: &[u8; 2] = b"l\0";
pub const PRId8: &[u8; 2] = b"d\0";
pub const PRId16: &[u8; 2] = b"d\0";
pub const PRId32: &[u8; 2] = b"d\0";
pub const PRId64: &[u8; 3] = b"ld\0";
pub const PRIdLEAST8: &[u8; 2] = b"d\0";
pub const PRIdLEAST16: &[u8; 2] = b"d\0";
pub const PRIdLEAST32: &[u8; 2] = b"d\0";
pub const PRIdLEAST64: &[u8; 3] = b"ld\0";
pub const PRIdFAST8: &[u8; 2] = b"d\0";
pub const PRIdFAST16: &[u8; 3] = b"ld\0";
pub const PRIdFAST32: &[u8; 3] = b"ld\0";
pub const PRIdFAST64: &[u8; 3] = b"ld\0";
pub const PRIdMAX: &[u8; 3] = b"jd\0";
pub const PRIdPTR: &[u8; 3] = b"ld\0";
pub const PRIi8: &[u8; 2] = b"i\0";
pub const PRIi16: &[u8; 2] = b"i\0";
pub const PRIi32: &[u8; 2] = b"i\0";
pub const PRIi64: &[u8; 3] = b"li\0";
pub const PRIiLEAST8: &[u8; 2] = b"i\0";
pub const PRIiLEAST16: &[u8; 2] = b"i\0";
pub const PRIiLEAST32: &[u8; 2] = b"i\0";
pub const PRIiLEAST64: &[u8; 3] = b"li\0";
pub const PRIiFAST8: &[u8; 2] = b"i\0";
pub const PRIiFAST16: &[u8; 3] = b"li\0";
pub const PRIiFAST32: &[u8; 3] = b"li\0";
pub const PRIiFAST64: &[u8; 3] = b"li\0";
pub const PRIiMAX: &[u8; 3] = b"ji\0";
pub const PRIiPTR: &[u8; 3] = b"li\0";
pub const PRIb8: &[u8; 2] = b"b\0";
pub const PRIb16: &[u8; 2] = b"b\0";
pub const PRIb32: &[u8; 2] = b"b\0";
pub const PRIb64: &[u8; 3] = b"lb\0";
pub const PRIbLEAST8: &[u8; 2] = b"b\0";
pub const PRIbLEAST16: &[u8; 2] = b"b\0";
pub const PRIbLEAST32: &[u8; 2] = b"b\0";
pub const PRIbLEAST64: &[u8; 3] = b"lb\0";
pub const PRIbFAST8: &[u8; 2] = b"b\0";
pub const PRIbFAST16: &[u8; 3] = b"lb\0";
pub const PRIbFAST32: &[u8; 3] = b"lb\0";
pub const PRIbFAST64: &[u8; 3] = b"lb\0";
pub const PRIbMAX: &[u8; 3] = b"jb\0";
pub const PRIbPTR: &[u8; 3] = b"lb\0";
pub const PRIB8: &[u8; 2] = b"B\0";
pub const PRIB16: &[u8; 2] = b"B\0";
pub const PRIB32: &[u8; 2] = b"B\0";
pub const PRIB64: &[u8; 3] = b"lB\0";
pub const PRIBLEAST8: &[u8; 2] = b"B\0";
pub const PRIBLEAST16: &[u8; 2] = b"B\0";
pub const PRIBLEAST32: &[u8; 2] = b"B\0";
pub const PRIBLEAST64: &[u8; 3] = b"lB\0";
pub const PRIBFAST8: &[u8; 2] = b"B\0";
pub const PRIBFAST16: &[u8; 3] = b"lB\0";
pub const PRIBFAST32: &[u8; 3] = b"lB\0";
pub const PRIBFAST64: &[u8; 3] = b"lB\0";
pub const PRIBMAX: &[u8; 3] = b"jB\0";
pub const PRIBPTR: &[u8; 3] = b"lB\0";
pub const PRIo8: &[u8; 2] = b"o\0";
pub const PRIo16: &[u8; 2] = b"o\0";
pub const PRIo32: &[u8; 2] = b"o\0";
pub const PRIo64: &[u8; 3] = b"lo\0";
pub const PRIoLEAST8: &[u8; 2] = b"o\0";
pub const PRIoLEAST16: &[u8; 2] = b"o\0";
pub const PRIoLEAST32: &[u8; 2] = b"o\0";
pub const PRIoLEAST64: &[u8; 3] = b"lo\0";
pub const PRIoFAST8: &[u8; 2] = b"o\0";
pub const PRIoFAST16: &[u8; 3] = b"lo\0";
pub const PRIoFAST32: &[u8; 3] = b"lo\0";
pub const PRIoFAST64: &[u8; 3] = b"lo\0";
pub const PRIoMAX: &[u8; 3] = b"jo\0";
pub const PRIoPTR: &[u8; 3] = b"lo\0";
pub const PRIu8: &[u8; 2] = b"u\0";
pub const PRIu16: &[u8; 2] = b"u\0";
pub const PRIu32: &[u8; 2] = b"u\0";
pub const PRIu64: &[u8; 3] = b"lu\0";
pub const PRIuLEAST8: &[u8; 2] = b"u\0";
pub const PRIuLEAST16: &[u8; 2] = b"u\0";
pub const PRIuLEAST32: &[u8; 2] = b"u\0";
pub const PRIuLEAST64: &[u8; 3] = b"lu\0";
pub const PRIuFAST8: &[u8; 2] = b"u\0";
pub const PRIuFAST16: &[u8; 3] = b"lu\0";
pub const PRIuFAST32: &[u8; 3] = b"lu\0";
pub const PRIuFAST64: &[u8; 3] = b"lu\0";
pub const PRIuMAX: &[u8; 3] = b"ju\0";
pub const PRIuPTR: &[u8; 3] = b"lu\0";
pub const PRIx8: &[u8; 2] = b"x\0";
pub const PRIx16: &[u8; 2] = b"x\0";
pub const PRIx32: &[u8; 2] = b"x\0";
pub const PRIx64: &[u8; 3] = b"lx\0";
pub const PRIxLEAST8: &[u8; 2] = b"x\0";
pub const PRIxLEAST16: &[u8; 2] = b"x\0";
pub const PRIxLEAST32: &[u8; 2] = b"x\0";
pub const PRIxLEAST64: &[u8; 3] = b"lx\0";
pub const PRIxFAST8: &[u8; 2] = b"x\0";
pub const PRIxFAST16: &[u8; 3] = b"lx\0";
pub const PRIxFAST32: &[u8; 3] = b"lx\0";
pub const PRIxFAST64: &[u8; 3] = b"lx\0";
pub const PRIxMAX: &[u8; 3] = b"jx\0";
pub const PRIxPTR: &[u8; 3] = b"lx\0";
pub const PRIX8: &[u8; 2] = b"X\0";
pub const PRIX16: &[u8; 2] = b"X\0";
pub const PRIX32: &[u8; 2] = b"X\0";
pub const PRIX64: &[u8; 3] = b"lX\0";
pub const PRIXLEAST8: &[u8; 2] = b"X\0";
pub const PRIXLEAST16: &[u8; 2] = b"X\0";
pub const PRIXLEAST32: &[u8; 2] = b"X\0";
pub const PRIXLEAST64: &[u8; 3] = b"lX\0";
pub const PRIXFAST8: &[u8; 2] = b"X\0";
pub const PRIXFAST16: &[u8; 3] = b"lX\0";
pub const PRIXFAST32: &[u8; 3] = b"lX\0";
pub const PRIXFAST64: &[u8; 3] = b"lX\0";
pub const PRIXMAX: &[u8; 3] = b"jX\0";
pub const PRIXPTR: &[u8; 3] = b"lX\0";
pub const SCNd8: &[u8; 4] = b"hhd\0";
pub const SCNd16: &[u8; 3] = b"hd\0";
pub const SCNd32: &[u8; 2] = b"d\0";
pub const SCNd64: &[u8; 3] = b"ld\0";
pub const SCNdLEAST8: &[u8; 4] = b"hhd\0";
pub const SCNdLEAST16: &[u8; 3] = b"hd\0";
pub const SCNdLEAST32: &[u8; 2] = b"d\0";
pub const SCNdLEAST64: &[u8; 3] = b"ld\0";
pub const SCNdFAST8: &[u8; 4] = b"hhd\0";
pub const SCNdFAST16: &[u8; 3] = b"ld\0";
pub const SCNdFAST32: &[u8; 3] = b"ld\0";
pub const SCNdFAST64: &[u8; 3] = b"ld\0";
pub const SCNdMAX: &[u8; 3] = b"jd\0";
pub const SCNdPTR: &[u8; 3] = b"ld\0";
pub const SCNi8: &[u8; 4] = b"hhi\0";
pub const SCNi16: &[u8; 3] = b"hi\0";
pub const SCNi32: &[u8; 2] = b"i\0";
pub const SCNi64: &[u8; 3] = b"li\0";
pub const SCNiLEAST8: &[u8; 4] = b"hhi\0";
pub const SCNiLEAST16: &[u8; 3] = b"hi\0";
pub const SCNiLEAST32: &[u8; 2] = b"i\0";
pub const SCNiLEAST64: &[u8; 3] = b"li\0";
pub const SCNiFAST8: &[u8; 4] = b"hhi\0";
pub const SCNiFAST16: &[u8; 3] = b"li\0";
pub const SCNiFAST32: &[u8; 3] = b"li\0";
pub const SCNiFAST64: &[u8; 3] = b"li\0";
pub const SCNiMAX: &[u8; 3] = b"ji\0";
pub const SCNiPTR: &[u8; 3] = b"li\0";
pub const SCNb8: &[u8; 4] = b"hhb\0";
pub const SCNb16: &[u8; 3] = b"hb\0";
pub const SCNb32: &[u8; 2] = b"b\0";
pub const SCNb64: &[u8; 3] = b"lb\0";
pub const SCNbLEAST8: &[u8; 4] = b"hhb\0";
pub const SCNbLEAST16: &[u8; 3] = b"hb\0";
pub const SCNbLEAST32: &[u8; 2] = b"b\0";
pub const SCNbLEAST64: &[u8; 3] = b"lb\0";
pub const SCNbFAST8: &[u8; 4] = b"hhb\0";
pub const SCNbFAST16: &[u8; 3] = b"lb\0";
pub const SCNbFAST32: &[u8; 3] = b"lb\0";
pub const SCNbFAST64: &[u8; 3] = b"lb\0";
pub const SCNbMAX: &[u8; 3] = b"jb\0";
pub const SCNbPTR: &[u8; 3] = b"lb\0";
pub const SCNB8: &[u8; 4] = b"hhB\0";
pub const SCNB16: &[u8; 3] = b"hB\0";
pub const SCNB32: &[u8; 2] = b"B\0";
pub const SCNB64: &[u8; 3] = b"lB\0";
pub const SCNBLEAST8: &[u8; 4] = b"hhB\0";
pub const SCNBLEAST16: &[u8; 3] = b"hB\0";
pub const SCNBLEAST32: &[u8; 2] = b"B\0";
pub const SCNBLEAST64: &[u8; 3] = b"lB\0";
pub const SCNBFAST8: &[u8; 4] = b"hhB\0";
pub const SCNBFAST16: &[u8; 3] = b"lB\0";
pub const SCNBFAST32: &[u8; 3] = b"lB\0";
pub const SCNBFAST64: &[u8; 3] = b"lB\0";
pub const SCNBMAX: &[u8; 3] = b"jB\0";
pub const SCNBPTR: &[u8; 3] = b"lB\0";
pub const SCNo8: &[u8; 4] = b"hho\0";
pub const SCNo16: &[u8; 3] = b"ho\0";
pub const SCNo32: &[u8; 2] = b"o\0";
pub const SCNo64: &[u8; 3] = b"lo\0";
pub const SCNoLEAST8: &[u8; 4] = b"hho\0";
pub const SCNoLEAST16: &[u8; 3] = b"ho\0";
pub const SCNoLEAST32: &[u8; 2] = b"o\0";
pub const SCNoLEAST64: &[u8; 3] = b"lo\0";
pub const SCNoFAST8: &[u8; 4] = b"hho\0";
pub const SCNoFAST16: &[u8; 3] = b"lo\0";
pub const SCNoFAST32: &[u8; 3] = b"lo\0";
pub const SCNoFAST64: &[u8; 3] = b"lo\0";
pub const SCNoMAX: &[u8; 3] = b"jo\0";
pub const SCNoPTR: &[u8; 3] = b"lo\0";
pub const SCNu8: &[u8; 4] = b"hhu\0";
pub const SCNu16: &[u8; 3] = b"hu\0";
pub const SCNu32: &[u8; 2] = b"u\0";
pub const SCNu64: &[u8; 3] = b"lu\0";
pub const SCNuLEAST8: &[u8; 4] = b"hhu\0";
pub const SCNuLEAST16: &[u8; 3] = b"hu\0";
pub const SCNuLEAST32: &[u8; 2] = b"u\0";
pub const SCNuLEAST64: &[u8; 3] = b"lu\0";
pub const SCNuFAST8: &[u8; 4] = b"hhu\0";
pub const SCNuFAST16: &[u8; 3] = b"lu\0";
pub const SCNuFAST32: &[u8; 3] = b"lu\0";
pub const SCNuFAST64: &[u8; 3] = b"lu\0";
pub const SCNuMAX: &[u8; 3] = b"ju\0";
pub const SCNuPTR: &[u8; 3] = b"lu\0";
pub const SCNx8: &[u8; 4] = b"hhx\0";
pub const SCNx16: &[u8; 3] = b"hx\0";
pub const SCNx32: &[u8; 2] = b"x\0";
pub const SCNx64: &[u8; 3] = b"lx\0";
pub const SCNxLEAST8: &[u8; 4] = b"hhx\0";
pub const SCNxLEAST16: &[u8; 3] = b"hx\0";
pub const SCNxLEAST32: &[u8; 2] = b"x\0";
pub const SCNxLEAST64: &[u8; 3] = b"lx\0";
pub const SCNxFAST8: &[u8; 4] = b"hhx\0";
pub const SCNxFAST16: &[u8; 3] = b"lx\0";
pub const SCNxFAST32: &[u8; 3] = b"lx\0";
pub const SCNxFAST64: &[u8; 3] = b"lx\0";
pub const SCNxMAX: &[u8; 3] = b"jx\0";
pub const SCNxPTR: &[u8; 3] = b"lx\0";
pub const __GNUC_VA_LIST: u32 = 1;
pub const JNI_FALSE: u32 = 0;
pub const JNI_TRUE: u32 = 1;
pub const JNI_VERSION_1_1: u32 = 65537;
pub const JNI_VERSION_1_2: u32 = 65538;
pub const JNI_VERSION_1_4: u32 = 65540;
pub const JNI_VERSION_1_6: u32 = 65542;
pub const JNI_OK: u32 = 0;
pub const JNI_ERR: i32 = -1;
pub const JNI_EDETACHED: i32 = -2;
pub const JNI_EVERSION: i32 = -3;
pub const JNI_ENOMEM: i32 = -4;
pub const JNI_EEXIST: i32 = -5;
pub const JNI_EINVAL: i32 = -6;
pub const JNI_COMMIT: u32 = 1;
pub const JNI_ABORT: u32 = 2;
pub const __BITS_PER_LONG: u32 = 64;
pub const __FD_SETSIZE: u32 = 1024;
pub const AMOTION_EVENT_ACTION_POINTER_INDEX_SHIFT: u32 = 8;
pub const true_: u32 = 1;
pub const false_: u32 = 0;
pub const __bool_true_false_are_defined: u32 = 1;
extern "C" {
    pub fn android_get_application_target_sdk_version() -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Returns the API level of the device we're actually running on, or -1 on failure.\n The returned values correspond to the named constants in `<android/api-level.h>`,\n and is equivalent to the Java `Build.VERSION.SDK_INT` API.\n\n See also android_get_application_target_sdk_version()."]
    pub fn android_get_device_api_level() -> ::std::os::raw::c_int;
}
pub type wchar_t = ::std::os::raw::c_int;
#[repr(C)]
#[repr(align(16))]
#[derive(Debug, Copy, Clone)]
pub struct max_align_t {
    pub __clang_max_align_nonce1: ::std::os::raw::c_longlong,
    pub __bindgen_padding_0: u64,
    pub __clang_max_align_nonce2: u128,
}
#[test]
fn bindgen_test_layout_max_align_t() {
    const UNINIT: ::std::mem::MaybeUninit<max_align_t> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<max_align_t>(),
        32usize,
        concat!("Size of: ", stringify!(max_align_t))
    );
    assert_eq!(
        ::std::mem::align_of::<max_align_t>(),
        16usize,
        concat!("Alignment of ", stringify!(max_align_t))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).__clang_max_align_nonce1) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(max_align_t),
            "::",
            stringify!(__clang_max_align_nonce1)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).__clang_max_align_nonce2) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(max_align_t),
            "::",
            stringify!(__clang_max_align_nonce2)
        )
    );
}
pub type __int8_t = ::std::os::raw::c_schar;
pub type __uint8_t = ::std::os::raw::c_uchar;
pub type __int16_t = ::std::os::raw::c_short;
pub type __uint16_t = ::std::os::raw::c_ushort;
pub type __int32_t = ::std::os::raw::c_int;
pub type __uint32_t = ::std::os::raw::c_uint;
pub type __int64_t = ::std::os::raw::c_long;
pub type __uint64_t = ::std::os::raw::c_ulong;
pub type __intptr_t = ::std::os::raw::c_long;
pub type __uintptr_t = ::std::os::raw::c_ulong;
pub type int_least8_t = i8;
pub type uint_least8_t = u8;
pub type int_least16_t = i16;
pub type uint_least16_t = u16;
pub type int_least32_t = i32;
pub type uint_least32_t = u32;
pub type int_least64_t = i64;
pub type uint_least64_t = u64;
pub type int_fast8_t = i8;
pub type uint_fast8_t = u8;
pub type int_fast64_t = i64;
pub type uint_fast64_t = u64;
pub type int_fast16_t = i64;
pub type uint_fast16_t = u64;
pub type int_fast32_t = i64;
pub type uint_fast32_t = u64;
pub type uintmax_t = u64;
pub type intmax_t = i64;
#[doc = " Rectangular window area.\n\n This is the NDK equivalent of the android.graphics.Rect class in Java. It is\n used with {@link ANativeActivityCallbacks::onContentRectChanged} event\n callback and the ANativeWindow_lock() function.\n\n In a valid ARect, left <= right and top <= bottom. ARect with left=0, top=10,\n right=1, bottom=11 contains only one pixel at x=0, y=10."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ARect {
    #[doc = " Minimum X coordinate of the rectangle."]
    pub left: i32,
    #[doc = " Minimum Y coordinate of the rectangle."]
    pub top: i32,
    #[doc = " Maximum X coordinate of the rectangle."]
    pub right: i32,
    #[doc = " Maximum Y coordinate of the rectangle."]
    pub bottom: i32,
}
#[test]
fn bindgen_test_layout_ARect() {
    const UNINIT: ::std::mem::MaybeUninit<ARect> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ARect>(),
        16usize,
        concat!("Size of: ", stringify!(ARect))
    );
    assert_eq!(
        ::std::mem::align_of::<ARect>(),
        4usize,
        concat!("Alignment of ", stringify!(ARect))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).left) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ARect),
            "::",
            stringify!(left)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).top) as usize - ptr as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(ARect),
            "::",
            stringify!(top)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).right) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ARect),
            "::",
            stringify!(right)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).bottom) as usize - ptr as usize },
        12usize,
        concat!(
            "Offset of field: ",
            stringify!(ARect),
            "::",
            stringify!(bottom)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct imaxdiv_t {
    pub quot: intmax_t,
    pub rem: intmax_t,
}
#[test]
fn bindgen_test_layout_imaxdiv_t() {
    const UNINIT: ::std::mem::MaybeUninit<imaxdiv_t> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<imaxdiv_t>(),
        16usize,
        concat!("Size of: ", stringify!(imaxdiv_t))
    );
    assert_eq!(
        ::std::mem::align_of::<imaxdiv_t>(),
        8usize,
        concat!("Alignment of ", stringify!(imaxdiv_t))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).quot) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(imaxdiv_t),
            "::",
            stringify!(quot)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).rem) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(imaxdiv_t),
            "::",
            stringify!(rem)
        )
    );
}
extern "C" {
    pub fn imaxabs(__i: intmax_t) -> intmax_t;
}
extern "C" {
    pub fn imaxdiv(__numerator: intmax_t, __denominator: intmax_t) -> imaxdiv_t;
}
extern "C" {
    pub fn strtoimax(
        __s: *const ::std::os::raw::c_char,
        __end_ptr: *mut *mut ::std::os::raw::c_char,
        __base: ::std::os::raw::c_int,
    ) -> intmax_t;
}
extern "C" {
    pub fn strtoumax(
        __s: *const ::std::os::raw::c_char,
        __end_ptr: *mut *mut ::std::os::raw::c_char,
        __base: ::std::os::raw::c_int,
    ) -> uintmax_t;
}
extern "C" {
    pub fn wcstoimax(
        __s: *const wchar_t,
        __end_ptr: *mut *mut wchar_t,
        __base: ::std::os::raw::c_int,
    ) -> intmax_t;
}
extern "C" {
    pub fn wcstoumax(
        __s: *const wchar_t,
        __end_ptr: *mut *mut wchar_t,
        __base: ::std::os::raw::c_int,
    ) -> uintmax_t;
}
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_R8G8B8A8_UNORM\n   OpenGL ES: GL_RGBA8"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_R8G8B8A8_UNORM: AHardwareBuffer_Format = 1;
#[doc = " 32 bits per pixel, 8 bits per channel format where alpha values are\n ignored (always opaque).\n Corresponding formats:\n   Vulkan: VK_FORMAT_R8G8B8A8_UNORM\n   OpenGL ES: GL_RGB8"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_R8G8B8X8_UNORM: AHardwareBuffer_Format = 2;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_R8G8B8_UNORM\n   OpenGL ES: GL_RGB8"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_R8G8B8_UNORM: AHardwareBuffer_Format = 3;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_R5G6B5_UNORM_PACK16\n   OpenGL ES: GL_RGB565"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_R5G6B5_UNORM: AHardwareBuffer_Format = 4;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_R16G16B16A16_SFLOAT\n   OpenGL ES: GL_RGBA16F"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_R16G16B16A16_FLOAT: AHardwareBuffer_Format =
    22;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_A2B10G10R10_UNORM_PACK32\n   OpenGL ES: GL_RGB10_A2"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_R10G10B10A2_UNORM: AHardwareBuffer_Format =
    43;
#[doc = " Opaque binary blob format.\n Must have height 1 and one layer, with width equal to the buffer\n size in bytes. Corresponds to Vulkan buffers and OpenGL buffer\n objects. Can be bound to the latter using GL_EXT_external_buffer."]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_BLOB: AHardwareBuffer_Format = 33;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_D16_UNORM\n   OpenGL ES: GL_DEPTH_COMPONENT16"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_D16_UNORM: AHardwareBuffer_Format = 48;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_X8_D24_UNORM_PACK32\n   OpenGL ES: GL_DEPTH_COMPONENT24"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_D24_UNORM: AHardwareBuffer_Format = 49;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_D24_UNORM_S8_UINT\n   OpenGL ES: GL_DEPTH24_STENCIL8"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_D24_UNORM_S8_UINT: AHardwareBuffer_Format =
    50;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_D32_SFLOAT\n   OpenGL ES: GL_DEPTH_COMPONENT32F"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_D32_FLOAT: AHardwareBuffer_Format = 51;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_D32_SFLOAT_S8_UINT\n   OpenGL ES: GL_DEPTH32F_STENCIL8"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_D32_FLOAT_S8_UINT: AHardwareBuffer_Format =
    52;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_S8_UINT\n   OpenGL ES: GL_STENCIL_INDEX8"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_S8_UINT: AHardwareBuffer_Format = 53;
#[doc = " YUV 420 888 format.\n Must have an even width and height. Can be accessed in OpenGL\n shaders through an external sampler. Does not support mip-maps\n cube-maps or multi-layered textures."]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_Y8Cb8Cr8_420: AHardwareBuffer_Format = 35;
#[doc = " YUV P010 format.\n Must have an even width and height. Can be accessed in OpenGL\n shaders through an external sampler. Does not support mip-maps\n cube-maps or multi-layered textures."]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_YCbCr_P010: AHardwareBuffer_Format = 54;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_R8_UNORM\n   OpenGL ES: GR_GL_R8"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_R8_UNORM: AHardwareBuffer_Format = 56;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_R16_UINT\n   OpenGL ES: GL_R16UI"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_R16_UINT: AHardwareBuffer_Format = 57;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_R16G16_UINT\n   OpenGL ES: GL_RG16UI"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_R16G16_UINT: AHardwareBuffer_Format = 58;
#[doc = " Corresponding formats:\n   Vulkan: VK_FORMAT_R10X6G10X6B10X6A10X6_UNORM_4PACK16\n   OpenGL ES: N/A"]
pub const AHardwareBuffer_Format_AHARDWAREBUFFER_FORMAT_R10G10B10A10_UNORM: AHardwareBuffer_Format =
    59;
#[doc = " Buffer pixel formats."]
pub type AHardwareBuffer_Format = ::std::os::raw::c_uint;
#[doc = " The buffer will never be locked for direct CPU reads using the\n AHardwareBuffer_lock() function. Note that reading the buffer\n using OpenGL or Vulkan functions or memory mappings is still\n allowed."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_CPU_READ_NEVER:
    AHardwareBuffer_UsageFlags = 0;
#[doc = " The buffer will sometimes be locked for direct CPU reads using\n the AHardwareBuffer_lock() function. Note that reading the\n buffer using OpenGL or Vulkan functions or memory mappings\n does not require the presence of this flag."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_CPU_READ_RARELY:
    AHardwareBuffer_UsageFlags = 2;
#[doc = " The buffer will often be locked for direct CPU reads using\n the AHardwareBuffer_lock() function. Note that reading the\n buffer using OpenGL or Vulkan functions or memory mappings\n does not require the presence of this flag."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_CPU_READ_OFTEN:
    AHardwareBuffer_UsageFlags = 3;
#[doc = " CPU read value mask."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_CPU_READ_MASK:
    AHardwareBuffer_UsageFlags = 15;
#[doc = " The buffer will never be locked for direct CPU writes using the\n AHardwareBuffer_lock() function. Note that writing the buffer\n using OpenGL or Vulkan functions or memory mappings is still\n allowed."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_CPU_WRITE_NEVER:
    AHardwareBuffer_UsageFlags = 0;
#[doc = " The buffer will sometimes be locked for direct CPU writes using\n the AHardwareBuffer_lock() function. Note that writing the\n buffer using OpenGL or Vulkan functions or memory mappings\n does not require the presence of this flag."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_CPU_WRITE_RARELY:
    AHardwareBuffer_UsageFlags = 32;
#[doc = " The buffer will often be locked for direct CPU writes using\n the AHardwareBuffer_lock() function. Note that writing the\n buffer using OpenGL or Vulkan functions or memory mappings\n does not require the presence of this flag."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_CPU_WRITE_OFTEN:
    AHardwareBuffer_UsageFlags = 48;
#[doc = " CPU write value mask."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_CPU_WRITE_MASK:
    AHardwareBuffer_UsageFlags = 240;
#[doc = " The buffer will be read from by the GPU as a texture."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_GPU_SAMPLED_IMAGE:
    AHardwareBuffer_UsageFlags = 256;
#[doc = " The buffer will be written to by the GPU as a framebuffer attachment."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_GPU_FRAMEBUFFER:
    AHardwareBuffer_UsageFlags = 512;
#[doc = " The buffer will be written to by the GPU as a framebuffer\n attachment.\n\n Note that the name of this flag is somewhat misleading: it does\n not imply that the buffer contains a color format. A buffer with\n depth or stencil format that will be used as a framebuffer\n attachment should also have this flag. Use the equivalent flag\n AHARDWAREBUFFER_USAGE_GPU_FRAMEBUFFER to avoid this confusion."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_GPU_COLOR_OUTPUT:
    AHardwareBuffer_UsageFlags = 512;
#[doc = " The buffer will be used as a composer HAL overlay layer.\n\n This flag is currently only needed when using ASurfaceTransaction_setBuffer\n to set a buffer. In all other cases, the framework adds this flag\n internally to buffers that could be presented in a composer overlay.\n ASurfaceTransaction_setBuffer is special because it uses buffers allocated\n directly through AHardwareBuffer_allocate instead of buffers allocated\n by the framework."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_COMPOSER_OVERLAY:
    AHardwareBuffer_UsageFlags = 2048;
#[doc = " The buffer is protected from direct CPU access or being read by\n non-secure hardware, such as video encoders.\n\n This flag is incompatible with CPU read and write flags. It is\n mainly used when handling DRM video. Refer to the EGL extension\n EGL_EXT_protected_content and GL extension\n GL_EXT_protected_textures for more information on how these\n buffers are expected to behave."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_PROTECTED_CONTENT:
    AHardwareBuffer_UsageFlags = 16384;
#[doc = " The buffer will be read by a hardware video encoder."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VIDEO_ENCODE:
    AHardwareBuffer_UsageFlags = 65536;
#[doc = " The buffer will be used for direct writes from sensors.\n When this flag is present, the format must be AHARDWAREBUFFER_FORMAT_BLOB."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_SENSOR_DIRECT_DATA:
    AHardwareBuffer_UsageFlags = 8388608;
#[doc = " The buffer will be used as a shader storage or uniform buffer object.\n When this flag is present, the format must be AHARDWAREBUFFER_FORMAT_BLOB."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_GPU_DATA_BUFFER:
    AHardwareBuffer_UsageFlags = 16777216;
#[doc = " The buffer will be used as a cube map texture.\n When this flag is present, the buffer must have a layer count\n that is a multiple of 6. Note that buffers with this flag must be\n bound to OpenGL textures using the extension\n GL_EXT_EGL_image_storage instead of GL_KHR_EGL_image."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_GPU_CUBE_MAP:
    AHardwareBuffer_UsageFlags = 33554432;
#[doc = " The buffer contains a complete mipmap hierarchy.\n Note that buffers with this flag must be bound to OpenGL textures using\n the extension GL_EXT_EGL_image_storage instead of GL_KHR_EGL_image."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_GPU_MIPMAP_COMPLETE:
    AHardwareBuffer_UsageFlags = 67108864;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_FRONT_BUFFER:
    AHardwareBuffer_UsageFlags = 4294967296;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_0: AHardwareBuffer_UsageFlags =
    268435456;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_1: AHardwareBuffer_UsageFlags =
    536870912;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_2: AHardwareBuffer_UsageFlags =
    1073741824;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_3: AHardwareBuffer_UsageFlags =
    2147483648;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_4: AHardwareBuffer_UsageFlags =
    281474976710656;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_5: AHardwareBuffer_UsageFlags =
    562949953421312;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_6: AHardwareBuffer_UsageFlags =
    1125899906842624;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_7: AHardwareBuffer_UsageFlags =
    2251799813685248;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_8: AHardwareBuffer_UsageFlags =
    4503599627370496;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_9: AHardwareBuffer_UsageFlags =
    9007199254740992;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_10: AHardwareBuffer_UsageFlags =
    18014398509481984;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_11: AHardwareBuffer_UsageFlags =
    36028797018963968;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_12: AHardwareBuffer_UsageFlags =
    72057594037927936;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_13: AHardwareBuffer_UsageFlags =
    144115188075855872;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_14: AHardwareBuffer_UsageFlags =
    288230376151711744;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_15: AHardwareBuffer_UsageFlags =
    576460752303423488;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_16: AHardwareBuffer_UsageFlags =
    1152921504606846976;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_17: AHardwareBuffer_UsageFlags =
    2305843009213693952;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_18: AHardwareBuffer_UsageFlags =
    4611686018427387904;
#[doc = " Usage: The buffer is used for front-buffer rendering. When\n front-buffering rendering is specified, different usages may adjust their\n behavior as a result. For example, when used as GPU_COLOR_OUTPUT the buffer\n will behave similar to a single-buffered window. When used with\n COMPOSER_OVERLAY, the system will try to prioritize the buffer receiving\n an overlay plane & avoid caching it in intermediate composition buffers."]
pub const AHardwareBuffer_UsageFlags_AHARDWAREBUFFER_USAGE_VENDOR_19: AHardwareBuffer_UsageFlags =
    9223372036854775808;
#[doc = " Buffer usage flags, specifying how the buffer will be accessed."]
pub type AHardwareBuffer_UsageFlags = ::std::os::raw::c_ulong;
#[doc = " Buffer description. Used for allocating new buffers and querying\n parameters of existing ones."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AHardwareBuffer_Desc {
    #[doc = "< Width in pixels."]
    pub width: u32,
    #[doc = "< Height in pixels."]
    pub height: u32,
    #[doc = " Number of images in an image array. AHardwareBuffers with one\n layer correspond to regular 2D textures. AHardwareBuffers with\n more than layer correspond to texture arrays. If the layer count\n is a multiple of 6 and the usage flag\n AHARDWAREBUFFER_USAGE_GPU_CUBE_MAP is present, the buffer is\n a cube map or a cube map array."]
    pub layers: u32,
    #[doc = "< One of AHardwareBuffer_Format."]
    pub format: u32,
    #[doc = "< Combination of AHardwareBuffer_UsageFlags."]
    pub usage: u64,
    #[doc = "< Row stride in pixels, ignored for AHardwareBuffer_allocate()"]
    pub stride: u32,
    #[doc = "< Initialize to zero, reserved for future use."]
    pub rfu0: u32,
    #[doc = "< Initialize to zero, reserved for future use."]
    pub rfu1: u64,
}
#[test]
fn bindgen_test_layout_AHardwareBuffer_Desc() {
    const UNINIT: ::std::mem::MaybeUninit<AHardwareBuffer_Desc> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<AHardwareBuffer_Desc>(),
        40usize,
        concat!("Size of: ", stringify!(AHardwareBuffer_Desc))
    );
    assert_eq!(
        ::std::mem::align_of::<AHardwareBuffer_Desc>(),
        8usize,
        concat!("Alignment of ", stringify!(AHardwareBuffer_Desc))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).width) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Desc),
            "::",
            stringify!(width)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).height) as usize - ptr as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Desc),
            "::",
            stringify!(height)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).layers) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Desc),
            "::",
            stringify!(layers)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).format) as usize - ptr as usize },
        12usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Desc),
            "::",
            stringify!(format)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).usage) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Desc),
            "::",
            stringify!(usage)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).stride) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Desc),
            "::",
            stringify!(stride)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).rfu0) as usize - ptr as usize },
        28usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Desc),
            "::",
            stringify!(rfu0)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).rfu1) as usize - ptr as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Desc),
            "::",
            stringify!(rfu1)
        )
    );
}
#[doc = " Holds data for a single image plane."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AHardwareBuffer_Plane {
    #[doc = "< Points to first byte in plane"]
    pub data: *mut ::std::os::raw::c_void,
    #[doc = "< Distance in bytes from the color channel of one pixel to the next"]
    pub pixelStride: u32,
    #[doc = "< Distance in bytes from the first value of one row of the image to"]
    pub rowStride: u32,
}
#[test]
fn bindgen_test_layout_AHardwareBuffer_Plane() {
    const UNINIT: ::std::mem::MaybeUninit<AHardwareBuffer_Plane> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<AHardwareBuffer_Plane>(),
        16usize,
        concat!("Size of: ", stringify!(AHardwareBuffer_Plane))
    );
    assert_eq!(
        ::std::mem::align_of::<AHardwareBuffer_Plane>(),
        8usize,
        concat!("Alignment of ", stringify!(AHardwareBuffer_Plane))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).data) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Plane),
            "::",
            stringify!(data)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).pixelStride) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Plane),
            "::",
            stringify!(pixelStride)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).rowStride) as usize - ptr as usize },
        12usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Plane),
            "::",
            stringify!(rowStride)
        )
    );
}
#[doc = " Holds all image planes that contain the pixel data."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AHardwareBuffer_Planes {
    #[doc = "< Number of distinct planes"]
    pub planeCount: u32,
    #[doc = "< Array of image planes"]
    pub planes: [AHardwareBuffer_Plane; 4usize],
}
#[test]
fn bindgen_test_layout_AHardwareBuffer_Planes() {
    const UNINIT: ::std::mem::MaybeUninit<AHardwareBuffer_Planes> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<AHardwareBuffer_Planes>(),
        72usize,
        concat!("Size of: ", stringify!(AHardwareBuffer_Planes))
    );
    assert_eq!(
        ::std::mem::align_of::<AHardwareBuffer_Planes>(),
        8usize,
        concat!("Alignment of ", stringify!(AHardwareBuffer_Planes))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).planeCount) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Planes),
            "::",
            stringify!(planeCount)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).planes) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(AHardwareBuffer_Planes),
            "::",
            stringify!(planes)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AHardwareBuffer {
    _unused: [u8; 0],
}
extern "C" {
    #[doc = " Allocates a buffer that matches the passed AHardwareBuffer_Desc.\n\n If allocation succeeds, the buffer can be used according to the\n usage flags specified in its description. If a buffer is used in ways\n not compatible with its usage flags, the results are undefined and\n may include program termination.\n\n Available since API level 26.\n\n \\return 0 on success, or an error number of the allocation fails for\n any reason. The returned buffer has a reference count of 1."]
    pub fn AHardwareBuffer_allocate(
        desc: *const AHardwareBuffer_Desc,
        outBuffer: *mut *mut AHardwareBuffer,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Acquire a reference on the given AHardwareBuffer object.\n\n This prevents the object from being deleted until the last reference\n is removed.\n\n Available since API level 26."]
    pub fn AHardwareBuffer_acquire(buffer: *mut AHardwareBuffer);
}
extern "C" {
    #[doc = " Remove a reference that was previously acquired with\n AHardwareBuffer_acquire() or AHardwareBuffer_allocate().\n\n Available since API level 26."]
    pub fn AHardwareBuffer_release(buffer: *mut AHardwareBuffer);
}
extern "C" {
    #[doc = " Return a description of the AHardwareBuffer in the passed\n AHardwareBuffer_Desc struct.\n\n Available since API level 26."]
    pub fn AHardwareBuffer_describe(
        buffer: *const AHardwareBuffer,
        outDesc: *mut AHardwareBuffer_Desc,
    );
}
extern "C" {
    #[doc = " Lock the AHardwareBuffer for direct CPU access.\n\n This function can lock the buffer for either reading or writing.\n It may block if the hardware needs to finish rendering, if CPU caches\n need to be synchronized, or possibly for other implementation-\n specific reasons.\n\n The passed AHardwareBuffer must have one layer, otherwise the call\n will fail.\n\n If \\a fence is not negative, it specifies a fence file descriptor on\n which to wait before locking the buffer. If it's negative, the caller\n is responsible for ensuring that writes to the buffer have completed\n before calling this function.  Using this parameter is more efficient\n than waiting on the fence and then calling this function.\n\n The \\a usage parameter may only specify AHARDWAREBUFFER_USAGE_CPU_*.\n If set, then outVirtualAddress is filled with the address of the\n buffer in virtual memory. The flags must also be compatible with\n usage flags specified at buffer creation: if a read flag is passed,\n the buffer must have been created with\n AHARDWAREBUFFER_USAGE_CPU_READ_RARELY or\n AHARDWAREBUFFER_USAGE_CPU_READ_OFTEN. If a write flag is passed, it\n must have been created with AHARDWAREBUFFER_USAGE_CPU_WRITE_RARELY or\n AHARDWAREBUFFER_USAGE_CPU_WRITE_OFTEN.\n\n If \\a rect is not NULL, the caller promises to modify only data in\n the area specified by rect. If rect is NULL, the caller may modify\n the contents of the entire buffer. The content of the buffer outside\n of the specified rect is NOT modified by this call.\n\n It is legal for several different threads to lock a buffer for read\n access; none of the threads are blocked.\n\n Locking a buffer simultaneously for write or read/write is undefined,\n but will neither terminate the process nor block the caller.\n AHardwareBuffer_lock may return an error or leave the buffer's\n content in an indeterminate state.\n\n If the buffer has AHARDWAREBUFFER_FORMAT_BLOB, it is legal lock it\n for reading and writing in multiple threads and/or processes\n simultaneously, and the contents of the buffer behave like shared\n memory.\n\n Available since API level 26.\n\n \\return 0 on success. -EINVAL if \\a buffer is NULL, the usage flags\n are not a combination of AHARDWAREBUFFER_USAGE_CPU_*, or the buffer\n has more than one layer. Error number if the lock fails for any other\n reason."]
    pub fn AHardwareBuffer_lock(
        buffer: *mut AHardwareBuffer,
        usage: u64,
        fence: i32,
        rect: *const ARect,
        outVirtualAddress: *mut *mut ::std::os::raw::c_void,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Unlock the AHardwareBuffer from direct CPU access.\n\n Must be called after all changes to the buffer are completed by the\n caller.  If \\a fence is NULL, the function will block until all work\n is completed.  Otherwise, \\a fence will be set either to a valid file\n descriptor or to -1.  The file descriptor will become signaled once\n the unlocking is complete and buffer contents are updated.\n The caller is responsible for closing the file descriptor once it's\n no longer needed.  The value -1 indicates that unlocking has already\n completed before the function returned and no further operations are\n necessary.\n\n Available since API level 26.\n\n \\return 0 on success. -EINVAL if \\a buffer is NULL. Error number if\n the unlock fails for any reason."]
    pub fn AHardwareBuffer_unlock(
        buffer: *mut AHardwareBuffer,
        fence: *mut i32,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Send the AHardwareBuffer to an AF_UNIX socket.\n\n Available since API level 26.\n\n \\return 0 on success, -EINVAL if \\a buffer is NULL, or an error\n number if the operation fails for any reason."]
    pub fn AHardwareBuffer_sendHandleToUnixSocket(
        buffer: *const AHardwareBuffer,
        socketFd: ::std::os::raw::c_int,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Receive an AHardwareBuffer from an AF_UNIX socket.\n\n Available since API level 26.\n\n \\return 0 on success, -EINVAL if \\a outBuffer is NULL, or an error\n number if the operation fails for any reason."]
    pub fn AHardwareBuffer_recvHandleFromUnixSocket(
        socketFd: ::std::os::raw::c_int,
        outBuffer: *mut *mut AHardwareBuffer,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Lock a potentially multi-planar AHardwareBuffer for direct CPU access.\n\n This function is similar to AHardwareBuffer_lock, but can lock multi-planar\n formats. The locked planes are returned in the \\a outPlanes argument. Note,\n that multi-planar should not be confused with multi-layer images, which this\n locking function does not support.\n\n YUV formats are always represented by three separate planes of data, one for\n each color plane. The order of planes in the array is guaranteed such that\n plane #0 is always Y, plane #1 is always U (Cb), and plane #2 is always V\n (Cr). All other formats are represented by a single plane.\n\n Additional information always accompanies the buffers, describing the row\n stride and the pixel stride for each plane.\n\n In case the buffer cannot be locked, \\a outPlanes will contain zero planes.\n\n See the AHardwareBuffer_lock documentation for all other locking semantics.\n\n Available since API level 29.\n\n \\return 0 on success. -EINVAL if \\a buffer is NULL, the usage flags\n are not a combination of AHARDWAREBUFFER_USAGE_CPU_*, or the buffer\n has more than one layer. Error number if the lock fails for any other\n reason."]
    pub fn AHardwareBuffer_lockPlanes(
        buffer: *mut AHardwareBuffer,
        usage: u64,
        fence: i32,
        rect: *const ARect,
        outPlanes: *mut AHardwareBuffer_Planes,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Test whether the given format and usage flag combination is\n allocatable.\n\n If this function returns true, it means that a buffer with the given\n description can be allocated on this implementation, unless resource\n exhaustion occurs. If this function returns false, it means that the\n allocation of the given description will never succeed.\n\n The return value of this function may depend on all fields in the\n description, except stride, which is always ignored. For example,\n some implementations have implementation-defined limits on texture\n size and layer count.\n\n Available since API level 29.\n\n \\return 1 if the format and usage flag combination is allocatable,\n     0 otherwise."]
    pub fn AHardwareBuffer_isSupported(desc: *const AHardwareBuffer_Desc) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Lock an AHardwareBuffer for direct CPU access.\n\n This function is the same as the above lock function, but passes back\n additional information about the bytes per pixel and the bytes per stride\n of the locked buffer.  If the bytes per pixel or bytes per stride are unknown\n or variable, or if the underlying mapper implementation does not support returning\n additional information, then this call will fail with INVALID_OPERATION\n\n Available since API level 29."]
    pub fn AHardwareBuffer_lockAndGetInfo(
        buffer: *mut AHardwareBuffer,
        usage: u64,
        fence: i32,
        rect: *const ARect,
        outVirtualAddress: *mut *mut ::std::os::raw::c_void,
        outBytesPerPixel: *mut i32,
        outBytesPerStride: *mut i32,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Get the system wide unique id for an AHardwareBuffer.\n\n Available since API level 31.\n\n \\return 0 on success, -EINVAL if \\a buffer or \\a outId is NULL, or an error number if the\n operation fails for any reason."]
    pub fn AHardwareBuffer_getId(
        buffer: *const AHardwareBuffer,
        outId: *mut u64,
    ) -> ::std::os::raw::c_int;
}
pub type va_list = __builtin_va_list;
pub type __gnuc_va_list = __builtin_va_list;
pub type jboolean = u8;
pub type jbyte = i8;
pub type jchar = u16;
pub type jshort = i16;
pub type jint = i32;
pub type jlong = i64;
pub type jfloat = f32;
pub type jdouble = f64;
pub type jsize = jint;
pub type jobject = *mut ::std::os::raw::c_void;
pub type jclass = jobject;
pub type jstring = jobject;
pub type jarray = jobject;
pub type jobjectArray = jarray;
pub type jbooleanArray = jarray;
pub type jbyteArray = jarray;
pub type jcharArray = jarray;
pub type jshortArray = jarray;
pub type jintArray = jarray;
pub type jlongArray = jarray;
pub type jfloatArray = jarray;
pub type jdoubleArray = jarray;
pub type jthrowable = jobject;
pub type jweak = jobject;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct _jfieldID {
    _unused: [u8; 0],
}
pub type jfieldID = *mut _jfieldID;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct _jmethodID {
    _unused: [u8; 0],
}
pub type jmethodID = *mut _jmethodID;
#[repr(C)]
#[derive(Copy, Clone)]
pub union jvalue {
    pub z: jboolean,
    pub b: jbyte,
    pub c: jchar,
    pub s: jshort,
    pub i: jint,
    pub j: jlong,
    pub f: jfloat,
    pub d: jdouble,
    pub l: jobject,
}
#[test]
fn bindgen_test_layout_jvalue() {
    const UNINIT: ::std::mem::MaybeUninit<jvalue> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<jvalue>(),
        8usize,
        concat!("Size of: ", stringify!(jvalue))
    );
    assert_eq!(
        ::std::mem::align_of::<jvalue>(),
        8usize,
        concat!("Alignment of ", stringify!(jvalue))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).z) as usize - ptr as usize },
        0usize,
        concat!("Offset of field: ", stringify!(jvalue), "::", stringify!(z))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).b) as usize - ptr as usize },
        0usize,
        concat!("Offset of field: ", stringify!(jvalue), "::", stringify!(b))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).c) as usize - ptr as usize },
        0usize,
        concat!("Offset of field: ", stringify!(jvalue), "::", stringify!(c))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).s) as usize - ptr as usize },
        0usize,
        concat!("Offset of field: ", stringify!(jvalue), "::", stringify!(s))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).i) as usize - ptr as usize },
        0usize,
        concat!("Offset of field: ", stringify!(jvalue), "::", stringify!(i))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).j) as usize - ptr as usize },
        0usize,
        concat!("Offset of field: ", stringify!(jvalue), "::", stringify!(j))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).f) as usize - ptr as usize },
        0usize,
        concat!("Offset of field: ", stringify!(jvalue), "::", stringify!(f))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).d) as usize - ptr as usize },
        0usize,
        concat!("Offset of field: ", stringify!(jvalue), "::", stringify!(d))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).l) as usize - ptr as usize },
        0usize,
        concat!("Offset of field: ", stringify!(jvalue), "::", stringify!(l))
    );
}
pub const jobjectRefType_JNIInvalidRefType: jobjectRefType = 0;
pub const jobjectRefType_JNILocalRefType: jobjectRefType = 1;
pub const jobjectRefType_JNIGlobalRefType: jobjectRefType = 2;
pub const jobjectRefType_JNIWeakGlobalRefType: jobjectRefType = 3;
pub type jobjectRefType = ::std::os::raw::c_uint;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct JNINativeMethod {
    pub name: *const ::std::os::raw::c_char,
    pub signature: *const ::std::os::raw::c_char,
    pub fnPtr: *mut ::std::os::raw::c_void,
}
#[test]
fn bindgen_test_layout_JNINativeMethod() {
    const UNINIT: ::std::mem::MaybeUninit<JNINativeMethod> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<JNINativeMethod>(),
        24usize,
        concat!("Size of: ", stringify!(JNINativeMethod))
    );
    assert_eq!(
        ::std::mem::align_of::<JNINativeMethod>(),
        8usize,
        concat!("Alignment of ", stringify!(JNINativeMethod))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).name) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeMethod),
            "::",
            stringify!(name)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).signature) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeMethod),
            "::",
            stringify!(signature)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).fnPtr) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeMethod),
            "::",
            stringify!(fnPtr)
        )
    );
}
pub type C_JNIEnv = *const JNINativeInterface;
pub type JNIEnv = *const JNINativeInterface;
pub type JavaVM = *const JNIInvokeInterface;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct JNINativeInterface {
    pub reserved0: *mut ::std::os::raw::c_void,
    pub reserved1: *mut ::std::os::raw::c_void,
    pub reserved2: *mut ::std::os::raw::c_void,
    pub reserved3: *mut ::std::os::raw::c_void,
    pub GetVersion: ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv) -> jint>,
    pub DefineClass: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: *const ::std::os::raw::c_char,
            arg3: jobject,
            arg4: *const jbyte,
            arg5: jsize,
        ) -> jclass,
    >,
    pub FindClass: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: *const ::std::os::raw::c_char) -> jclass,
    >,
    pub FromReflectedMethod:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jmethodID>,
    pub FromReflectedField:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jfieldID>,
    pub ToReflectedMethod: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: jboolean,
        ) -> jobject,
    >,
    pub GetSuperclass:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass) -> jclass>,
    pub IsAssignableFrom: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jclass) -> jboolean,
    >,
    pub ToReflectedField: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jfieldID,
            arg4: jboolean,
        ) -> jobject,
    >,
    pub Throw:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jthrowable) -> jint>,
    pub ThrowNew: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: *const ::std::os::raw::c_char,
        ) -> jint,
    >,
    pub ExceptionOccurred:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv) -> jthrowable>,
    pub ExceptionDescribe: ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv)>,
    pub ExceptionClear: ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv)>,
    pub FatalError: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: *const ::std::os::raw::c_char),
    >,
    pub PushLocalFrame:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jint) -> jint>,
    pub PopLocalFrame:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jobject>,
    pub NewGlobalRef:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jobject>,
    pub DeleteGlobalRef:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject)>,
    pub DeleteLocalRef:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject)>,
    pub IsSameObject: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jobject) -> jboolean,
    >,
    pub NewLocalRef:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jobject>,
    pub EnsureLocalCapacity:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jint) -> jint>,
    pub AllocObject:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass) -> jobject>,
    pub NewObject: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...) -> jobject,
    >,
    pub NewObjectV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jobject,
    >,
    pub NewObjectA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jobject,
    >,
    pub GetObjectClass:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jclass>,
    pub IsInstanceOf: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jclass) -> jboolean,
    >,
    pub GetMethodID: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: *const ::std::os::raw::c_char,
            arg4: *const ::std::os::raw::c_char,
        ) -> jmethodID,
    >,
    pub CallObjectMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jmethodID, ...) -> jobject,
    >,
    pub CallObjectMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jobject,
    >,
    pub CallObjectMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jobject,
    >,
    pub CallBooleanMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jmethodID, ...) -> jboolean,
    >,
    pub CallBooleanMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jboolean,
    >,
    pub CallBooleanMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jboolean,
    >,
    pub CallByteMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jmethodID, ...) -> jbyte,
    >,
    pub CallByteMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jbyte,
    >,
    pub CallByteMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jbyte,
    >,
    pub CallCharMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jmethodID, ...) -> jchar,
    >,
    pub CallCharMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jchar,
    >,
    pub CallCharMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jchar,
    >,
    pub CallShortMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jmethodID, ...) -> jshort,
    >,
    pub CallShortMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jshort,
    >,
    pub CallShortMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jshort,
    >,
    pub CallIntMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jmethodID, ...) -> jint,
    >,
    pub CallIntMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jint,
    >,
    pub CallIntMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jint,
    >,
    pub CallLongMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jmethodID, ...) -> jlong,
    >,
    pub CallLongMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jlong,
    >,
    pub CallLongMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jlong,
    >,
    pub CallFloatMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jmethodID, ...) -> jfloat,
    >,
    pub CallFloatMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jfloat,
    >,
    pub CallFloatMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jfloat,
    >,
    pub CallDoubleMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jmethodID, ...) -> jdouble,
    >,
    pub CallDoubleMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jdouble,
    >,
    pub CallDoubleMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jdouble,
    >,
    pub CallVoidMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jmethodID, ...),
    >,
    pub CallVoidMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ),
    >,
    pub CallVoidMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jmethodID,
            arg4: *const jvalue,
        ),
    >,
    pub CallNonvirtualObjectMethod: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            ...
        ) -> jobject,
    >,
    pub CallNonvirtualObjectMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *mut __va_list_tag,
        ) -> jobject,
    >,
    pub CallNonvirtualObjectMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *const jvalue,
        ) -> jobject,
    >,
    pub CallNonvirtualBooleanMethod: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            ...
        ) -> jboolean,
    >,
    pub CallNonvirtualBooleanMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *mut __va_list_tag,
        ) -> jboolean,
    >,
    pub CallNonvirtualBooleanMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *const jvalue,
        ) -> jboolean,
    >,
    pub CallNonvirtualByteMethod: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            ...
        ) -> jbyte,
    >,
    pub CallNonvirtualByteMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *mut __va_list_tag,
        ) -> jbyte,
    >,
    pub CallNonvirtualByteMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *const jvalue,
        ) -> jbyte,
    >,
    pub CallNonvirtualCharMethod: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            ...
        ) -> jchar,
    >,
    pub CallNonvirtualCharMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *mut __va_list_tag,
        ) -> jchar,
    >,
    pub CallNonvirtualCharMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *const jvalue,
        ) -> jchar,
    >,
    pub CallNonvirtualShortMethod: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            ...
        ) -> jshort,
    >,
    pub CallNonvirtualShortMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *mut __va_list_tag,
        ) -> jshort,
    >,
    pub CallNonvirtualShortMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *const jvalue,
        ) -> jshort,
    >,
    pub CallNonvirtualIntMethod: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            ...
        ) -> jint,
    >,
    pub CallNonvirtualIntMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *mut __va_list_tag,
        ) -> jint,
    >,
    pub CallNonvirtualIntMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *const jvalue,
        ) -> jint,
    >,
    pub CallNonvirtualLongMethod: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            ...
        ) -> jlong,
    >,
    pub CallNonvirtualLongMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *mut __va_list_tag,
        ) -> jlong,
    >,
    pub CallNonvirtualLongMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *const jvalue,
        ) -> jlong,
    >,
    pub CallNonvirtualFloatMethod: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            ...
        ) -> jfloat,
    >,
    pub CallNonvirtualFloatMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *mut __va_list_tag,
        ) -> jfloat,
    >,
    pub CallNonvirtualFloatMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *const jvalue,
        ) -> jfloat,
    >,
    pub CallNonvirtualDoubleMethod: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            ...
        ) -> jdouble,
    >,
    pub CallNonvirtualDoubleMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *mut __va_list_tag,
        ) -> jdouble,
    >,
    pub CallNonvirtualDoubleMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *const jvalue,
        ) -> jdouble,
    >,
    pub CallNonvirtualVoidMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jclass, arg4: jmethodID, ...),
    >,
    pub CallNonvirtualVoidMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *mut __va_list_tag,
        ),
    >,
    pub CallNonvirtualVoidMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jobject,
            arg3: jclass,
            arg4: jmethodID,
            arg5: *const jvalue,
        ),
    >,
    pub GetFieldID: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: *const ::std::os::raw::c_char,
            arg4: *const ::std::os::raw::c_char,
        ) -> jfieldID,
    >,
    pub GetObjectField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID) -> jobject,
    >,
    pub GetBooleanField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID) -> jboolean,
    >,
    pub GetByteField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID) -> jbyte,
    >,
    pub GetCharField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID) -> jchar,
    >,
    pub GetShortField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID) -> jshort,
    >,
    pub GetIntField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID) -> jint,
    >,
    pub GetLongField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID) -> jlong,
    >,
    pub GetFloatField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID) -> jfloat,
    >,
    pub GetDoubleField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID) -> jdouble,
    >,
    pub SetObjectField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID, arg4: jobject),
    >,
    pub SetBooleanField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID, arg4: jboolean),
    >,
    pub SetByteField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID, arg4: jbyte),
    >,
    pub SetCharField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID, arg4: jchar),
    >,
    pub SetShortField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID, arg4: jshort),
    >,
    pub SetIntField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID, arg4: jint),
    >,
    pub SetLongField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID, arg4: jlong),
    >,
    pub SetFloatField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID, arg4: jfloat),
    >,
    pub SetDoubleField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject, arg3: jfieldID, arg4: jdouble),
    >,
    pub GetStaticMethodID: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: *const ::std::os::raw::c_char,
            arg4: *const ::std::os::raw::c_char,
        ) -> jmethodID,
    >,
    pub CallStaticObjectMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...) -> jobject,
    >,
    pub CallStaticObjectMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jobject,
    >,
    pub CallStaticObjectMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jobject,
    >,
    pub CallStaticBooleanMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...) -> jboolean,
    >,
    pub CallStaticBooleanMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jboolean,
    >,
    pub CallStaticBooleanMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jboolean,
    >,
    pub CallStaticByteMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...) -> jbyte,
    >,
    pub CallStaticByteMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jbyte,
    >,
    pub CallStaticByteMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jbyte,
    >,
    pub CallStaticCharMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...) -> jchar,
    >,
    pub CallStaticCharMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jchar,
    >,
    pub CallStaticCharMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jchar,
    >,
    pub CallStaticShortMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...) -> jshort,
    >,
    pub CallStaticShortMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jshort,
    >,
    pub CallStaticShortMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jshort,
    >,
    pub CallStaticIntMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...) -> jint,
    >,
    pub CallStaticIntMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jint,
    >,
    pub CallStaticIntMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jint,
    >,
    pub CallStaticLongMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...) -> jlong,
    >,
    pub CallStaticLongMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jlong,
    >,
    pub CallStaticLongMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jlong,
    >,
    pub CallStaticFloatMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...) -> jfloat,
    >,
    pub CallStaticFloatMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jfloat,
    >,
    pub CallStaticFloatMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jfloat,
    >,
    pub CallStaticDoubleMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...) -> jdouble,
    >,
    pub CallStaticDoubleMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ) -> jdouble,
    >,
    pub CallStaticDoubleMethodA: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *const jvalue,
        ) -> jdouble,
    >,
    pub CallStaticVoidMethod: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, ...),
    >,
    pub CallStaticVoidMethodV: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: jmethodID,
            arg4: *mut __va_list_tag,
        ),
    >,
    pub CallStaticVoidMethodA: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jmethodID, arg4: *const jvalue),
    >,
    pub GetStaticFieldID: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: *const ::std::os::raw::c_char,
            arg4: *const ::std::os::raw::c_char,
        ) -> jfieldID,
    >,
    pub GetStaticObjectField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID) -> jobject,
    >,
    pub GetStaticBooleanField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID) -> jboolean,
    >,
    pub GetStaticByteField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID) -> jbyte,
    >,
    pub GetStaticCharField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID) -> jchar,
    >,
    pub GetStaticShortField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID) -> jshort,
    >,
    pub GetStaticIntField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID) -> jint,
    >,
    pub GetStaticLongField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID) -> jlong,
    >,
    pub GetStaticFloatField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID) -> jfloat,
    >,
    pub GetStaticDoubleField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID) -> jdouble,
    >,
    pub SetStaticObjectField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID, arg4: jobject),
    >,
    pub SetStaticBooleanField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID, arg4: jboolean),
    >,
    pub SetStaticByteField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID, arg4: jbyte),
    >,
    pub SetStaticCharField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID, arg4: jchar),
    >,
    pub SetStaticShortField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID, arg4: jshort),
    >,
    pub SetStaticIntField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID, arg4: jint),
    >,
    pub SetStaticLongField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID, arg4: jlong),
    >,
    pub SetStaticFloatField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID, arg4: jfloat),
    >,
    pub SetStaticDoubleField: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass, arg3: jfieldID, arg4: jdouble),
    >,
    pub NewString: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: *const jchar, arg3: jsize) -> jstring,
    >,
    pub GetStringLength:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jstring) -> jsize>,
    pub GetStringChars: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jstring, arg3: *mut jboolean) -> *const jchar,
    >,
    pub ReleaseStringChars: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jstring, arg3: *const jchar),
    >,
    pub NewStringUTF: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: *const ::std::os::raw::c_char) -> jstring,
    >,
    pub GetStringUTFLength:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jstring) -> jsize>,
    pub GetStringUTFChars: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jstring,
            arg3: *mut jboolean,
        ) -> *const ::std::os::raw::c_char,
    >,
    pub ReleaseStringUTFChars: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jstring, arg3: *const ::std::os::raw::c_char),
    >,
    pub GetArrayLength:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jarray) -> jsize>,
    pub NewObjectArray: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jsize,
            arg3: jclass,
            arg4: jobject,
        ) -> jobjectArray,
    >,
    pub GetObjectArrayElement: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobjectArray, arg3: jsize) -> jobject,
    >,
    pub SetObjectArrayElement: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobjectArray, arg3: jsize, arg4: jobject),
    >,
    pub NewBooleanArray: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jsize) -> jbooleanArray,
    >,
    pub NewByteArray:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jsize) -> jbyteArray>,
    pub NewCharArray:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jsize) -> jcharArray>,
    pub NewShortArray:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jsize) -> jshortArray>,
    pub NewIntArray:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jsize) -> jintArray>,
    pub NewLongArray:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jsize) -> jlongArray>,
    pub NewFloatArray:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jsize) -> jfloatArray>,
    pub NewDoubleArray:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jsize) -> jdoubleArray>,
    pub GetBooleanArrayElements: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jbooleanArray,
            arg3: *mut jboolean,
        ) -> *mut jboolean,
    >,
    pub GetByteArrayElements: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jbyteArray,
            arg3: *mut jboolean,
        ) -> *mut jbyte,
    >,
    pub GetCharArrayElements: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jcharArray,
            arg3: *mut jboolean,
        ) -> *mut jchar,
    >,
    pub GetShortArrayElements: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jshortArray,
            arg3: *mut jboolean,
        ) -> *mut jshort,
    >,
    pub GetIntArrayElements: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jintArray, arg3: *mut jboolean) -> *mut jint,
    >,
    pub GetLongArrayElements: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jlongArray,
            arg3: *mut jboolean,
        ) -> *mut jlong,
    >,
    pub GetFloatArrayElements: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jfloatArray,
            arg3: *mut jboolean,
        ) -> *mut jfloat,
    >,
    pub GetDoubleArrayElements: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jdoubleArray,
            arg3: *mut jboolean,
        ) -> *mut jdouble,
    >,
    pub ReleaseBooleanArrayElements: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jbooleanArray,
            arg3: *mut jboolean,
            arg4: jint,
        ),
    >,
    pub ReleaseByteArrayElements: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jbyteArray, arg3: *mut jbyte, arg4: jint),
    >,
    pub ReleaseCharArrayElements: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jcharArray, arg3: *mut jchar, arg4: jint),
    >,
    pub ReleaseShortArrayElements: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jshortArray, arg3: *mut jshort, arg4: jint),
    >,
    pub ReleaseIntArrayElements: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jintArray, arg3: *mut jint, arg4: jint),
    >,
    pub ReleaseLongArrayElements: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jlongArray, arg3: *mut jlong, arg4: jint),
    >,
    pub ReleaseFloatArrayElements: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jfloatArray, arg3: *mut jfloat, arg4: jint),
    >,
    pub ReleaseDoubleArrayElements: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jdoubleArray, arg3: *mut jdouble, arg4: jint),
    >,
    pub GetBooleanArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jbooleanArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *mut jboolean,
        ),
    >,
    pub GetByteArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jbyteArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *mut jbyte,
        ),
    >,
    pub GetCharArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jcharArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *mut jchar,
        ),
    >,
    pub GetShortArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jshortArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *mut jshort,
        ),
    >,
    pub GetIntArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jintArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *mut jint,
        ),
    >,
    pub GetLongArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jlongArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *mut jlong,
        ),
    >,
    pub GetFloatArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jfloatArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *mut jfloat,
        ),
    >,
    pub GetDoubleArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jdoubleArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *mut jdouble,
        ),
    >,
    pub SetBooleanArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jbooleanArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *const jboolean,
        ),
    >,
    pub SetByteArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jbyteArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *const jbyte,
        ),
    >,
    pub SetCharArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jcharArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *const jchar,
        ),
    >,
    pub SetShortArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jshortArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *const jshort,
        ),
    >,
    pub SetIntArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jintArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *const jint,
        ),
    >,
    pub SetLongArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jlongArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *const jlong,
        ),
    >,
    pub SetFloatArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jfloatArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *const jfloat,
        ),
    >,
    pub SetDoubleArrayRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jdoubleArray,
            arg3: jsize,
            arg4: jsize,
            arg5: *const jdouble,
        ),
    >,
    pub RegisterNatives: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jclass,
            arg3: *const JNINativeMethod,
            arg4: jint,
        ) -> jint,
    >,
    pub UnregisterNatives:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jclass) -> jint>,
    pub MonitorEnter:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jint>,
    pub MonitorExit:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jint>,
    pub GetJavaVM: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: *mut *mut JavaVM) -> jint,
    >,
    pub GetStringRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jstring,
            arg3: jsize,
            arg4: jsize,
            arg5: *mut jchar,
        ),
    >,
    pub GetStringUTFRegion: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jstring,
            arg3: jsize,
            arg4: jsize,
            arg5: *mut ::std::os::raw::c_char,
        ),
    >,
    pub GetPrimitiveArrayCritical: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jarray,
            arg3: *mut jboolean,
        ) -> *mut ::std::os::raw::c_void,
    >,
    pub ReleasePrimitiveArrayCritical: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: jarray,
            arg3: *mut ::std::os::raw::c_void,
            arg4: jint,
        ),
    >,
    pub GetStringCritical: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jstring, arg3: *mut jboolean) -> *const jchar,
    >,
    pub ReleaseStringCritical: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jstring, arg3: *const jchar),
    >,
    pub NewWeakGlobalRef:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jweak>,
    pub DeleteWeakGlobalRef:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jweak)>,
    pub ExceptionCheck: ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv) -> jboolean>,
    pub NewDirectByteBuffer: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JNIEnv,
            arg2: *mut ::std::os::raw::c_void,
            arg3: jlong,
        ) -> jobject,
    >,
    pub GetDirectBufferAddress: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> *mut ::std::os::raw::c_void,
    >,
    pub GetDirectBufferCapacity:
        ::std::option::Option<unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jlong>,
    pub GetObjectRefType: ::std::option::Option<
        unsafe extern "C" fn(arg1: *mut JNIEnv, arg2: jobject) -> jobjectRefType,
    >,
}
#[test]
fn bindgen_test_layout_JNINativeInterface() {
    const UNINIT: ::std::mem::MaybeUninit<JNINativeInterface> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<JNINativeInterface>(),
        1864usize,
        concat!("Size of: ", stringify!(JNINativeInterface))
    );
    assert_eq!(
        ::std::mem::align_of::<JNINativeInterface>(),
        8usize,
        concat!("Alignment of ", stringify!(JNINativeInterface))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reserved0) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(reserved0)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reserved1) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(reserved1)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reserved2) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(reserved2)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reserved3) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(reserved3)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetVersion) as usize - ptr as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetVersion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).DefineClass) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(DefineClass)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).FindClass) as usize - ptr as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(FindClass)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).FromReflectedMethod) as usize - ptr as usize },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(FromReflectedMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).FromReflectedField) as usize - ptr as usize },
        64usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(FromReflectedField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ToReflectedMethod) as usize - ptr as usize },
        72usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ToReflectedMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetSuperclass) as usize - ptr as usize },
        80usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetSuperclass)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).IsAssignableFrom) as usize - ptr as usize },
        88usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(IsAssignableFrom)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ToReflectedField) as usize - ptr as usize },
        96usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ToReflectedField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).Throw) as usize - ptr as usize },
        104usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(Throw)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ThrowNew) as usize - ptr as usize },
        112usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ThrowNew)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ExceptionOccurred) as usize - ptr as usize },
        120usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ExceptionOccurred)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ExceptionDescribe) as usize - ptr as usize },
        128usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ExceptionDescribe)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ExceptionClear) as usize - ptr as usize },
        136usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ExceptionClear)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).FatalError) as usize - ptr as usize },
        144usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(FatalError)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).PushLocalFrame) as usize - ptr as usize },
        152usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(PushLocalFrame)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).PopLocalFrame) as usize - ptr as usize },
        160usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(PopLocalFrame)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewGlobalRef) as usize - ptr as usize },
        168usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewGlobalRef)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).DeleteGlobalRef) as usize - ptr as usize },
        176usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(DeleteGlobalRef)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).DeleteLocalRef) as usize - ptr as usize },
        184usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(DeleteLocalRef)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).IsSameObject) as usize - ptr as usize },
        192usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(IsSameObject)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewLocalRef) as usize - ptr as usize },
        200usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewLocalRef)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).EnsureLocalCapacity) as usize - ptr as usize },
        208usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(EnsureLocalCapacity)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).AllocObject) as usize - ptr as usize },
        216usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(AllocObject)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewObject) as usize - ptr as usize },
        224usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewObject)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewObjectV) as usize - ptr as usize },
        232usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewObjectV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewObjectA) as usize - ptr as usize },
        240usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewObjectA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetObjectClass) as usize - ptr as usize },
        248usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetObjectClass)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).IsInstanceOf) as usize - ptr as usize },
        256usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(IsInstanceOf)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetMethodID) as usize - ptr as usize },
        264usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetMethodID)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallObjectMethod) as usize - ptr as usize },
        272usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallObjectMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallObjectMethodV) as usize - ptr as usize },
        280usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallObjectMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallObjectMethodA) as usize - ptr as usize },
        288usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallObjectMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallBooleanMethod) as usize - ptr as usize },
        296usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallBooleanMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallBooleanMethodV) as usize - ptr as usize },
        304usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallBooleanMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallBooleanMethodA) as usize - ptr as usize },
        312usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallBooleanMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallByteMethod) as usize - ptr as usize },
        320usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallByteMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallByteMethodV) as usize - ptr as usize },
        328usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallByteMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallByteMethodA) as usize - ptr as usize },
        336usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallByteMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallCharMethod) as usize - ptr as usize },
        344usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallCharMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallCharMethodV) as usize - ptr as usize },
        352usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallCharMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallCharMethodA) as usize - ptr as usize },
        360usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallCharMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallShortMethod) as usize - ptr as usize },
        368usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallShortMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallShortMethodV) as usize - ptr as usize },
        376usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallShortMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallShortMethodA) as usize - ptr as usize },
        384usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallShortMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallIntMethod) as usize - ptr as usize },
        392usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallIntMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallIntMethodV) as usize - ptr as usize },
        400usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallIntMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallIntMethodA) as usize - ptr as usize },
        408usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallIntMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallLongMethod) as usize - ptr as usize },
        416usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallLongMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallLongMethodV) as usize - ptr as usize },
        424usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallLongMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallLongMethodA) as usize - ptr as usize },
        432usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallLongMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallFloatMethod) as usize - ptr as usize },
        440usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallFloatMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallFloatMethodV) as usize - ptr as usize },
        448usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallFloatMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallFloatMethodA) as usize - ptr as usize },
        456usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallFloatMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallDoubleMethod) as usize - ptr as usize },
        464usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallDoubleMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallDoubleMethodV) as usize - ptr as usize },
        472usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallDoubleMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallDoubleMethodA) as usize - ptr as usize },
        480usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallDoubleMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallVoidMethod) as usize - ptr as usize },
        488usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallVoidMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallVoidMethodV) as usize - ptr as usize },
        496usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallVoidMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallVoidMethodA) as usize - ptr as usize },
        504usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallVoidMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualObjectMethod) as usize - ptr as usize },
        512usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualObjectMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualObjectMethodV) as usize - ptr as usize },
        520usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualObjectMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualObjectMethodA) as usize - ptr as usize },
        528usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualObjectMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualBooleanMethod) as usize - ptr as usize },
        536usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualBooleanMethod)
        )
    );
    assert_eq!(
        unsafe {
            ::std::ptr::addr_of!((*ptr).CallNonvirtualBooleanMethodV) as usize - ptr as usize
        },
        544usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualBooleanMethodV)
        )
    );
    assert_eq!(
        unsafe {
            ::std::ptr::addr_of!((*ptr).CallNonvirtualBooleanMethodA) as usize - ptr as usize
        },
        552usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualBooleanMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualByteMethod) as usize - ptr as usize },
        560usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualByteMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualByteMethodV) as usize - ptr as usize },
        568usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualByteMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualByteMethodA) as usize - ptr as usize },
        576usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualByteMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualCharMethod) as usize - ptr as usize },
        584usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualCharMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualCharMethodV) as usize - ptr as usize },
        592usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualCharMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualCharMethodA) as usize - ptr as usize },
        600usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualCharMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualShortMethod) as usize - ptr as usize },
        608usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualShortMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualShortMethodV) as usize - ptr as usize },
        616usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualShortMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualShortMethodA) as usize - ptr as usize },
        624usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualShortMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualIntMethod) as usize - ptr as usize },
        632usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualIntMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualIntMethodV) as usize - ptr as usize },
        640usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualIntMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualIntMethodA) as usize - ptr as usize },
        648usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualIntMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualLongMethod) as usize - ptr as usize },
        656usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualLongMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualLongMethodV) as usize - ptr as usize },
        664usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualLongMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualLongMethodA) as usize - ptr as usize },
        672usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualLongMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualFloatMethod) as usize - ptr as usize },
        680usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualFloatMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualFloatMethodV) as usize - ptr as usize },
        688usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualFloatMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualFloatMethodA) as usize - ptr as usize },
        696usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualFloatMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualDoubleMethod) as usize - ptr as usize },
        704usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualDoubleMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualDoubleMethodV) as usize - ptr as usize },
        712usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualDoubleMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualDoubleMethodA) as usize - ptr as usize },
        720usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualDoubleMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualVoidMethod) as usize - ptr as usize },
        728usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualVoidMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualVoidMethodV) as usize - ptr as usize },
        736usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualVoidMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallNonvirtualVoidMethodA) as usize - ptr as usize },
        744usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallNonvirtualVoidMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetFieldID) as usize - ptr as usize },
        752usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetFieldID)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetObjectField) as usize - ptr as usize },
        760usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetObjectField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetBooleanField) as usize - ptr as usize },
        768usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetBooleanField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetByteField) as usize - ptr as usize },
        776usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetByteField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetCharField) as usize - ptr as usize },
        784usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetCharField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetShortField) as usize - ptr as usize },
        792usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetShortField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetIntField) as usize - ptr as usize },
        800usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetIntField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetLongField) as usize - ptr as usize },
        808usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetLongField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetFloatField) as usize - ptr as usize },
        816usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetFloatField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetDoubleField) as usize - ptr as usize },
        824usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetDoubleField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetObjectField) as usize - ptr as usize },
        832usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetObjectField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetBooleanField) as usize - ptr as usize },
        840usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetBooleanField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetByteField) as usize - ptr as usize },
        848usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetByteField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetCharField) as usize - ptr as usize },
        856usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetCharField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetShortField) as usize - ptr as usize },
        864usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetShortField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetIntField) as usize - ptr as usize },
        872usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetIntField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetLongField) as usize - ptr as usize },
        880usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetLongField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetFloatField) as usize - ptr as usize },
        888usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetFloatField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetDoubleField) as usize - ptr as usize },
        896usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetDoubleField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticMethodID) as usize - ptr as usize },
        904usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticMethodID)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticObjectMethod) as usize - ptr as usize },
        912usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticObjectMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticObjectMethodV) as usize - ptr as usize },
        920usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticObjectMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticObjectMethodA) as usize - ptr as usize },
        928usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticObjectMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticBooleanMethod) as usize - ptr as usize },
        936usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticBooleanMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticBooleanMethodV) as usize - ptr as usize },
        944usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticBooleanMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticBooleanMethodA) as usize - ptr as usize },
        952usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticBooleanMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticByteMethod) as usize - ptr as usize },
        960usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticByteMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticByteMethodV) as usize - ptr as usize },
        968usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticByteMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticByteMethodA) as usize - ptr as usize },
        976usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticByteMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticCharMethod) as usize - ptr as usize },
        984usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticCharMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticCharMethodV) as usize - ptr as usize },
        992usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticCharMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticCharMethodA) as usize - ptr as usize },
        1000usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticCharMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticShortMethod) as usize - ptr as usize },
        1008usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticShortMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticShortMethodV) as usize - ptr as usize },
        1016usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticShortMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticShortMethodA) as usize - ptr as usize },
        1024usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticShortMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticIntMethod) as usize - ptr as usize },
        1032usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticIntMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticIntMethodV) as usize - ptr as usize },
        1040usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticIntMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticIntMethodA) as usize - ptr as usize },
        1048usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticIntMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticLongMethod) as usize - ptr as usize },
        1056usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticLongMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticLongMethodV) as usize - ptr as usize },
        1064usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticLongMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticLongMethodA) as usize - ptr as usize },
        1072usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticLongMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticFloatMethod) as usize - ptr as usize },
        1080usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticFloatMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticFloatMethodV) as usize - ptr as usize },
        1088usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticFloatMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticFloatMethodA) as usize - ptr as usize },
        1096usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticFloatMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticDoubleMethod) as usize - ptr as usize },
        1104usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticDoubleMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticDoubleMethodV) as usize - ptr as usize },
        1112usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticDoubleMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticDoubleMethodA) as usize - ptr as usize },
        1120usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticDoubleMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticVoidMethod) as usize - ptr as usize },
        1128usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticVoidMethod)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticVoidMethodV) as usize - ptr as usize },
        1136usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticVoidMethodV)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).CallStaticVoidMethodA) as usize - ptr as usize },
        1144usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(CallStaticVoidMethodA)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticFieldID) as usize - ptr as usize },
        1152usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticFieldID)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticObjectField) as usize - ptr as usize },
        1160usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticObjectField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticBooleanField) as usize - ptr as usize },
        1168usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticBooleanField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticByteField) as usize - ptr as usize },
        1176usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticByteField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticCharField) as usize - ptr as usize },
        1184usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticCharField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticShortField) as usize - ptr as usize },
        1192usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticShortField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticIntField) as usize - ptr as usize },
        1200usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticIntField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticLongField) as usize - ptr as usize },
        1208usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticLongField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticFloatField) as usize - ptr as usize },
        1216usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticFloatField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStaticDoubleField) as usize - ptr as usize },
        1224usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStaticDoubleField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetStaticObjectField) as usize - ptr as usize },
        1232usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetStaticObjectField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetStaticBooleanField) as usize - ptr as usize },
        1240usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetStaticBooleanField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetStaticByteField) as usize - ptr as usize },
        1248usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetStaticByteField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetStaticCharField) as usize - ptr as usize },
        1256usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetStaticCharField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetStaticShortField) as usize - ptr as usize },
        1264usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetStaticShortField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetStaticIntField) as usize - ptr as usize },
        1272usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetStaticIntField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetStaticLongField) as usize - ptr as usize },
        1280usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetStaticLongField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetStaticFloatField) as usize - ptr as usize },
        1288usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetStaticFloatField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetStaticDoubleField) as usize - ptr as usize },
        1296usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetStaticDoubleField)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewString) as usize - ptr as usize },
        1304usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewString)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStringLength) as usize - ptr as usize },
        1312usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStringLength)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStringChars) as usize - ptr as usize },
        1320usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStringChars)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseStringChars) as usize - ptr as usize },
        1328usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseStringChars)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewStringUTF) as usize - ptr as usize },
        1336usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewStringUTF)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStringUTFLength) as usize - ptr as usize },
        1344usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStringUTFLength)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStringUTFChars) as usize - ptr as usize },
        1352usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStringUTFChars)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseStringUTFChars) as usize - ptr as usize },
        1360usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseStringUTFChars)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetArrayLength) as usize - ptr as usize },
        1368usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetArrayLength)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewObjectArray) as usize - ptr as usize },
        1376usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewObjectArray)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetObjectArrayElement) as usize - ptr as usize },
        1384usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetObjectArrayElement)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetObjectArrayElement) as usize - ptr as usize },
        1392usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetObjectArrayElement)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewBooleanArray) as usize - ptr as usize },
        1400usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewBooleanArray)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewByteArray) as usize - ptr as usize },
        1408usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewByteArray)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewCharArray) as usize - ptr as usize },
        1416usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewCharArray)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewShortArray) as usize - ptr as usize },
        1424usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewShortArray)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewIntArray) as usize - ptr as usize },
        1432usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewIntArray)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewLongArray) as usize - ptr as usize },
        1440usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewLongArray)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewFloatArray) as usize - ptr as usize },
        1448usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewFloatArray)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewDoubleArray) as usize - ptr as usize },
        1456usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewDoubleArray)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetBooleanArrayElements) as usize - ptr as usize },
        1464usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetBooleanArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetByteArrayElements) as usize - ptr as usize },
        1472usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetByteArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetCharArrayElements) as usize - ptr as usize },
        1480usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetCharArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetShortArrayElements) as usize - ptr as usize },
        1488usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetShortArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetIntArrayElements) as usize - ptr as usize },
        1496usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetIntArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetLongArrayElements) as usize - ptr as usize },
        1504usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetLongArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetFloatArrayElements) as usize - ptr as usize },
        1512usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetFloatArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetDoubleArrayElements) as usize - ptr as usize },
        1520usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetDoubleArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseBooleanArrayElements) as usize - ptr as usize },
        1528usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseBooleanArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseByteArrayElements) as usize - ptr as usize },
        1536usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseByteArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseCharArrayElements) as usize - ptr as usize },
        1544usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseCharArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseShortArrayElements) as usize - ptr as usize },
        1552usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseShortArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseIntArrayElements) as usize - ptr as usize },
        1560usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseIntArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseLongArrayElements) as usize - ptr as usize },
        1568usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseLongArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseFloatArrayElements) as usize - ptr as usize },
        1576usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseFloatArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseDoubleArrayElements) as usize - ptr as usize },
        1584usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseDoubleArrayElements)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetBooleanArrayRegion) as usize - ptr as usize },
        1592usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetBooleanArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetByteArrayRegion) as usize - ptr as usize },
        1600usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetByteArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetCharArrayRegion) as usize - ptr as usize },
        1608usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetCharArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetShortArrayRegion) as usize - ptr as usize },
        1616usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetShortArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetIntArrayRegion) as usize - ptr as usize },
        1624usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetIntArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetLongArrayRegion) as usize - ptr as usize },
        1632usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetLongArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetFloatArrayRegion) as usize - ptr as usize },
        1640usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetFloatArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetDoubleArrayRegion) as usize - ptr as usize },
        1648usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetDoubleArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetBooleanArrayRegion) as usize - ptr as usize },
        1656usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetBooleanArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetByteArrayRegion) as usize - ptr as usize },
        1664usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetByteArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetCharArrayRegion) as usize - ptr as usize },
        1672usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetCharArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetShortArrayRegion) as usize - ptr as usize },
        1680usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetShortArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetIntArrayRegion) as usize - ptr as usize },
        1688usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetIntArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetLongArrayRegion) as usize - ptr as usize },
        1696usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetLongArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetFloatArrayRegion) as usize - ptr as usize },
        1704usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetFloatArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).SetDoubleArrayRegion) as usize - ptr as usize },
        1712usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(SetDoubleArrayRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).RegisterNatives) as usize - ptr as usize },
        1720usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(RegisterNatives)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).UnregisterNatives) as usize - ptr as usize },
        1728usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(UnregisterNatives)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).MonitorEnter) as usize - ptr as usize },
        1736usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(MonitorEnter)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).MonitorExit) as usize - ptr as usize },
        1744usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(MonitorExit)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetJavaVM) as usize - ptr as usize },
        1752usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetJavaVM)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStringRegion) as usize - ptr as usize },
        1760usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStringRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStringUTFRegion) as usize - ptr as usize },
        1768usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStringUTFRegion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetPrimitiveArrayCritical) as usize - ptr as usize },
        1776usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetPrimitiveArrayCritical)
        )
    );
    assert_eq!(
        unsafe {
            ::std::ptr::addr_of!((*ptr).ReleasePrimitiveArrayCritical) as usize - ptr as usize
        },
        1784usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleasePrimitiveArrayCritical)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetStringCritical) as usize - ptr as usize },
        1792usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetStringCritical)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ReleaseStringCritical) as usize - ptr as usize },
        1800usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ReleaseStringCritical)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewWeakGlobalRef) as usize - ptr as usize },
        1808usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewWeakGlobalRef)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).DeleteWeakGlobalRef) as usize - ptr as usize },
        1816usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(DeleteWeakGlobalRef)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ExceptionCheck) as usize - ptr as usize },
        1824usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(ExceptionCheck)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).NewDirectByteBuffer) as usize - ptr as usize },
        1832usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(NewDirectByteBuffer)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetDirectBufferAddress) as usize - ptr as usize },
        1840usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetDirectBufferAddress)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetDirectBufferCapacity) as usize - ptr as usize },
        1848usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetDirectBufferCapacity)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetObjectRefType) as usize - ptr as usize },
        1856usize,
        concat!(
            "Offset of field: ",
            stringify!(JNINativeInterface),
            "::",
            stringify!(GetObjectRefType)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct _JNIEnv {
    pub functions: *const JNINativeInterface,
}
#[test]
fn bindgen_test_layout__JNIEnv() {
    const UNINIT: ::std::mem::MaybeUninit<_JNIEnv> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<_JNIEnv>(),
        8usize,
        concat!("Size of: ", stringify!(_JNIEnv))
    );
    assert_eq!(
        ::std::mem::align_of::<_JNIEnv>(),
        8usize,
        concat!("Alignment of ", stringify!(_JNIEnv))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).functions) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(_JNIEnv),
            "::",
            stringify!(functions)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct JNIInvokeInterface {
    pub reserved0: *mut ::std::os::raw::c_void,
    pub reserved1: *mut ::std::os::raw::c_void,
    pub reserved2: *mut ::std::os::raw::c_void,
    pub DestroyJavaVM: ::std::option::Option<unsafe extern "C" fn(arg1: *mut JavaVM) -> jint>,
    pub AttachCurrentThread: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JavaVM,
            arg2: *mut *mut JNIEnv,
            arg3: *mut ::std::os::raw::c_void,
        ) -> jint,
    >,
    pub DetachCurrentThread: ::std::option::Option<unsafe extern "C" fn(arg1: *mut JavaVM) -> jint>,
    pub GetEnv: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JavaVM,
            arg2: *mut *mut ::std::os::raw::c_void,
            arg3: jint,
        ) -> jint,
    >,
    pub AttachCurrentThreadAsDaemon: ::std::option::Option<
        unsafe extern "C" fn(
            arg1: *mut JavaVM,
            arg2: *mut *mut JNIEnv,
            arg3: *mut ::std::os::raw::c_void,
        ) -> jint,
    >,
}
#[test]
fn bindgen_test_layout_JNIInvokeInterface() {
    const UNINIT: ::std::mem::MaybeUninit<JNIInvokeInterface> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<JNIInvokeInterface>(),
        64usize,
        concat!("Size of: ", stringify!(JNIInvokeInterface))
    );
    assert_eq!(
        ::std::mem::align_of::<JNIInvokeInterface>(),
        8usize,
        concat!("Alignment of ", stringify!(JNIInvokeInterface))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reserved0) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(JNIInvokeInterface),
            "::",
            stringify!(reserved0)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reserved1) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(JNIInvokeInterface),
            "::",
            stringify!(reserved1)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reserved2) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(JNIInvokeInterface),
            "::",
            stringify!(reserved2)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).DestroyJavaVM) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(JNIInvokeInterface),
            "::",
            stringify!(DestroyJavaVM)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).AttachCurrentThread) as usize - ptr as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(JNIInvokeInterface),
            "::",
            stringify!(AttachCurrentThread)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).DetachCurrentThread) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(JNIInvokeInterface),
            "::",
            stringify!(DetachCurrentThread)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).GetEnv) as usize - ptr as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(JNIInvokeInterface),
            "::",
            stringify!(GetEnv)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).AttachCurrentThreadAsDaemon) as usize - ptr as usize },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(JNIInvokeInterface),
            "::",
            stringify!(AttachCurrentThreadAsDaemon)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct _JavaVM {
    pub functions: *const JNIInvokeInterface,
}
#[test]
fn bindgen_test_layout__JavaVM() {
    const UNINIT: ::std::mem::MaybeUninit<_JavaVM> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<_JavaVM>(),
        8usize,
        concat!("Size of: ", stringify!(_JavaVM))
    );
    assert_eq!(
        ::std::mem::align_of::<_JavaVM>(),
        8usize,
        concat!("Alignment of ", stringify!(_JavaVM))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).functions) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(_JavaVM),
            "::",
            stringify!(functions)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct JavaVMAttachArgs {
    pub version: jint,
    pub name: *const ::std::os::raw::c_char,
    pub group: jobject,
}
#[test]
fn bindgen_test_layout_JavaVMAttachArgs() {
    const UNINIT: ::std::mem::MaybeUninit<JavaVMAttachArgs> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<JavaVMAttachArgs>(),
        24usize,
        concat!("Size of: ", stringify!(JavaVMAttachArgs))
    );
    assert_eq!(
        ::std::mem::align_of::<JavaVMAttachArgs>(),
        8usize,
        concat!("Alignment of ", stringify!(JavaVMAttachArgs))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).version) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(JavaVMAttachArgs),
            "::",
            stringify!(version)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).name) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(JavaVMAttachArgs),
            "::",
            stringify!(name)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).group) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(JavaVMAttachArgs),
            "::",
            stringify!(group)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct JavaVMOption {
    pub optionString: *const ::std::os::raw::c_char,
    pub extraInfo: *mut ::std::os::raw::c_void,
}
#[test]
fn bindgen_test_layout_JavaVMOption() {
    const UNINIT: ::std::mem::MaybeUninit<JavaVMOption> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<JavaVMOption>(),
        16usize,
        concat!("Size of: ", stringify!(JavaVMOption))
    );
    assert_eq!(
        ::std::mem::align_of::<JavaVMOption>(),
        8usize,
        concat!("Alignment of ", stringify!(JavaVMOption))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).optionString) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(JavaVMOption),
            "::",
            stringify!(optionString)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).extraInfo) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(JavaVMOption),
            "::",
            stringify!(extraInfo)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct JavaVMInitArgs {
    pub version: jint,
    pub nOptions: jint,
    pub options: *mut JavaVMOption,
    pub ignoreUnrecognized: jboolean,
}
#[test]
fn bindgen_test_layout_JavaVMInitArgs() {
    const UNINIT: ::std::mem::MaybeUninit<JavaVMInitArgs> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<JavaVMInitArgs>(),
        24usize,
        concat!("Size of: ", stringify!(JavaVMInitArgs))
    );
    assert_eq!(
        ::std::mem::align_of::<JavaVMInitArgs>(),
        8usize,
        concat!("Alignment of ", stringify!(JavaVMInitArgs))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).version) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(JavaVMInitArgs),
            "::",
            stringify!(version)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).nOptions) as usize - ptr as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(JavaVMInitArgs),
            "::",
            stringify!(nOptions)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).options) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(JavaVMInitArgs),
            "::",
            stringify!(options)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).ignoreUnrecognized) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(JavaVMInitArgs),
            "::",
            stringify!(ignoreUnrecognized)
        )
    );
}
extern "C" {
    pub fn JNI_GetDefaultJavaVMInitArgs(arg1: *mut ::std::os::raw::c_void) -> jint;
}
extern "C" {
    pub fn JNI_CreateJavaVM(
        arg1: *mut *mut JavaVM,
        arg2: *mut *mut JNIEnv,
        arg3: *mut ::std::os::raw::c_void,
    ) -> jint;
}
extern "C" {
    pub fn JNI_GetCreatedJavaVMs(arg1: *mut *mut JavaVM, arg2: jsize, arg3: *mut jsize) -> jint;
}
extern "C" {
    pub fn JNI_OnLoad(vm: *mut JavaVM, reserved: *mut ::std::os::raw::c_void) -> jint;
}
extern "C" {
    pub fn JNI_OnUnload(vm: *mut JavaVM, reserved: *mut ::std::os::raw::c_void);
}
extern "C" {
    #[doc = " Return the AHardwareBuffer wrapped by a Java HardwareBuffer object.\n\n This method does not acquire any additional reference to the AHardwareBuffer\n that is returned. To keep the AHardwareBuffer alive after the Java\n HardwareBuffer object is closed, explicitly or by the garbage collector, be\n sure to use AHardwareBuffer_acquire() to acquire an additional reference.\n\n Available since API level 26."]
    pub fn AHardwareBuffer_fromHardwareBuffer(
        env: *mut JNIEnv,
        hardwareBufferObj: jobject,
    ) -> *mut AHardwareBuffer;
}
extern "C" {
    #[doc = " Return a new Java HardwareBuffer object that wraps the passed native\n AHardwareBuffer object. The Java HardwareBuffer will acquire a reference to\n the internal buffer and manage its lifetime. For example:\n\n <pre><code>\n AHardwareBuffer* buffer;\n AHardwareBuffer_allocate(..., &buffer);  // `buffer` has reference count 1\n jobject java_result = AHardwareBuffer_toHardwareBuffer(buffer);  // `buffer` has reference count 2.\n AHardwareBuffer_release(buffer); // `buffer` has reference count 1\n return result;  // The underlying buffer is kept alive by `java_result` and\n                 // will be set to 0 when it is closed on the Java side with\n                 // HardwareBuffer::close().\n </code></pre>\n\n Available since API level 26."]
    pub fn AHardwareBuffer_toHardwareBuffer(
        env: *mut JNIEnv,
        hardwareBuffer: *mut AHardwareBuffer,
    ) -> jobject;
}
#[doc = " For internal use only."]
pub const android_LogPriority_ANDROID_LOG_UNKNOWN: android_LogPriority = 0;
#[doc = " The default priority, for internal use only."]
pub const android_LogPriority_ANDROID_LOG_DEFAULT: android_LogPriority = 1;
#[doc = " Verbose logging. Should typically be disabled for a release apk."]
pub const android_LogPriority_ANDROID_LOG_VERBOSE: android_LogPriority = 2;
#[doc = " Debug logging. Should typically be disabled for a release apk."]
pub const android_LogPriority_ANDROID_LOG_DEBUG: android_LogPriority = 3;
#[doc = " Informational logging. Should typically be disabled for a release apk."]
pub const android_LogPriority_ANDROID_LOG_INFO: android_LogPriority = 4;
#[doc = " Warning logging. For use with recoverable failures."]
pub const android_LogPriority_ANDROID_LOG_WARN: android_LogPriority = 5;
#[doc = " Error logging. For use with unrecoverable failures."]
pub const android_LogPriority_ANDROID_LOG_ERROR: android_LogPriority = 6;
#[doc = " Fatal logging. For use when aborting."]
pub const android_LogPriority_ANDROID_LOG_FATAL: android_LogPriority = 7;
#[doc = " For internal use only."]
pub const android_LogPriority_ANDROID_LOG_SILENT: android_LogPriority = 8;
#[doc = " Android log priority values, in increasing order of priority."]
pub type android_LogPriority = ::std::os::raw::c_uint;
extern "C" {
    #[doc = " Writes the constant string `text` to the log, with priority `prio` and tag\n `tag`."]
    pub fn __android_log_write(
        prio: ::std::os::raw::c_int,
        tag: *const ::std::os::raw::c_char,
        text: *const ::std::os::raw::c_char,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Writes a formatted string to the log, with priority `prio` and tag `tag`.\n The details of formatting are the same as for\n [printf(3)](http://man7.org/linux/man-pages/man3/printf.3.html)."]
    pub fn __android_log_print(
        prio: ::std::os::raw::c_int,
        tag: *const ::std::os::raw::c_char,
        fmt: *const ::std::os::raw::c_char,
        ...
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Equivalent to `__android_log_print`, but taking a `va_list`.\n (If `__android_log_print` is like `printf`, this is like `vprintf`.)"]
    pub fn __android_log_vprint(
        prio: ::std::os::raw::c_int,
        tag: *const ::std::os::raw::c_char,
        fmt: *const ::std::os::raw::c_char,
        ap: *mut __va_list_tag,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Writes an assertion failure to the log (as `ANDROID_LOG_FATAL`) and to\n stderr, before calling\n [abort(3)](http://man7.org/linux/man-pages/man3/abort.3.html).\n\n If `fmt` is non-null, `cond` is unused. If `fmt` is null, the string\n `Assertion failed: %s` is used with `cond` as the string argument.\n If both `fmt` and `cond` are null, a default string is provided.\n\n Most callers should use\n [assert(3)](http://man7.org/linux/man-pages/man3/assert.3.html) from\n `&lt;assert.h&gt;` instead, or the `__assert` and `__assert2` functions\n provided by bionic if more control is needed. They support automatically\n including the source filename and line number more conveniently than this\n function."]
    pub fn __android_log_assert(
        cond: *const ::std::os::raw::c_char,
        tag: *const ::std::os::raw::c_char,
        fmt: *const ::std::os::raw::c_char,
        ...
    ) -> !;
}
pub const log_id_LOG_ID_MIN: log_id = 0;
#[doc = " The main log buffer. This is the only log buffer available to apps."]
pub const log_id_LOG_ID_MAIN: log_id = 0;
#[doc = " The radio log buffer."]
pub const log_id_LOG_ID_RADIO: log_id = 1;
#[doc = " The event log buffer."]
pub const log_id_LOG_ID_EVENTS: log_id = 2;
#[doc = " The system log buffer."]
pub const log_id_LOG_ID_SYSTEM: log_id = 3;
#[doc = " The crash log buffer."]
pub const log_id_LOG_ID_CRASH: log_id = 4;
#[doc = " The statistics log buffer."]
pub const log_id_LOG_ID_STATS: log_id = 5;
#[doc = " The security log buffer."]
pub const log_id_LOG_ID_SECURITY: log_id = 6;
#[doc = " The kernel log buffer."]
pub const log_id_LOG_ID_KERNEL: log_id = 7;
#[doc = " The kernel log buffer."]
pub const log_id_LOG_ID_MAX: log_id = 8;
#[doc = " Let the logging function choose the best log target."]
pub const log_id_LOG_ID_DEFAULT: log_id = 2147483647;
#[doc = " Identifies a specific log buffer for __android_log_buf_write()\n and __android_log_buf_print()."]
pub type log_id = ::std::os::raw::c_uint;
#[doc = " Identifies a specific log buffer for __android_log_buf_write()\n and __android_log_buf_print()."]
pub use self::log_id as log_id_t;
extern "C" {
    #[doc = " Writes the constant string `text` to the log buffer `id`,\n with priority `prio` and tag `tag`.\n\n Apps should use __android_log_write() instead."]
    pub fn __android_log_buf_write(
        bufID: ::std::os::raw::c_int,
        prio: ::std::os::raw::c_int,
        tag: *const ::std::os::raw::c_char,
        text: *const ::std::os::raw::c_char,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Writes a formatted string to log buffer `id`,\n with priority `prio` and tag `tag`.\n The details of formatting are the same as for\n [printf(3)](http://man7.org/linux/man-pages/man3/printf.3.html).\n\n Apps should use __android_log_print() instead."]
    pub fn __android_log_buf_print(
        bufID: ::std::os::raw::c_int,
        prio: ::std::os::raw::c_int,
        tag: *const ::std::os::raw::c_char,
        fmt: *const ::std::os::raw::c_char,
        ...
    ) -> ::std::os::raw::c_int;
}
#[doc = " Logger data struct used for writing log messages to liblog via __android_log_write_logger_data()\n and sending log messages to user defined loggers specified in __android_log_set_logger()."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct __android_log_message {
    #[doc = " Must be set to sizeof(__android_log_message) and is used for versioning."]
    pub struct_size: usize,
    #[doc = " {@link log_id_t} values."]
    pub buffer_id: i32,
    #[doc = " {@link android_LogPriority} values."]
    pub priority: i32,
    #[doc = " The tag for the log message."]
    pub tag: *const ::std::os::raw::c_char,
    #[doc = " Optional file name, may be set to nullptr."]
    pub file: *const ::std::os::raw::c_char,
    #[doc = " Optional line number, ignore if file is nullptr."]
    pub line: u32,
    #[doc = " The log message itself."]
    pub message: *const ::std::os::raw::c_char,
}
#[test]
fn bindgen_test_layout___android_log_message() {
    const UNINIT: ::std::mem::MaybeUninit<__android_log_message> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<__android_log_message>(),
        48usize,
        concat!("Size of: ", stringify!(__android_log_message))
    );
    assert_eq!(
        ::std::mem::align_of::<__android_log_message>(),
        8usize,
        concat!("Alignment of ", stringify!(__android_log_message))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).struct_size) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(__android_log_message),
            "::",
            stringify!(struct_size)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).buffer_id) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(__android_log_message),
            "::",
            stringify!(buffer_id)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).priority) as usize - ptr as usize },
        12usize,
        concat!(
            "Offset of field: ",
            stringify!(__android_log_message),
            "::",
            stringify!(priority)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).tag) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(__android_log_message),
            "::",
            stringify!(tag)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).file) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(__android_log_message),
            "::",
            stringify!(file)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).line) as usize - ptr as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(__android_log_message),
            "::",
            stringify!(line)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).message) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(__android_log_message),
            "::",
            stringify!(message)
        )
    );
}
#[doc = " Prototype for the 'logger' function that is called for every log message."]
pub type __android_logger_function =
    ::std::option::Option<unsafe extern "C" fn(log_message: *const __android_log_message)>;
#[doc = " Prototype for the 'abort' function that is called when liblog will abort due to\n __android_log_assert() failures."]
pub type __android_aborter_function =
    ::std::option::Option<unsafe extern "C" fn(abort_message: *const ::std::os::raw::c_char)>;
extern "C" {
    #[doc = " Writes the log message specified by log_message.  log_message includes additional file name and\n line number information that a logger may use.  log_message is versioned for backwards\n compatibility.\n This assumes that loggability has already been checked through __android_log_is_loggable().\n Higher level logging libraries, such as libbase, first check loggability, then format their\n buffers, then pass the message to liblog via this function, and therefore we do not want to\n duplicate the loggability check here.\n\n @param log_message the log message itself, see __android_log_message.\n\n Available since API level 30."]
    pub fn __android_log_write_log_message(log_message: *mut __android_log_message);
}
extern "C" {
    #[doc = " Sets a user defined logger function.  All log messages sent to liblog will be set to the\n function pointer specified by logger for processing.  It is not expected that log messages are\n already terminated with a new line.  This function should add new lines if required for line\n separation.\n\n @param logger the new function that will handle log messages.\n\n Available since API level 30."]
    pub fn __android_log_set_logger(logger: __android_logger_function);
}
extern "C" {
    #[doc = " Writes the log message to logd.  This is an __android_logger_function and can be provided to\n __android_log_set_logger().  It is the default logger when running liblog on a device.\n\n @param log_message the log message to write, see __android_log_message.\n\n Available since API level 30."]
    pub fn __android_log_logd_logger(log_message: *const __android_log_message);
}
extern "C" {
    #[doc = " Writes the log message to stderr.  This is an __android_logger_function and can be provided to\n __android_log_set_logger().  It is the default logger when running liblog on host.\n\n @param log_message the log message to write, see __android_log_message.\n\n Available since API level 30."]
    pub fn __android_log_stderr_logger(log_message: *const __android_log_message);
}
extern "C" {
    #[doc = " Sets a user defined aborter function that is called for __android_log_assert() failures.  This\n user defined aborter function is highly recommended to abort and be noreturn, but is not strictly\n required to.\n\n @param aborter the new aborter function, see __android_aborter_function.\n\n Available since API level 30."]
    pub fn __android_log_set_aborter(aborter: __android_aborter_function);
}
extern "C" {
    #[doc = " Calls the stored aborter function.  This allows for other logging libraries to use the same\n aborter function by calling this function in liblog.\n\n @param abort_message an additional message supplied when aborting, for example this is used to\n                      call android_set_abort_message() in __android_log_default_aborter().\n\n Available since API level 30."]
    pub fn __android_log_call_aborter(abort_message: *const ::std::os::raw::c_char);
}
extern "C" {
    #[doc = " Sets android_set_abort_message() on device then aborts().  This is the default aborter.\n\n @param abort_message an additional message supplied when aborting.  This functions calls\n                      android_set_abort_message() with its contents.\n\n Available since API level 30."]
    pub fn __android_log_default_aborter(abort_message: *const ::std::os::raw::c_char) -> !;
}
extern "C" {
    #[doc = " Use the per-tag properties \"log.tag.<tagname>\" along with the minimum priority from\n __android_log_set_minimum_priority() to determine if a log message with a given prio and tag will\n be printed.  A non-zero result indicates yes, zero indicates false.\n\n If both a priority for a tag and a minimum priority are set by\n __android_log_set_minimum_priority(), then the lowest of the two values are to determine the\n minimum priority needed to log.  If only one is set, then that value is used to determine the\n minimum priority needed.  If none are set, then default_priority is used.\n\n @param prio         the priority to test, takes android_LogPriority values.\n @param tag          the tag to test.\n @param default_prio the default priority to use if no properties or minimum priority are set.\n @return an integer where 1 indicates that the message is loggable and 0 indicates that it is not.\n\n Available since API level 30."]
    pub fn __android_log_is_loggable(
        prio: ::std::os::raw::c_int,
        tag: *const ::std::os::raw::c_char,
        default_prio: ::std::os::raw::c_int,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Use the per-tag properties \"log.tag.<tagname>\" along with the minimum priority from\n __android_log_set_minimum_priority() to determine if a log message with a given prio and tag will\n be printed.  A non-zero result indicates yes, zero indicates false.\n\n If both a priority for a tag and a minimum priority are set by\n __android_log_set_minimum_priority(), then the lowest of the two values are to determine the\n minimum priority needed to log.  If only one is set, then that value is used to determine the\n minimum priority needed.  If none are set, then default_priority is used.\n\n @param prio         the priority to test, takes android_LogPriority values.\n @param tag          the tag to test.\n @param len          the length of the tag.\n @param default_prio the default priority to use if no properties or minimum priority are set.\n @return an integer where 1 indicates that the message is loggable and 0 indicates that it is not.\n\n Available since API level 30."]
    pub fn __android_log_is_loggable_len(
        prio: ::std::os::raw::c_int,
        tag: *const ::std::os::raw::c_char,
        len: usize,
        default_prio: ::std::os::raw::c_int,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Sets the minimum priority that will be logged for this process.\n\n @param priority the new minimum priority to set, takes android_LogPriority values.\n @return the previous set minimum priority as android_LogPriority values, or\n         ANDROID_LOG_DEFAULT if none was set.\n\n Available since API level 30."]
    pub fn __android_log_set_minimum_priority(priority: i32) -> i32;
}
extern "C" {
    #[doc = " Gets the minimum priority that will be logged for this process.  If none has been set by a\n previous __android_log_set_minimum_priority() call, this returns ANDROID_LOG_DEFAULT.\n\n @return the current minimum priority as android_LogPriority values, or\n         ANDROID_LOG_DEFAULT if none is set.\n\n Available since API level 30."]
    pub fn __android_log_get_minimum_priority() -> i32;
}
extern "C" {
    #[doc = " Sets the default tag if no tag is provided when writing a log message.  Defaults to\n getprogname().  This truncates tag to the maximum log message size, though appropriate tags\n should be much smaller.\n\n @param tag the new log tag.\n\n Available since API level 30."]
    pub fn __android_log_set_default_tag(tag: *const ::std::os::raw::c_char);
}
pub type __s8 = ::std::os::raw::c_schar;
pub type __u8 = ::std::os::raw::c_uchar;
pub type __s16 = ::std::os::raw::c_short;
pub type __u16 = ::std::os::raw::c_ushort;
pub type __s32 = ::std::os::raw::c_int;
pub type __u32 = ::std::os::raw::c_uint;
pub type __s64 = ::std::os::raw::c_longlong;
pub type __u64 = ::std::os::raw::c_ulonglong;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct __kernel_fd_set {
    pub fds_bits: [::std::os::raw::c_ulong; 16usize],
}
#[test]
fn bindgen_test_layout___kernel_fd_set() {
    const UNINIT: ::std::mem::MaybeUninit<__kernel_fd_set> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<__kernel_fd_set>(),
        128usize,
        concat!("Size of: ", stringify!(__kernel_fd_set))
    );
    assert_eq!(
        ::std::mem::align_of::<__kernel_fd_set>(),
        8usize,
        concat!("Alignment of ", stringify!(__kernel_fd_set))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).fds_bits) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(__kernel_fd_set),
            "::",
            stringify!(fds_bits)
        )
    );
}
pub type __kernel_sighandler_t =
    ::std::option::Option<unsafe extern "C" fn(arg1: ::std::os::raw::c_int)>;
pub type __kernel_key_t = ::std::os::raw::c_int;
pub type __kernel_mqd_t = ::std::os::raw::c_int;
pub type __kernel_old_uid_t = ::std::os::raw::c_ushort;
pub type __kernel_old_gid_t = ::std::os::raw::c_ushort;
pub type __kernel_old_dev_t = ::std::os::raw::c_ulong;
pub type __kernel_long_t = ::std::os::raw::c_long;
pub type __kernel_ulong_t = ::std::os::raw::c_ulong;
pub type __kernel_ino_t = __kernel_ulong_t;
pub type __kernel_mode_t = ::std::os::raw::c_uint;
pub type __kernel_pid_t = ::std::os::raw::c_int;
pub type __kernel_ipc_pid_t = ::std::os::raw::c_int;
pub type __kernel_uid_t = ::std::os::raw::c_uint;
pub type __kernel_gid_t = ::std::os::raw::c_uint;
pub type __kernel_suseconds_t = __kernel_long_t;
pub type __kernel_daddr_t = ::std::os::raw::c_int;
pub type __kernel_uid32_t = ::std::os::raw::c_uint;
pub type __kernel_gid32_t = ::std::os::raw::c_uint;
pub type __kernel_size_t = __kernel_ulong_t;
pub type __kernel_ssize_t = __kernel_long_t;
pub type __kernel_ptrdiff_t = __kernel_long_t;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct __kernel_fsid_t {
    pub val: [::std::os::raw::c_int; 2usize],
}
#[test]
fn bindgen_test_layout___kernel_fsid_t() {
    const UNINIT: ::std::mem::MaybeUninit<__kernel_fsid_t> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<__kernel_fsid_t>(),
        8usize,
        concat!("Size of: ", stringify!(__kernel_fsid_t))
    );
    assert_eq!(
        ::std::mem::align_of::<__kernel_fsid_t>(),
        4usize,
        concat!("Alignment of ", stringify!(__kernel_fsid_t))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).val) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(__kernel_fsid_t),
            "::",
            stringify!(val)
        )
    );
}
pub type __kernel_off_t = __kernel_long_t;
pub type __kernel_loff_t = ::std::os::raw::c_longlong;
pub type __kernel_old_time_t = __kernel_long_t;
pub type __kernel_time_t = __kernel_long_t;
pub type __kernel_time64_t = ::std::os::raw::c_longlong;
pub type __kernel_clock_t = __kernel_long_t;
pub type __kernel_timer_t = ::std::os::raw::c_int;
pub type __kernel_clockid_t = ::std::os::raw::c_int;
pub type __kernel_caddr_t = *mut ::std::os::raw::c_char;
pub type __kernel_uid16_t = ::std::os::raw::c_ushort;
pub type __kernel_gid16_t = ::std::os::raw::c_ushort;
pub type __le16 = __u16;
pub type __be16 = __u16;
pub type __le32 = __u32;
pub type __be32 = __u32;
pub type __le64 = __u64;
pub type __be64 = __u64;
pub type __sum16 = __u16;
pub type __wsum = __u32;
pub type __poll_t = ::std::os::raw::c_uint;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct pthread_attr_t {
    pub flags: u32,
    pub stack_base: *mut ::std::os::raw::c_void,
    pub stack_size: usize,
    pub guard_size: usize,
    pub sched_policy: i32,
    pub sched_priority: i32,
    pub __reserved: [::std::os::raw::c_char; 16usize],
}
#[test]
fn bindgen_test_layout_pthread_attr_t() {
    const UNINIT: ::std::mem::MaybeUninit<pthread_attr_t> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<pthread_attr_t>(),
        56usize,
        concat!("Size of: ", stringify!(pthread_attr_t))
    );
    assert_eq!(
        ::std::mem::align_of::<pthread_attr_t>(),
        8usize,
        concat!("Alignment of ", stringify!(pthread_attr_t))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).flags) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_attr_t),
            "::",
            stringify!(flags)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).stack_base) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_attr_t),
            "::",
            stringify!(stack_base)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).stack_size) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_attr_t),
            "::",
            stringify!(stack_size)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).guard_size) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_attr_t),
            "::",
            stringify!(guard_size)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).sched_policy) as usize - ptr as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_attr_t),
            "::",
            stringify!(sched_policy)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).sched_priority) as usize - ptr as usize },
        36usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_attr_t),
            "::",
            stringify!(sched_priority)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).__reserved) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_attr_t),
            "::",
            stringify!(__reserved)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct pthread_barrier_t {
    pub __private: [i64; 4usize],
}
#[test]
fn bindgen_test_layout_pthread_barrier_t() {
    const UNINIT: ::std::mem::MaybeUninit<pthread_barrier_t> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<pthread_barrier_t>(),
        32usize,
        concat!("Size of: ", stringify!(pthread_barrier_t))
    );
    assert_eq!(
        ::std::mem::align_of::<pthread_barrier_t>(),
        8usize,
        concat!("Alignment of ", stringify!(pthread_barrier_t))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).__private) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_barrier_t),
            "::",
            stringify!(__private)
        )
    );
}
pub type pthread_barrierattr_t = ::std::os::raw::c_int;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct pthread_cond_t {
    pub __private: [i32; 12usize],
}
#[test]
fn bindgen_test_layout_pthread_cond_t() {
    const UNINIT: ::std::mem::MaybeUninit<pthread_cond_t> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<pthread_cond_t>(),
        48usize,
        concat!("Size of: ", stringify!(pthread_cond_t))
    );
    assert_eq!(
        ::std::mem::align_of::<pthread_cond_t>(),
        4usize,
        concat!("Alignment of ", stringify!(pthread_cond_t))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).__private) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_cond_t),
            "::",
            stringify!(__private)
        )
    );
}
pub type pthread_condattr_t = ::std::os::raw::c_long;
pub type pthread_key_t = ::std::os::raw::c_int;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct pthread_mutex_t {
    pub __private: [i32; 10usize],
}
#[test]
fn bindgen_test_layout_pthread_mutex_t() {
    const UNINIT: ::std::mem::MaybeUninit<pthread_mutex_t> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<pthread_mutex_t>(),
        40usize,
        concat!("Size of: ", stringify!(pthread_mutex_t))
    );
    assert_eq!(
        ::std::mem::align_of::<pthread_mutex_t>(),
        4usize,
        concat!("Alignment of ", stringify!(pthread_mutex_t))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).__private) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_mutex_t),
            "::",
            stringify!(__private)
        )
    );
}
pub type pthread_mutexattr_t = ::std::os::raw::c_long;
pub type pthread_once_t = ::std::os::raw::c_int;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct pthread_rwlock_t {
    pub __private: [i32; 14usize],
}
#[test]
fn bindgen_test_layout_pthread_rwlock_t() {
    const UNINIT: ::std::mem::MaybeUninit<pthread_rwlock_t> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<pthread_rwlock_t>(),
        56usize,
        concat!("Size of: ", stringify!(pthread_rwlock_t))
    );
    assert_eq!(
        ::std::mem::align_of::<pthread_rwlock_t>(),
        4usize,
        concat!("Alignment of ", stringify!(pthread_rwlock_t))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).__private) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_rwlock_t),
            "::",
            stringify!(__private)
        )
    );
}
pub type pthread_rwlockattr_t = ::std::os::raw::c_long;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct pthread_spinlock_t {
    pub __private: i64,
}
#[test]
fn bindgen_test_layout_pthread_spinlock_t() {
    const UNINIT: ::std::mem::MaybeUninit<pthread_spinlock_t> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<pthread_spinlock_t>(),
        8usize,
        concat!("Size of: ", stringify!(pthread_spinlock_t))
    );
    assert_eq!(
        ::std::mem::align_of::<pthread_spinlock_t>(),
        8usize,
        concat!("Alignment of ", stringify!(pthread_spinlock_t))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).__private) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(pthread_spinlock_t),
            "::",
            stringify!(__private)
        )
    );
}
pub type pthread_t = ::std::os::raw::c_long;
pub type __gid_t = __kernel_gid32_t;
pub type gid_t = __gid_t;
pub type __uid_t = __kernel_uid32_t;
pub type uid_t = __uid_t;
pub type __pid_t = __kernel_pid_t;
pub type pid_t = __pid_t;
pub type __id_t = u32;
pub type id_t = __id_t;
pub type blkcnt_t = ::std::os::raw::c_ulong;
pub type blksize_t = ::std::os::raw::c_ulong;
pub type caddr_t = __kernel_caddr_t;
pub type clock_t = __kernel_clock_t;
pub type __clockid_t = __kernel_clockid_t;
pub type clockid_t = __clockid_t;
pub type daddr_t = __kernel_daddr_t;
pub type fsblkcnt_t = ::std::os::raw::c_ulong;
pub type fsfilcnt_t = ::std::os::raw::c_ulong;
pub type __mode_t = __kernel_mode_t;
pub type mode_t = __mode_t;
pub type __key_t = __kernel_key_t;
pub type key_t = __key_t;
pub type __ino_t = __kernel_ino_t;
pub type ino_t = __ino_t;
pub type ino64_t = u64;
pub type __nlink_t = u32;
pub type nlink_t = __nlink_t;
pub type __timer_t = *mut ::std::os::raw::c_void;
pub type timer_t = __timer_t;
pub type __suseconds_t = __kernel_suseconds_t;
pub type suseconds_t = __suseconds_t;
pub type __useconds_t = u32;
pub type useconds_t = __useconds_t;
pub type dev_t = u64;
pub type __time_t = __kernel_time_t;
pub type time_t = __time_t;
pub type off_t = i64;
pub type loff_t = off_t;
pub type off64_t = loff_t;
pub type __socklen_t = u32;
pub type socklen_t = __socklen_t;
pub type __va_list = __builtin_va_list;
pub type uint_t = ::std::os::raw::c_uint;
pub type uint = ::std::os::raw::c_uint;
pub type u_char = ::std::os::raw::c_uchar;
pub type u_short = ::std::os::raw::c_ushort;
pub type u_int = ::std::os::raw::c_uint;
pub type u_long = ::std::os::raw::c_ulong;
pub type u_int32_t = u32;
pub type u_int16_t = u16;
pub type u_int8_t = u8;
pub type u_int64_t = u64;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AAssetManager {
    _unused: [u8; 0],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AAssetDir {
    _unused: [u8; 0],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AAsset {
    _unused: [u8; 0],
}
#[doc = " No specific information about how data will be accessed."]
pub const AASSET_MODE_UNKNOWN: _bindgen_ty_1 = 0;
#[doc = " Read chunks, and seek forward and backward."]
pub const AASSET_MODE_RANDOM: _bindgen_ty_1 = 1;
#[doc = " Read sequentially, with an occasional forward seek."]
pub const AASSET_MODE_STREAMING: _bindgen_ty_1 = 2;
#[doc = " Caller plans to ask for a read-only buffer with all data."]
pub const AASSET_MODE_BUFFER: _bindgen_ty_1 = 3;
#[doc = " Available access modes for opening assets with {@link AAssetManager_open}"]
pub type _bindgen_ty_1 = ::std::os::raw::c_uint;
extern "C" {
    #[doc = " Open the named directory within the asset hierarchy.  The directory can then\n be inspected with the AAssetDir functions.  To open the top-level directory,\n pass in \"\" as the dirName.\n\n The object returned here should be freed by calling AAssetDir_close()."]
    pub fn AAssetManager_openDir(
        mgr: *mut AAssetManager,
        dirName: *const ::std::os::raw::c_char,
    ) -> *mut AAssetDir;
}
extern "C" {
    #[doc = " Open an asset.\n\n The object returned here should be freed by calling AAsset_close()."]
    pub fn AAssetManager_open(
        mgr: *mut AAssetManager,
        filename: *const ::std::os::raw::c_char,
        mode: ::std::os::raw::c_int,
    ) -> *mut AAsset;
}
extern "C" {
    #[doc = " Iterate over the files in an asset directory.  A NULL string is returned\n when all the file names have been returned.\n\n The returned file name is suitable for passing to AAssetManager_open().\n\n The string returned here is owned by the AssetDir implementation and is not\n guaranteed to remain valid if any other calls are made on this AAssetDir\n instance."]
    pub fn AAssetDir_getNextFileName(assetDir: *mut AAssetDir) -> *const ::std::os::raw::c_char;
}
extern "C" {
    #[doc = " Reset the iteration state of AAssetDir_getNextFileName() to the beginning."]
    pub fn AAssetDir_rewind(assetDir: *mut AAssetDir);
}
extern "C" {
    #[doc = " Close an opened AAssetDir, freeing any related resources."]
    pub fn AAssetDir_close(assetDir: *mut AAssetDir);
}
extern "C" {
    #[doc = " Attempt to read 'count' bytes of data from the current offset.\n\n Returns the number of bytes read, zero on EOF, or < 0 on error."]
    pub fn AAsset_read(
        asset: *mut AAsset,
        buf: *mut ::std::os::raw::c_void,
        count: usize,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Seek to the specified offset within the asset data.  'whence' uses the\n same constants as lseek()/fseek().\n\n Returns the new position on success, or (off_t) -1 on error."]
    pub fn AAsset_seek(asset: *mut AAsset, offset: off_t, whence: ::std::os::raw::c_int) -> off_t;
}
extern "C" {
    #[doc = " Seek to the specified offset within the asset data.  'whence' uses the\n same constants as lseek()/fseek().\n\n Uses 64-bit data type for large files as opposed to the 32-bit type used\n by AAsset_seek.\n\n Returns the new position on success, or (off64_t) -1 on error."]
    pub fn AAsset_seek64(
        asset: *mut AAsset,
        offset: off64_t,
        whence: ::std::os::raw::c_int,
    ) -> off64_t;
}
extern "C" {
    #[doc = " Close the asset, freeing all associated resources."]
    pub fn AAsset_close(asset: *mut AAsset);
}
extern "C" {
    #[doc = " Get a pointer to a buffer holding the entire contents of the assset.\n\n Returns NULL on failure."]
    pub fn AAsset_getBuffer(asset: *mut AAsset) -> *const ::std::os::raw::c_void;
}
extern "C" {
    #[doc = " Report the total size of the asset data."]
    pub fn AAsset_getLength(asset: *mut AAsset) -> off_t;
}
extern "C" {
    #[doc = " Report the total size of the asset data. Reports the size using a 64-bit\n number insted of 32-bit as AAsset_getLength."]
    pub fn AAsset_getLength64(asset: *mut AAsset) -> off64_t;
}
extern "C" {
    #[doc = " Report the total amount of asset data that can be read from the current position."]
    pub fn AAsset_getRemainingLength(asset: *mut AAsset) -> off_t;
}
extern "C" {
    #[doc = " Report the total amount of asset data that can be read from the current position.\n\n Uses a 64-bit number instead of a 32-bit number as AAsset_getRemainingLength does."]
    pub fn AAsset_getRemainingLength64(asset: *mut AAsset) -> off64_t;
}
extern "C" {
    #[doc = " Open a new file descriptor that can be used to read the asset data. If the\n start or length cannot be represented by a 32-bit number, it will be\n truncated. If the file is large, use AAsset_openFileDescriptor64 instead.\n\n Returns < 0 if direct fd access is not possible (for example, if the asset is\n compressed)."]
    pub fn AAsset_openFileDescriptor(
        asset: *mut AAsset,
        outStart: *mut off_t,
        outLength: *mut off_t,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Open a new file descriptor that can be used to read the asset data.\n\n Uses a 64-bit number for the offset and length instead of 32-bit instead of\n as AAsset_openFileDescriptor does.\n\n Returns < 0 if direct fd access is not possible (for example, if the asset is\n compressed)."]
    pub fn AAsset_openFileDescriptor64(
        asset: *mut AAsset,
        outStart: *mut off64_t,
        outLength: *mut off64_t,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Returns whether this asset's internal buffer is allocated in ordinary RAM (i.e. not\n mmapped)."]
    pub fn AAsset_isAllocated(asset: *mut AAsset) -> ::std::os::raw::c_int;
}
#[doc = " Unknown key code."]
pub const AKEYCODE_UNKNOWN: _bindgen_ty_2 = 0;
#[doc = " Soft Left key.\n Usually situated below the display on phones and used as a multi-function\n feature key for selecting a software defined function shown on the bottom left\n of the display."]
pub const AKEYCODE_SOFT_LEFT: _bindgen_ty_2 = 1;
#[doc = " Soft Right key.\n Usually situated below the display on phones and used as a multi-function\n feature key for selecting a software defined function shown on the bottom right\n of the display."]
pub const AKEYCODE_SOFT_RIGHT: _bindgen_ty_2 = 2;
#[doc = " Home key.\n This key is handled by the framework and is never delivered to applications."]
pub const AKEYCODE_HOME: _bindgen_ty_2 = 3;
#[doc = " Back key."]
pub const AKEYCODE_BACK: _bindgen_ty_2 = 4;
#[doc = " Call key."]
pub const AKEYCODE_CALL: _bindgen_ty_2 = 5;
#[doc = " End Call key."]
pub const AKEYCODE_ENDCALL: _bindgen_ty_2 = 6;
#[doc = " '0' key."]
pub const AKEYCODE_0: _bindgen_ty_2 = 7;
#[doc = " '1' key."]
pub const AKEYCODE_1: _bindgen_ty_2 = 8;
#[doc = " '2' key."]
pub const AKEYCODE_2: _bindgen_ty_2 = 9;
#[doc = " '3' key."]
pub const AKEYCODE_3: _bindgen_ty_2 = 10;
#[doc = " '4' key."]
pub const AKEYCODE_4: _bindgen_ty_2 = 11;
#[doc = " '5' key."]
pub const AKEYCODE_5: _bindgen_ty_2 = 12;
#[doc = " '6' key."]
pub const AKEYCODE_6: _bindgen_ty_2 = 13;
#[doc = " '7' key."]
pub const AKEYCODE_7: _bindgen_ty_2 = 14;
#[doc = " '8' key."]
pub const AKEYCODE_8: _bindgen_ty_2 = 15;
#[doc = " '9' key."]
pub const AKEYCODE_9: _bindgen_ty_2 = 16;
#[doc = " '*' key."]
pub const AKEYCODE_STAR: _bindgen_ty_2 = 17;
#[doc = " '#' key."]
pub const AKEYCODE_POUND: _bindgen_ty_2 = 18;
#[doc = " Directional Pad Up key.\n May also be synthesized from trackball motions."]
pub const AKEYCODE_DPAD_UP: _bindgen_ty_2 = 19;
#[doc = " Directional Pad Down key.\n May also be synthesized from trackball motions."]
pub const AKEYCODE_DPAD_DOWN: _bindgen_ty_2 = 20;
#[doc = " Directional Pad Left key.\n May also be synthesized from trackball motions."]
pub const AKEYCODE_DPAD_LEFT: _bindgen_ty_2 = 21;
#[doc = " Directional Pad Right key.\n May also be synthesized from trackball motions."]
pub const AKEYCODE_DPAD_RIGHT: _bindgen_ty_2 = 22;
#[doc = " Directional Pad Center key.\n May also be synthesized from trackball motions."]
pub const AKEYCODE_DPAD_CENTER: _bindgen_ty_2 = 23;
#[doc = " Volume Up key.\n Adjusts the speaker volume up."]
pub const AKEYCODE_VOLUME_UP: _bindgen_ty_2 = 24;
#[doc = " Volume Down key.\n Adjusts the speaker volume down."]
pub const AKEYCODE_VOLUME_DOWN: _bindgen_ty_2 = 25;
#[doc = " Power key."]
pub const AKEYCODE_POWER: _bindgen_ty_2 = 26;
#[doc = " Camera key.\n Used to launch a camera application or take pictures."]
pub const AKEYCODE_CAMERA: _bindgen_ty_2 = 27;
#[doc = " Clear key."]
pub const AKEYCODE_CLEAR: _bindgen_ty_2 = 28;
#[doc = " 'A' key."]
pub const AKEYCODE_A: _bindgen_ty_2 = 29;
#[doc = " 'B' key."]
pub const AKEYCODE_B: _bindgen_ty_2 = 30;
#[doc = " 'C' key."]
pub const AKEYCODE_C: _bindgen_ty_2 = 31;
#[doc = " 'D' key."]
pub const AKEYCODE_D: _bindgen_ty_2 = 32;
#[doc = " 'E' key."]
pub const AKEYCODE_E: _bindgen_ty_2 = 33;
#[doc = " 'F' key."]
pub const AKEYCODE_F: _bindgen_ty_2 = 34;
#[doc = " 'G' key."]
pub const AKEYCODE_G: _bindgen_ty_2 = 35;
#[doc = " 'H' key."]
pub const AKEYCODE_H: _bindgen_ty_2 = 36;
#[doc = " 'I' key."]
pub const AKEYCODE_I: _bindgen_ty_2 = 37;
#[doc = " 'J' key."]
pub const AKEYCODE_J: _bindgen_ty_2 = 38;
#[doc = " 'K' key."]
pub const AKEYCODE_K: _bindgen_ty_2 = 39;
#[doc = " 'L' key."]
pub const AKEYCODE_L: _bindgen_ty_2 = 40;
#[doc = " 'M' key."]
pub const AKEYCODE_M: _bindgen_ty_2 = 41;
#[doc = " 'N' key."]
pub const AKEYCODE_N: _bindgen_ty_2 = 42;
#[doc = " 'O' key."]
pub const AKEYCODE_O: _bindgen_ty_2 = 43;
#[doc = " 'P' key."]
pub const AKEYCODE_P: _bindgen_ty_2 = 44;
#[doc = " 'Q' key."]
pub const AKEYCODE_Q: _bindgen_ty_2 = 45;
#[doc = " 'R' key."]
pub const AKEYCODE_R: _bindgen_ty_2 = 46;
#[doc = " 'S' key."]
pub const AKEYCODE_S: _bindgen_ty_2 = 47;
#[doc = " 'T' key."]
pub const AKEYCODE_T: _bindgen_ty_2 = 48;
#[doc = " 'U' key."]
pub const AKEYCODE_U: _bindgen_ty_2 = 49;
#[doc = " 'V' key."]
pub const AKEYCODE_V: _bindgen_ty_2 = 50;
#[doc = " 'W' key."]
pub const AKEYCODE_W: _bindgen_ty_2 = 51;
#[doc = " 'X' key."]
pub const AKEYCODE_X: _bindgen_ty_2 = 52;
#[doc = " 'Y' key."]
pub const AKEYCODE_Y: _bindgen_ty_2 = 53;
#[doc = " 'Z' key."]
pub const AKEYCODE_Z: _bindgen_ty_2 = 54;
#[doc = " ',' key."]
pub const AKEYCODE_COMMA: _bindgen_ty_2 = 55;
#[doc = " '.' key."]
pub const AKEYCODE_PERIOD: _bindgen_ty_2 = 56;
#[doc = " Left Alt modifier key."]
pub const AKEYCODE_ALT_LEFT: _bindgen_ty_2 = 57;
#[doc = " Right Alt modifier key."]
pub const AKEYCODE_ALT_RIGHT: _bindgen_ty_2 = 58;
#[doc = " Left Shift modifier key."]
pub const AKEYCODE_SHIFT_LEFT: _bindgen_ty_2 = 59;
#[doc = " Right Shift modifier key."]
pub const AKEYCODE_SHIFT_RIGHT: _bindgen_ty_2 = 60;
#[doc = " Tab key."]
pub const AKEYCODE_TAB: _bindgen_ty_2 = 61;
#[doc = " Space key."]
pub const AKEYCODE_SPACE: _bindgen_ty_2 = 62;
#[doc = " Symbol modifier key.\n Used to enter alternate symbols."]
pub const AKEYCODE_SYM: _bindgen_ty_2 = 63;
#[doc = " Explorer special function key.\n Used to launch a browser application."]
pub const AKEYCODE_EXPLORER: _bindgen_ty_2 = 64;
#[doc = " Envelope special function key.\n Used to launch a mail application."]
pub const AKEYCODE_ENVELOPE: _bindgen_ty_2 = 65;
#[doc = " Enter key."]
pub const AKEYCODE_ENTER: _bindgen_ty_2 = 66;
#[doc = " Backspace key.\n Deletes characters before the insertion point, unlike {@link AKEYCODE_FORWARD_DEL}."]
pub const AKEYCODE_DEL: _bindgen_ty_2 = 67;
#[doc = " '`' (backtick) key."]
pub const AKEYCODE_GRAVE: _bindgen_ty_2 = 68;
#[doc = " '-'."]
pub const AKEYCODE_MINUS: _bindgen_ty_2 = 69;
#[doc = " '=' key."]
pub const AKEYCODE_EQUALS: _bindgen_ty_2 = 70;
#[doc = " '[' key."]
pub const AKEYCODE_LEFT_BRACKET: _bindgen_ty_2 = 71;
#[doc = " ']' key."]
pub const AKEYCODE_RIGHT_BRACKET: _bindgen_ty_2 = 72;
#[doc = " '\\' key."]
pub const AKEYCODE_BACKSLASH: _bindgen_ty_2 = 73;
#[doc = " ';' key."]
pub const AKEYCODE_SEMICOLON: _bindgen_ty_2 = 74;
#[doc = " ''' (apostrophe) key."]
pub const AKEYCODE_APOSTROPHE: _bindgen_ty_2 = 75;
#[doc = " '/' key."]
pub const AKEYCODE_SLASH: _bindgen_ty_2 = 76;
#[doc = " '@' key."]
pub const AKEYCODE_AT: _bindgen_ty_2 = 77;
#[doc = " Number modifier key.\n Used to enter numeric symbols.\n This key is not {@link AKEYCODE_NUM_LOCK}; it is more like {@link AKEYCODE_ALT_LEFT}."]
pub const AKEYCODE_NUM: _bindgen_ty_2 = 78;
#[doc = " Headset Hook key.\n Used to hang up calls and stop media."]
pub const AKEYCODE_HEADSETHOOK: _bindgen_ty_2 = 79;
#[doc = " Camera Focus key.\n Used to focus the camera."]
pub const AKEYCODE_FOCUS: _bindgen_ty_2 = 80;
#[doc = " '+' key."]
pub const AKEYCODE_PLUS: _bindgen_ty_2 = 81;
#[doc = " Menu key."]
pub const AKEYCODE_MENU: _bindgen_ty_2 = 82;
#[doc = " Notification key."]
pub const AKEYCODE_NOTIFICATION: _bindgen_ty_2 = 83;
#[doc = " Search key."]
pub const AKEYCODE_SEARCH: _bindgen_ty_2 = 84;
#[doc = " Play/Pause media key."]
pub const AKEYCODE_MEDIA_PLAY_PAUSE: _bindgen_ty_2 = 85;
#[doc = " Stop media key."]
pub const AKEYCODE_MEDIA_STOP: _bindgen_ty_2 = 86;
#[doc = " Play Next media key."]
pub const AKEYCODE_MEDIA_NEXT: _bindgen_ty_2 = 87;
#[doc = " Play Previous media key."]
pub const AKEYCODE_MEDIA_PREVIOUS: _bindgen_ty_2 = 88;
#[doc = " Rewind media key."]
pub const AKEYCODE_MEDIA_REWIND: _bindgen_ty_2 = 89;
#[doc = " Fast Forward media key."]
pub const AKEYCODE_MEDIA_FAST_FORWARD: _bindgen_ty_2 = 90;
#[doc = " Mute key.\n Mutes the microphone, unlike {@link AKEYCODE_VOLUME_MUTE}."]
pub const AKEYCODE_MUTE: _bindgen_ty_2 = 91;
#[doc = " Page Up key."]
pub const AKEYCODE_PAGE_UP: _bindgen_ty_2 = 92;
#[doc = " Page Down key."]
pub const AKEYCODE_PAGE_DOWN: _bindgen_ty_2 = 93;
#[doc = " Picture Symbols modifier key.\n Used to switch symbol sets (Emoji, Kao-moji)."]
pub const AKEYCODE_PICTSYMBOLS: _bindgen_ty_2 = 94;
#[doc = " Switch Charset modifier key.\n Used to switch character sets (Kanji, Katakana)."]
pub const AKEYCODE_SWITCH_CHARSET: _bindgen_ty_2 = 95;
#[doc = " A Button key.\n On a game controller, the A button should be either the button labeled A\n or the first button on the bottom row of controller buttons."]
pub const AKEYCODE_BUTTON_A: _bindgen_ty_2 = 96;
#[doc = " B Button key.\n On a game controller, the B button should be either the button labeled B\n or the second button on the bottom row of controller buttons."]
pub const AKEYCODE_BUTTON_B: _bindgen_ty_2 = 97;
#[doc = " C Button key.\n On a game controller, the C button should be either the button labeled C\n or the third button on the bottom row of controller buttons."]
pub const AKEYCODE_BUTTON_C: _bindgen_ty_2 = 98;
#[doc = " X Button key.\n On a game controller, the X button should be either the button labeled X\n or the first button on the upper row of controller buttons."]
pub const AKEYCODE_BUTTON_X: _bindgen_ty_2 = 99;
#[doc = " Y Button key.\n On a game controller, the Y button should be either the button labeled Y\n or the second button on the upper row of controller buttons."]
pub const AKEYCODE_BUTTON_Y: _bindgen_ty_2 = 100;
#[doc = " Z Button key.\n On a game controller, the Z button should be either the button labeled Z\n or the third button on the upper row of controller buttons."]
pub const AKEYCODE_BUTTON_Z: _bindgen_ty_2 = 101;
#[doc = " L1 Button key.\n On a game controller, the L1 button should be either the button labeled L1 (or L)\n or the top left trigger button."]
pub const AKEYCODE_BUTTON_L1: _bindgen_ty_2 = 102;
#[doc = " R1 Button key.\n On a game controller, the R1 button should be either the button labeled R1 (or R)\n or the top right trigger button."]
pub const AKEYCODE_BUTTON_R1: _bindgen_ty_2 = 103;
#[doc = " L2 Button key.\n On a game controller, the L2 button should be either the button labeled L2\n or the bottom left trigger button."]
pub const AKEYCODE_BUTTON_L2: _bindgen_ty_2 = 104;
#[doc = " R2 Button key.\n On a game controller, the R2 button should be either the button labeled R2\n or the bottom right trigger button."]
pub const AKEYCODE_BUTTON_R2: _bindgen_ty_2 = 105;
#[doc = " Left Thumb Button key.\n On a game controller, the left thumb button indicates that the left (or only)\n joystick is pressed."]
pub const AKEYCODE_BUTTON_THUMBL: _bindgen_ty_2 = 106;
#[doc = " Right Thumb Button key.\n On a game controller, the right thumb button indicates that the right\n joystick is pressed."]
pub const AKEYCODE_BUTTON_THUMBR: _bindgen_ty_2 = 107;
#[doc = " Start Button key.\n On a game controller, the button labeled Start."]
pub const AKEYCODE_BUTTON_START: _bindgen_ty_2 = 108;
#[doc = " Select Button key.\n On a game controller, the button labeled Select."]
pub const AKEYCODE_BUTTON_SELECT: _bindgen_ty_2 = 109;
#[doc = " Mode Button key.\n On a game controller, the button labeled Mode."]
pub const AKEYCODE_BUTTON_MODE: _bindgen_ty_2 = 110;
#[doc = " Escape key."]
pub const AKEYCODE_ESCAPE: _bindgen_ty_2 = 111;
#[doc = " Forward Delete key.\n Deletes characters ahead of the insertion point, unlike {@link AKEYCODE_DEL}."]
pub const AKEYCODE_FORWARD_DEL: _bindgen_ty_2 = 112;
#[doc = " Left Control modifier key."]
pub const AKEYCODE_CTRL_LEFT: _bindgen_ty_2 = 113;
#[doc = " Right Control modifier key."]
pub const AKEYCODE_CTRL_RIGHT: _bindgen_ty_2 = 114;
#[doc = " Caps Lock key."]
pub const AKEYCODE_CAPS_LOCK: _bindgen_ty_2 = 115;
#[doc = " Scroll Lock key."]
pub const AKEYCODE_SCROLL_LOCK: _bindgen_ty_2 = 116;
#[doc = " Left Meta modifier key."]
pub const AKEYCODE_META_LEFT: _bindgen_ty_2 = 117;
#[doc = " Right Meta modifier key."]
pub const AKEYCODE_META_RIGHT: _bindgen_ty_2 = 118;
#[doc = " Function modifier key."]
pub const AKEYCODE_FUNCTION: _bindgen_ty_2 = 119;
#[doc = " System Request / Print Screen key."]
pub const AKEYCODE_SYSRQ: _bindgen_ty_2 = 120;
#[doc = " Break / Pause key."]
pub const AKEYCODE_BREAK: _bindgen_ty_2 = 121;
#[doc = " Home Movement key.\n Used for scrolling or moving the cursor around to the start of a line\n or to the top of a list."]
pub const AKEYCODE_MOVE_HOME: _bindgen_ty_2 = 122;
#[doc = " End Movement key.\n Used for scrolling or moving the cursor around to the end of a line\n or to the bottom of a list."]
pub const AKEYCODE_MOVE_END: _bindgen_ty_2 = 123;
#[doc = " Insert key.\n Toggles insert / overwrite edit mode."]
pub const AKEYCODE_INSERT: _bindgen_ty_2 = 124;
#[doc = " Forward key.\n Navigates forward in the history stack.  Complement of {@link AKEYCODE_BACK}."]
pub const AKEYCODE_FORWARD: _bindgen_ty_2 = 125;
#[doc = " Play media key."]
pub const AKEYCODE_MEDIA_PLAY: _bindgen_ty_2 = 126;
#[doc = " Pause media key."]
pub const AKEYCODE_MEDIA_PAUSE: _bindgen_ty_2 = 127;
#[doc = " Close media key.\n May be used to close a CD tray, for example."]
pub const AKEYCODE_MEDIA_CLOSE: _bindgen_ty_2 = 128;
#[doc = " Eject media key.\n May be used to eject a CD tray, for example."]
pub const AKEYCODE_MEDIA_EJECT: _bindgen_ty_2 = 129;
#[doc = " Record media key."]
pub const AKEYCODE_MEDIA_RECORD: _bindgen_ty_2 = 130;
#[doc = " F1 key."]
pub const AKEYCODE_F1: _bindgen_ty_2 = 131;
#[doc = " F2 key."]
pub const AKEYCODE_F2: _bindgen_ty_2 = 132;
#[doc = " F3 key."]
pub const AKEYCODE_F3: _bindgen_ty_2 = 133;
#[doc = " F4 key."]
pub const AKEYCODE_F4: _bindgen_ty_2 = 134;
#[doc = " F5 key."]
pub const AKEYCODE_F5: _bindgen_ty_2 = 135;
#[doc = " F6 key."]
pub const AKEYCODE_F6: _bindgen_ty_2 = 136;
#[doc = " F7 key."]
pub const AKEYCODE_F7: _bindgen_ty_2 = 137;
#[doc = " F8 key."]
pub const AKEYCODE_F8: _bindgen_ty_2 = 138;
#[doc = " F9 key."]
pub const AKEYCODE_F9: _bindgen_ty_2 = 139;
#[doc = " F10 key."]
pub const AKEYCODE_F10: _bindgen_ty_2 = 140;
#[doc = " F11 key."]
pub const AKEYCODE_F11: _bindgen_ty_2 = 141;
#[doc = " F12 key."]
pub const AKEYCODE_F12: _bindgen_ty_2 = 142;
#[doc = " Num Lock key.\n This is the Num Lock key; it is different from {@link AKEYCODE_NUM}.\n This key alters the behavior of other keys on the numeric keypad."]
pub const AKEYCODE_NUM_LOCK: _bindgen_ty_2 = 143;
#[doc = " Numeric keypad '0' key."]
pub const AKEYCODE_NUMPAD_0: _bindgen_ty_2 = 144;
#[doc = " Numeric keypad '1' key."]
pub const AKEYCODE_NUMPAD_1: _bindgen_ty_2 = 145;
#[doc = " Numeric keypad '2' key."]
pub const AKEYCODE_NUMPAD_2: _bindgen_ty_2 = 146;
#[doc = " Numeric keypad '3' key."]
pub const AKEYCODE_NUMPAD_3: _bindgen_ty_2 = 147;
#[doc = " Numeric keypad '4' key."]
pub const AKEYCODE_NUMPAD_4: _bindgen_ty_2 = 148;
#[doc = " Numeric keypad '5' key."]
pub const AKEYCODE_NUMPAD_5: _bindgen_ty_2 = 149;
#[doc = " Numeric keypad '6' key."]
pub const AKEYCODE_NUMPAD_6: _bindgen_ty_2 = 150;
#[doc = " Numeric keypad '7' key."]
pub const AKEYCODE_NUMPAD_7: _bindgen_ty_2 = 151;
#[doc = " Numeric keypad '8' key."]
pub const AKEYCODE_NUMPAD_8: _bindgen_ty_2 = 152;
#[doc = " Numeric keypad '9' key."]
pub const AKEYCODE_NUMPAD_9: _bindgen_ty_2 = 153;
#[doc = " Numeric keypad '/' key (for division)."]
pub const AKEYCODE_NUMPAD_DIVIDE: _bindgen_ty_2 = 154;
#[doc = " Numeric keypad '*' key (for multiplication)."]
pub const AKEYCODE_NUMPAD_MULTIPLY: _bindgen_ty_2 = 155;
#[doc = " Numeric keypad '-' key (for subtraction)."]
pub const AKEYCODE_NUMPAD_SUBTRACT: _bindgen_ty_2 = 156;
#[doc = " Numeric keypad '+' key (for addition)."]
pub const AKEYCODE_NUMPAD_ADD: _bindgen_ty_2 = 157;
#[doc = " Numeric keypad '.' key (for decimals or digit grouping)."]
pub const AKEYCODE_NUMPAD_DOT: _bindgen_ty_2 = 158;
#[doc = " Numeric keypad ',' key (for decimals or digit grouping)."]
pub const AKEYCODE_NUMPAD_COMMA: _bindgen_ty_2 = 159;
#[doc = " Numeric keypad Enter key."]
pub const AKEYCODE_NUMPAD_ENTER: _bindgen_ty_2 = 160;
#[doc = " Numeric keypad '=' key."]
pub const AKEYCODE_NUMPAD_EQUALS: _bindgen_ty_2 = 161;
#[doc = " Numeric keypad '(' key."]
pub const AKEYCODE_NUMPAD_LEFT_PAREN: _bindgen_ty_2 = 162;
#[doc = " Numeric keypad ')' key."]
pub const AKEYCODE_NUMPAD_RIGHT_PAREN: _bindgen_ty_2 = 163;
#[doc = " Volume Mute key.\n Mutes the speaker, unlike {@link AKEYCODE_MUTE}.\n This key should normally be implemented as a toggle such that the first press\n mutes the speaker and the second press restores the original volume."]
pub const AKEYCODE_VOLUME_MUTE: _bindgen_ty_2 = 164;
#[doc = " Info key.\n Common on TV remotes to show additional information related to what is\n currently being viewed."]
pub const AKEYCODE_INFO: _bindgen_ty_2 = 165;
#[doc = " Channel up key.\n On TV remotes, increments the television channel."]
pub const AKEYCODE_CHANNEL_UP: _bindgen_ty_2 = 166;
#[doc = " Channel down key.\n On TV remotes, decrements the television channel."]
pub const AKEYCODE_CHANNEL_DOWN: _bindgen_ty_2 = 167;
#[doc = " Zoom in key."]
pub const AKEYCODE_ZOOM_IN: _bindgen_ty_2 = 168;
#[doc = " Zoom out key."]
pub const AKEYCODE_ZOOM_OUT: _bindgen_ty_2 = 169;
#[doc = " TV key.\n On TV remotes, switches to viewing live TV."]
pub const AKEYCODE_TV: _bindgen_ty_2 = 170;
#[doc = " Window key.\n On TV remotes, toggles picture-in-picture mode or other windowing functions."]
pub const AKEYCODE_WINDOW: _bindgen_ty_2 = 171;
#[doc = " Guide key.\n On TV remotes, shows a programming guide."]
pub const AKEYCODE_GUIDE: _bindgen_ty_2 = 172;
#[doc = " DVR key.\n On some TV remotes, switches to a DVR mode for recorded shows."]
pub const AKEYCODE_DVR: _bindgen_ty_2 = 173;
#[doc = " Bookmark key.\n On some TV remotes, bookmarks content or web pages."]
pub const AKEYCODE_BOOKMARK: _bindgen_ty_2 = 174;
#[doc = " Toggle captions key.\n Switches the mode for closed-captioning text, for example during television shows."]
pub const AKEYCODE_CAPTIONS: _bindgen_ty_2 = 175;
#[doc = " Settings key.\n Starts the system settings activity."]
pub const AKEYCODE_SETTINGS: _bindgen_ty_2 = 176;
#[doc = " TV power key.\n On TV remotes, toggles the power on a television screen."]
pub const AKEYCODE_TV_POWER: _bindgen_ty_2 = 177;
#[doc = " TV input key.\n On TV remotes, switches the input on a television screen."]
pub const AKEYCODE_TV_INPUT: _bindgen_ty_2 = 178;
#[doc = " Set-top-box power key.\n On TV remotes, toggles the power on an external Set-top-box."]
pub const AKEYCODE_STB_POWER: _bindgen_ty_2 = 179;
#[doc = " Set-top-box input key.\n On TV remotes, switches the input mode on an external Set-top-box."]
pub const AKEYCODE_STB_INPUT: _bindgen_ty_2 = 180;
#[doc = " A/V Receiver power key.\n On TV remotes, toggles the power on an external A/V Receiver."]
pub const AKEYCODE_AVR_POWER: _bindgen_ty_2 = 181;
#[doc = " A/V Receiver input key.\n On TV remotes, switches the input mode on an external A/V Receiver."]
pub const AKEYCODE_AVR_INPUT: _bindgen_ty_2 = 182;
#[doc = " Red \"programmable\" key.\n On TV remotes, acts as a contextual/programmable key."]
pub const AKEYCODE_PROG_RED: _bindgen_ty_2 = 183;
#[doc = " Green \"programmable\" key.\n On TV remotes, actsas a contextual/programmable key."]
pub const AKEYCODE_PROG_GREEN: _bindgen_ty_2 = 184;
#[doc = " Yellow \"programmable\" key.\n On TV remotes, acts as a contextual/programmable key."]
pub const AKEYCODE_PROG_YELLOW: _bindgen_ty_2 = 185;
#[doc = " Blue \"programmable\" key.\n On TV remotes, acts as a contextual/programmable key."]
pub const AKEYCODE_PROG_BLUE: _bindgen_ty_2 = 186;
#[doc = " App switch key.\n Should bring up the application switcher dialog."]
pub const AKEYCODE_APP_SWITCH: _bindgen_ty_2 = 187;
#[doc = " Generic Game Pad Button #1."]
pub const AKEYCODE_BUTTON_1: _bindgen_ty_2 = 188;
#[doc = " Generic Game Pad Button #2."]
pub const AKEYCODE_BUTTON_2: _bindgen_ty_2 = 189;
#[doc = " Generic Game Pad Button #3."]
pub const AKEYCODE_BUTTON_3: _bindgen_ty_2 = 190;
#[doc = " Generic Game Pad Button #4."]
pub const AKEYCODE_BUTTON_4: _bindgen_ty_2 = 191;
#[doc = " Generic Game Pad Button #5."]
pub const AKEYCODE_BUTTON_5: _bindgen_ty_2 = 192;
#[doc = " Generic Game Pad Button #6."]
pub const AKEYCODE_BUTTON_6: _bindgen_ty_2 = 193;
#[doc = " Generic Game Pad Button #7."]
pub const AKEYCODE_BUTTON_7: _bindgen_ty_2 = 194;
#[doc = " Generic Game Pad Button #8."]
pub const AKEYCODE_BUTTON_8: _bindgen_ty_2 = 195;
#[doc = " Generic Game Pad Button #9."]
pub const AKEYCODE_BUTTON_9: _bindgen_ty_2 = 196;
#[doc = " Generic Game Pad Button #10."]
pub const AKEYCODE_BUTTON_10: _bindgen_ty_2 = 197;
#[doc = " Generic Game Pad Button #11."]
pub const AKEYCODE_BUTTON_11: _bindgen_ty_2 = 198;
#[doc = " Generic Game Pad Button #12."]
pub const AKEYCODE_BUTTON_12: _bindgen_ty_2 = 199;
#[doc = " Generic Game Pad Button #13."]
pub const AKEYCODE_BUTTON_13: _bindgen_ty_2 = 200;
#[doc = " Generic Game Pad Button #14."]
pub const AKEYCODE_BUTTON_14: _bindgen_ty_2 = 201;
#[doc = " Generic Game Pad Button #15."]
pub const AKEYCODE_BUTTON_15: _bindgen_ty_2 = 202;
#[doc = " Generic Game Pad Button #16."]
pub const AKEYCODE_BUTTON_16: _bindgen_ty_2 = 203;
#[doc = " Language Switch key.\n Toggles the current input language such as switching between English and Japanese on\n a QWERTY keyboard.  On some devices, the same function may be performed by\n pressing Shift+Spacebar."]
pub const AKEYCODE_LANGUAGE_SWITCH: _bindgen_ty_2 = 204;
#[doc = " Manner Mode key.\n Toggles silent or vibrate mode on and off to make the device behave more politely\n in certain settings such as on a crowded train.  On some devices, the key may only\n operate when long-pressed."]
pub const AKEYCODE_MANNER_MODE: _bindgen_ty_2 = 205;
#[doc = " 3D Mode key.\n Toggles the display between 2D and 3D mode."]
pub const AKEYCODE_3D_MODE: _bindgen_ty_2 = 206;
#[doc = " Contacts special function key.\n Used to launch an address book application."]
pub const AKEYCODE_CONTACTS: _bindgen_ty_2 = 207;
#[doc = " Calendar special function key.\n Used to launch a calendar application."]
pub const AKEYCODE_CALENDAR: _bindgen_ty_2 = 208;
#[doc = " Music special function key.\n Used to launch a music player application."]
pub const AKEYCODE_MUSIC: _bindgen_ty_2 = 209;
#[doc = " Calculator special function key.\n Used to launch a calculator application."]
pub const AKEYCODE_CALCULATOR: _bindgen_ty_2 = 210;
#[doc = " Japanese full-width / half-width key."]
pub const AKEYCODE_ZENKAKU_HANKAKU: _bindgen_ty_2 = 211;
#[doc = " Japanese alphanumeric key."]
pub const AKEYCODE_EISU: _bindgen_ty_2 = 212;
#[doc = " Japanese non-conversion key."]
pub const AKEYCODE_MUHENKAN: _bindgen_ty_2 = 213;
#[doc = " Japanese conversion key."]
pub const AKEYCODE_HENKAN: _bindgen_ty_2 = 214;
#[doc = " Japanese katakana / hiragana key."]
pub const AKEYCODE_KATAKANA_HIRAGANA: _bindgen_ty_2 = 215;
#[doc = " Japanese Yen key."]
pub const AKEYCODE_YEN: _bindgen_ty_2 = 216;
#[doc = " Japanese Ro key."]
pub const AKEYCODE_RO: _bindgen_ty_2 = 217;
#[doc = " Japanese kana key."]
pub const AKEYCODE_KANA: _bindgen_ty_2 = 218;
#[doc = " Assist key.\n Launches the global assist activity.  Not delivered to applications."]
pub const AKEYCODE_ASSIST: _bindgen_ty_2 = 219;
#[doc = " Brightness Down key.\n Adjusts the screen brightness down."]
pub const AKEYCODE_BRIGHTNESS_DOWN: _bindgen_ty_2 = 220;
#[doc = " Brightness Up key.\n Adjusts the screen brightness up."]
pub const AKEYCODE_BRIGHTNESS_UP: _bindgen_ty_2 = 221;
#[doc = " Audio Track key.\n Switches the audio tracks."]
pub const AKEYCODE_MEDIA_AUDIO_TRACK: _bindgen_ty_2 = 222;
#[doc = " Sleep key.\n Puts the device to sleep.  Behaves somewhat like {@link AKEYCODE_POWER} but it\n has no effect if the device is already asleep."]
pub const AKEYCODE_SLEEP: _bindgen_ty_2 = 223;
#[doc = " Wakeup key.\n Wakes up the device.  Behaves somewhat like {@link AKEYCODE_POWER} but it\n has no effect if the device is already awake."]
pub const AKEYCODE_WAKEUP: _bindgen_ty_2 = 224;
#[doc = " Pairing key.\n Initiates peripheral pairing mode. Useful for pairing remote control\n devices or game controllers, especially if no other input mode is\n available."]
pub const AKEYCODE_PAIRING: _bindgen_ty_2 = 225;
#[doc = " Media Top Menu key.\n Goes to the top of media menu."]
pub const AKEYCODE_MEDIA_TOP_MENU: _bindgen_ty_2 = 226;
#[doc = " '11' key."]
pub const AKEYCODE_11: _bindgen_ty_2 = 227;
#[doc = " '12' key."]
pub const AKEYCODE_12: _bindgen_ty_2 = 228;
#[doc = " Last Channel key.\n Goes to the last viewed channel."]
pub const AKEYCODE_LAST_CHANNEL: _bindgen_ty_2 = 229;
#[doc = " TV data service key.\n Displays data services like weather, sports."]
pub const AKEYCODE_TV_DATA_SERVICE: _bindgen_ty_2 = 230;
#[doc = " Voice Assist key.\n Launches the global voice assist activity. Not delivered to applications."]
pub const AKEYCODE_VOICE_ASSIST: _bindgen_ty_2 = 231;
#[doc = " Radio key.\n Toggles TV service / Radio service."]
pub const AKEYCODE_TV_RADIO_SERVICE: _bindgen_ty_2 = 232;
#[doc = " Teletext key.\n Displays Teletext service."]
pub const AKEYCODE_TV_TELETEXT: _bindgen_ty_2 = 233;
#[doc = " Number entry key.\n Initiates to enter multi-digit channel nubmber when each digit key is assigned\n for selecting separate channel. Corresponds to Number Entry Mode (0x1D) of CEC\n User Control Code."]
pub const AKEYCODE_TV_NUMBER_ENTRY: _bindgen_ty_2 = 234;
#[doc = " Analog Terrestrial key.\n Switches to analog terrestrial broadcast service."]
pub const AKEYCODE_TV_TERRESTRIAL_ANALOG: _bindgen_ty_2 = 235;
#[doc = " Digital Terrestrial key.\n Switches to digital terrestrial broadcast service."]
pub const AKEYCODE_TV_TERRESTRIAL_DIGITAL: _bindgen_ty_2 = 236;
#[doc = " Satellite key.\n Switches to digital satellite broadcast service."]
pub const AKEYCODE_TV_SATELLITE: _bindgen_ty_2 = 237;
#[doc = " BS key.\n Switches to BS digital satellite broadcasting service available in Japan."]
pub const AKEYCODE_TV_SATELLITE_BS: _bindgen_ty_2 = 238;
#[doc = " CS key.\n Switches to CS digital satellite broadcasting service available in Japan."]
pub const AKEYCODE_TV_SATELLITE_CS: _bindgen_ty_2 = 239;
#[doc = " BS/CS key.\n Toggles between BS and CS digital satellite services."]
pub const AKEYCODE_TV_SATELLITE_SERVICE: _bindgen_ty_2 = 240;
#[doc = " Toggle Network key.\n Toggles selecting broacast services."]
pub const AKEYCODE_TV_NETWORK: _bindgen_ty_2 = 241;
#[doc = " Antenna/Cable key.\n Toggles broadcast input source between antenna and cable."]
pub const AKEYCODE_TV_ANTENNA_CABLE: _bindgen_ty_2 = 242;
#[doc = " HDMI #1 key.\n Switches to HDMI input #1."]
pub const AKEYCODE_TV_INPUT_HDMI_1: _bindgen_ty_2 = 243;
#[doc = " HDMI #2 key.\n Switches to HDMI input #2."]
pub const AKEYCODE_TV_INPUT_HDMI_2: _bindgen_ty_2 = 244;
#[doc = " HDMI #3 key.\n Switches to HDMI input #3."]
pub const AKEYCODE_TV_INPUT_HDMI_3: _bindgen_ty_2 = 245;
#[doc = " HDMI #4 key.\n Switches to HDMI input #4."]
pub const AKEYCODE_TV_INPUT_HDMI_4: _bindgen_ty_2 = 246;
#[doc = " Composite #1 key.\n Switches to composite video input #1."]
pub const AKEYCODE_TV_INPUT_COMPOSITE_1: _bindgen_ty_2 = 247;
#[doc = " Composite #2 key.\n Switches to composite video input #2."]
pub const AKEYCODE_TV_INPUT_COMPOSITE_2: _bindgen_ty_2 = 248;
#[doc = " Component #1 key.\n Switches to component video input #1."]
pub const AKEYCODE_TV_INPUT_COMPONENT_1: _bindgen_ty_2 = 249;
#[doc = " Component #2 key.\n Switches to component video input #2."]
pub const AKEYCODE_TV_INPUT_COMPONENT_2: _bindgen_ty_2 = 250;
#[doc = " VGA #1 key.\n Switches to VGA (analog RGB) input #1."]
pub const AKEYCODE_TV_INPUT_VGA_1: _bindgen_ty_2 = 251;
#[doc = " Audio description key.\n Toggles audio description off / on."]
pub const AKEYCODE_TV_AUDIO_DESCRIPTION: _bindgen_ty_2 = 252;
#[doc = " Audio description mixing volume up key.\n Louden audio description volume as compared with normal audio volume."]
pub const AKEYCODE_TV_AUDIO_DESCRIPTION_MIX_UP: _bindgen_ty_2 = 253;
#[doc = " Audio description mixing volume down key.\n Lessen audio description volume as compared with normal audio volume."]
pub const AKEYCODE_TV_AUDIO_DESCRIPTION_MIX_DOWN: _bindgen_ty_2 = 254;
#[doc = " Zoom mode key.\n Changes Zoom mode (Normal, Full, Zoom, Wide-zoom, etc.)"]
pub const AKEYCODE_TV_ZOOM_MODE: _bindgen_ty_2 = 255;
#[doc = " Contents menu key.\n Goes to the title list. Corresponds to Contents Menu (0x0B) of CEC User Control\n Code"]
pub const AKEYCODE_TV_CONTENTS_MENU: _bindgen_ty_2 = 256;
#[doc = " Media context menu key.\n Goes to the context menu of media contents. Corresponds to Media Context-sensitive\n Menu (0x11) of CEC User Control Code."]
pub const AKEYCODE_TV_MEDIA_CONTEXT_MENU: _bindgen_ty_2 = 257;
#[doc = " Timer programming key.\n Goes to the timer recording menu. Corresponds to Timer Programming (0x54) of\n CEC User Control Code."]
pub const AKEYCODE_TV_TIMER_PROGRAMMING: _bindgen_ty_2 = 258;
#[doc = " Help key."]
pub const AKEYCODE_HELP: _bindgen_ty_2 = 259;
#[doc = " Help key."]
pub const AKEYCODE_NAVIGATE_PREVIOUS: _bindgen_ty_2 = 260;
#[doc = " Help key."]
pub const AKEYCODE_NAVIGATE_NEXT: _bindgen_ty_2 = 261;
#[doc = " Help key."]
pub const AKEYCODE_NAVIGATE_IN: _bindgen_ty_2 = 262;
#[doc = " Help key."]
pub const AKEYCODE_NAVIGATE_OUT: _bindgen_ty_2 = 263;
#[doc = " Primary stem key for Wear\n Main power/reset button on watch."]
pub const AKEYCODE_STEM_PRIMARY: _bindgen_ty_2 = 264;
#[doc = " Generic stem key 1 for Wear"]
pub const AKEYCODE_STEM_1: _bindgen_ty_2 = 265;
#[doc = " Generic stem key 2 for Wear"]
pub const AKEYCODE_STEM_2: _bindgen_ty_2 = 266;
#[doc = " Generic stem key 3 for Wear"]
pub const AKEYCODE_STEM_3: _bindgen_ty_2 = 267;
#[doc = " Directional Pad Up-Left"]
pub const AKEYCODE_DPAD_UP_LEFT: _bindgen_ty_2 = 268;
#[doc = " Directional Pad Down-Left"]
pub const AKEYCODE_DPAD_DOWN_LEFT: _bindgen_ty_2 = 269;
#[doc = " Directional Pad Up-Right"]
pub const AKEYCODE_DPAD_UP_RIGHT: _bindgen_ty_2 = 270;
#[doc = " Directional Pad Down-Right"]
pub const AKEYCODE_DPAD_DOWN_RIGHT: _bindgen_ty_2 = 271;
#[doc = " Skip forward media key"]
pub const AKEYCODE_MEDIA_SKIP_FORWARD: _bindgen_ty_2 = 272;
#[doc = " Skip backward media key"]
pub const AKEYCODE_MEDIA_SKIP_BACKWARD: _bindgen_ty_2 = 273;
#[doc = " Step forward media key.\n Steps media forward one from at a time."]
pub const AKEYCODE_MEDIA_STEP_FORWARD: _bindgen_ty_2 = 274;
#[doc = " Step backward media key.\n Steps media backward one from at a time."]
pub const AKEYCODE_MEDIA_STEP_BACKWARD: _bindgen_ty_2 = 275;
#[doc = " Put device to sleep unless a wakelock is held."]
pub const AKEYCODE_SOFT_SLEEP: _bindgen_ty_2 = 276;
#[doc = " Cut key."]
pub const AKEYCODE_CUT: _bindgen_ty_2 = 277;
#[doc = " Copy key."]
pub const AKEYCODE_COPY: _bindgen_ty_2 = 278;
#[doc = " Paste key."]
pub const AKEYCODE_PASTE: _bindgen_ty_2 = 279;
#[doc = " fingerprint navigation key, up."]
pub const AKEYCODE_SYSTEM_NAVIGATION_UP: _bindgen_ty_2 = 280;
#[doc = " fingerprint navigation key, down."]
pub const AKEYCODE_SYSTEM_NAVIGATION_DOWN: _bindgen_ty_2 = 281;
#[doc = " fingerprint navigation key, left."]
pub const AKEYCODE_SYSTEM_NAVIGATION_LEFT: _bindgen_ty_2 = 282;
#[doc = " fingerprint navigation key, right."]
pub const AKEYCODE_SYSTEM_NAVIGATION_RIGHT: _bindgen_ty_2 = 283;
#[doc = " all apps"]
pub const AKEYCODE_ALL_APPS: _bindgen_ty_2 = 284;
#[doc = " refresh key"]
pub const AKEYCODE_REFRESH: _bindgen_ty_2 = 285;
#[doc = " Thumbs up key. Apps can use this to let user upvote content."]
pub const AKEYCODE_THUMBS_UP: _bindgen_ty_2 = 286;
#[doc = " Thumbs down key. Apps can use this to let user downvote content."]
pub const AKEYCODE_THUMBS_DOWN: _bindgen_ty_2 = 287;
#[doc = " Used to switch current account that is consuming content.\n May be consumed by system to switch current viewer profile."]
pub const AKEYCODE_PROFILE_SWITCH: _bindgen_ty_2 = 288;
#[doc = " Video Application key #1."]
pub const AKEYCODE_VIDEO_APP_1: _bindgen_ty_2 = 289;
#[doc = " Video Application key #2."]
pub const AKEYCODE_VIDEO_APP_2: _bindgen_ty_2 = 290;
#[doc = " Video Application key #3."]
pub const AKEYCODE_VIDEO_APP_3: _bindgen_ty_2 = 291;
#[doc = " Video Application key #4."]
pub const AKEYCODE_VIDEO_APP_4: _bindgen_ty_2 = 292;
#[doc = " Video Application key #5."]
pub const AKEYCODE_VIDEO_APP_5: _bindgen_ty_2 = 293;
#[doc = " Video Application key #6."]
pub const AKEYCODE_VIDEO_APP_6: _bindgen_ty_2 = 294;
#[doc = " Video Application key #7."]
pub const AKEYCODE_VIDEO_APP_7: _bindgen_ty_2 = 295;
#[doc = " Video Application key #8."]
pub const AKEYCODE_VIDEO_APP_8: _bindgen_ty_2 = 296;
#[doc = " Featured Application key #1."]
pub const AKEYCODE_FEATURED_APP_1: _bindgen_ty_2 = 297;
#[doc = " Featured Application key #2."]
pub const AKEYCODE_FEATURED_APP_2: _bindgen_ty_2 = 298;
#[doc = " Featured Application key #3."]
pub const AKEYCODE_FEATURED_APP_3: _bindgen_ty_2 = 299;
#[doc = " Featured Application key #4."]
pub const AKEYCODE_FEATURED_APP_4: _bindgen_ty_2 = 300;
#[doc = " Demo Application key #1."]
pub const AKEYCODE_DEMO_APP_1: _bindgen_ty_2 = 301;
#[doc = " Demo Application key #2."]
pub const AKEYCODE_DEMO_APP_2: _bindgen_ty_2 = 302;
#[doc = " Demo Application key #3."]
pub const AKEYCODE_DEMO_APP_3: _bindgen_ty_2 = 303;
#[doc = " Demo Application key #4."]
pub const AKEYCODE_DEMO_APP_4: _bindgen_ty_2 = 304;
#[doc = " Key codes."]
pub type _bindgen_ty_2 = ::std::os::raw::c_uint;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ALooper {
    _unused: [u8; 0],
}
extern "C" {
    #[doc = " Returns the looper associated with the calling thread, or NULL if\n there is not one."]
    pub fn ALooper_forThread() -> *mut ALooper;
}
#[doc = " This looper will accept calls to ALooper_addFd() that do not\n have a callback (that is provide NULL for the callback).  In\n this case the caller of ALooper_pollOnce() or ALooper_pollAll()\n MUST check the return from these functions to discover when\n data is available on such fds and process it."]
pub const ALOOPER_PREPARE_ALLOW_NON_CALLBACKS: _bindgen_ty_3 = 1;
#[doc = " Option for for ALooper_prepare()."]
pub type _bindgen_ty_3 = ::std::os::raw::c_uint;
extern "C" {
    #[doc = " Prepares a looper associated with the calling thread, and returns it.\n If the thread already has a looper, it is returned.  Otherwise, a new\n one is created, associated with the thread, and returned.\n\n The opts may be ALOOPER_PREPARE_ALLOW_NON_CALLBACKS or 0."]
    pub fn ALooper_prepare(opts: ::std::os::raw::c_int) -> *mut ALooper;
}
#[doc = " The poll was awoken using wake() before the timeout expired\n and no callbacks were executed and no other file descriptors were ready."]
pub const ALOOPER_POLL_WAKE: _bindgen_ty_4 = -1;
#[doc = " Result from ALooper_pollOnce() and ALooper_pollAll():\n One or more callbacks were executed."]
pub const ALOOPER_POLL_CALLBACK: _bindgen_ty_4 = -2;
#[doc = " Result from ALooper_pollOnce() and ALooper_pollAll():\n The timeout expired."]
pub const ALOOPER_POLL_TIMEOUT: _bindgen_ty_4 = -3;
#[doc = " Result from ALooper_pollOnce() and ALooper_pollAll():\n An error occurred."]
pub const ALOOPER_POLL_ERROR: _bindgen_ty_4 = -4;
#[doc = " Result from ALooper_pollOnce() and ALooper_pollAll()."]
pub type _bindgen_ty_4 = ::std::os::raw::c_int;
extern "C" {
    #[doc = " Acquire a reference on the given ALooper object.  This prevents the object\n from being deleted until the reference is removed.  This is only needed\n to safely hand an ALooper from one thread to another."]
    pub fn ALooper_acquire(looper: *mut ALooper);
}
extern "C" {
    #[doc = " Remove a reference that was previously acquired with ALooper_acquire()."]
    pub fn ALooper_release(looper: *mut ALooper);
}
#[doc = " The file descriptor is available for read operations."]
pub const ALOOPER_EVENT_INPUT: _bindgen_ty_5 = 1;
#[doc = " The file descriptor is available for write operations."]
pub const ALOOPER_EVENT_OUTPUT: _bindgen_ty_5 = 2;
#[doc = " The file descriptor has encountered an error condition.\n\n The looper always sends notifications about errors; it is not necessary\n to specify this event flag in the requested event set."]
pub const ALOOPER_EVENT_ERROR: _bindgen_ty_5 = 4;
#[doc = " The file descriptor was hung up.\n For example, indicates that the remote end of a pipe or socket was closed.\n\n The looper always sends notifications about hangups; it is not necessary\n to specify this event flag in the requested event set."]
pub const ALOOPER_EVENT_HANGUP: _bindgen_ty_5 = 8;
#[doc = " The file descriptor is invalid.\n For example, the file descriptor was closed prematurely.\n\n The looper always sends notifications about invalid file descriptors; it is not necessary\n to specify this event flag in the requested event set."]
pub const ALOOPER_EVENT_INVALID: _bindgen_ty_5 = 16;
#[doc = " Flags for file descriptor events that a looper can monitor.\n\n These flag bits can be combined to monitor multiple events at once."]
pub type _bindgen_ty_5 = ::std::os::raw::c_uint;
#[doc = " For callback-based event loops, this is the prototype of the function\n that is called when a file descriptor event occurs.\n It is given the file descriptor it is associated with,\n a bitmask of the poll events that were triggered (typically ALOOPER_EVENT_INPUT),\n and the data pointer that was originally supplied.\n\n Implementations should return 1 to continue receiving callbacks, or 0\n to have this file descriptor and callback unregistered from the looper."]
pub type ALooper_callbackFunc = ::std::option::Option<
    unsafe extern "C" fn(
        fd: ::std::os::raw::c_int,
        events: ::std::os::raw::c_int,
        data: *mut ::std::os::raw::c_void,
    ) -> ::std::os::raw::c_int,
>;
extern "C" {
    #[doc = " Waits for events to be available, with optional timeout in milliseconds.\n Invokes callbacks for all file descriptors on which an event occurred.\n\n If the timeout is zero, returns immediately without blocking.\n If the timeout is negative, waits indefinitely until an event appears.\n\n Returns ALOOPER_POLL_WAKE if the poll was awoken using wake() before\n the timeout expired and no callbacks were invoked and no other file\n descriptors were ready.\n\n Returns ALOOPER_POLL_CALLBACK if one or more callbacks were invoked.\n\n Returns ALOOPER_POLL_TIMEOUT if there was no data before the given\n timeout expired.\n\n Returns ALOOPER_POLL_ERROR if an error occurred.\n\n Returns a value >= 0 containing an identifier (the same identifier\n `ident` passed to ALooper_addFd()) if its file descriptor has data\n and it has no callback function (requiring the caller here to\n handle it).  In this (and only this) case outFd, outEvents and\n outData will contain the poll events and data associated with the\n fd, otherwise they will be set to NULL.\n\n This method does not return until it has finished invoking the appropriate callbacks\n for all file descriptors that were signalled."]
    pub fn ALooper_pollOnce(
        timeoutMillis: ::std::os::raw::c_int,
        outFd: *mut ::std::os::raw::c_int,
        outEvents: *mut ::std::os::raw::c_int,
        outData: *mut *mut ::std::os::raw::c_void,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Like ALooper_pollOnce(), but performs all pending callbacks until all\n data has been consumed or a file descriptor is available with no callback.\n This function will never return ALOOPER_POLL_CALLBACK."]
    pub fn ALooper_pollAll(
        timeoutMillis: ::std::os::raw::c_int,
        outFd: *mut ::std::os::raw::c_int,
        outEvents: *mut ::std::os::raw::c_int,
        outData: *mut *mut ::std::os::raw::c_void,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Wakes the poll asynchronously.\n\n This method can be called on any thread.\n This method returns immediately."]
    pub fn ALooper_wake(looper: *mut ALooper);
}
extern "C" {
    #[doc = " Adds a new file descriptor to be polled by the looper.\n If the same file descriptor was previously added, it is replaced.\n\n \"fd\" is the file descriptor to be added.\n \"ident\" is an identifier for this event, which is returned from ALooper_pollOnce().\n The identifier must be >= 0, or ALOOPER_POLL_CALLBACK if providing a non-NULL callback.\n \"events\" are the poll events to wake up on.  Typically this is ALOOPER_EVENT_INPUT.\n \"callback\" is the function to call when there is an event on the file descriptor.\n \"data\" is a private data pointer to supply to the callback.\n\n There are two main uses of this function:\n\n (1) If \"callback\" is non-NULL, then this function will be called when there is\n data on the file descriptor.  It should execute any events it has pending,\n appropriately reading from the file descriptor.  The 'ident' is ignored in this case.\n\n (2) If \"callback\" is NULL, the 'ident' will be returned by ALooper_pollOnce\n when its file descriptor has data available, requiring the caller to take\n care of processing it.\n\n Returns 1 if the file descriptor was added or -1 if an error occurred.\n\n This method can be called on any thread.\n This method may block briefly if it needs to wake the poll."]
    pub fn ALooper_addFd(
        looper: *mut ALooper,
        fd: ::std::os::raw::c_int,
        ident: ::std::os::raw::c_int,
        events: ::std::os::raw::c_int,
        callback: ALooper_callbackFunc,
        data: *mut ::std::os::raw::c_void,
    ) -> ::std::os::raw::c_int;
}
extern "C" {
    #[doc = " Removes a previously added file descriptor from the looper.\n\n When this method returns, it is safe to close the file descriptor since the looper\n will no longer have a reference to it.  However, it is possible for the callback to\n already be running or for it to run one last time if the file descriptor was already\n signalled.  Calling code is responsible for ensuring that this case is safely handled.\n For example, if the callback takes care of removing itself during its own execution either\n by returning 0 or by calling this method, then it can be guaranteed to not be invoked\n again at any later time unless registered anew.\n\n Returns 1 if the file descriptor was removed, 0 if none was previously registered\n or -1 if an error occurred.\n\n This method can be called on any thread.\n This method may block briefly if it needs to wake the poll."]
    pub fn ALooper_removeFd(
        looper: *mut ALooper,
        fd: ::std::os::raw::c_int,
    ) -> ::std::os::raw::c_int;
}
#[doc = " The key state is unknown or the requested key itself is not supported."]
pub const AKEY_STATE_UNKNOWN: _bindgen_ty_6 = -1;
#[doc = " The key is up."]
pub const AKEY_STATE_UP: _bindgen_ty_6 = 0;
#[doc = " The key is down."]
pub const AKEY_STATE_DOWN: _bindgen_ty_6 = 1;
#[doc = " The key is down but is a virtual key press that is being emulated by the system."]
pub const AKEY_STATE_VIRTUAL: _bindgen_ty_6 = 2;
#[doc = " Key states (may be returned by queries about the current state of a\n particular key code, scan code or switch)."]
pub type _bindgen_ty_6 = ::std::os::raw::c_int;
#[doc = " No meta keys are pressed."]
pub const AMETA_NONE: _bindgen_ty_7 = 0;
#[doc = " This mask is used to check whether one of the ALT meta keys is pressed."]
pub const AMETA_ALT_ON: _bindgen_ty_7 = 2;
#[doc = " This mask is used to check whether the left ALT meta key is pressed."]
pub const AMETA_ALT_LEFT_ON: _bindgen_ty_7 = 16;
#[doc = " This mask is used to check whether the right ALT meta key is pressed."]
pub const AMETA_ALT_RIGHT_ON: _bindgen_ty_7 = 32;
#[doc = " This mask is used to check whether one of the SHIFT meta keys is pressed."]
pub const AMETA_SHIFT_ON: _bindgen_ty_7 = 1;
#[doc = " This mask is used to check whether the left SHIFT meta key is pressed."]
pub const AMETA_SHIFT_LEFT_ON: _bindgen_ty_7 = 64;
#[doc = " This mask is used to check whether the right SHIFT meta key is pressed."]
pub const AMETA_SHIFT_RIGHT_ON: _bindgen_ty_7 = 128;
#[doc = " This mask is used to check whether the SYM meta key is pressed."]
pub const AMETA_SYM_ON: _bindgen_ty_7 = 4;
#[doc = " This mask is used to check whether the FUNCTION meta key is pressed."]
pub const AMETA_FUNCTION_ON: _bindgen_ty_7 = 8;
#[doc = " This mask is used to check whether one of the CTRL meta keys is pressed."]
pub const AMETA_CTRL_ON: _bindgen_ty_7 = 4096;
#[doc = " This mask is used to check whether the left CTRL meta key is pressed."]
pub const AMETA_CTRL_LEFT_ON: _bindgen_ty_7 = 8192;
#[doc = " This mask is used to check whether the right CTRL meta key is pressed."]
pub const AMETA_CTRL_RIGHT_ON: _bindgen_ty_7 = 16384;
#[doc = " This mask is used to check whether one of the META meta keys is pressed."]
pub const AMETA_META_ON: _bindgen_ty_7 = 65536;
#[doc = " This mask is used to check whether the left META meta key is pressed."]
pub const AMETA_META_LEFT_ON: _bindgen_ty_7 = 131072;
#[doc = " This mask is used to check whether the right META meta key is pressed."]
pub const AMETA_META_RIGHT_ON: _bindgen_ty_7 = 262144;
#[doc = " This mask is used to check whether the CAPS LOCK meta key is on."]
pub const AMETA_CAPS_LOCK_ON: _bindgen_ty_7 = 1048576;
#[doc = " This mask is used to check whether the NUM LOCK meta key is on."]
pub const AMETA_NUM_LOCK_ON: _bindgen_ty_7 = 2097152;
#[doc = " This mask is used to check whether the SCROLL LOCK meta key is on."]
pub const AMETA_SCROLL_LOCK_ON: _bindgen_ty_7 = 4194304;
#[doc = " Meta key / modifier state."]
pub type _bindgen_ty_7 = ::std::os::raw::c_uint;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AInputEvent {
    _unused: [u8; 0],
}
#[doc = " Indicates that the input event is a key event."]
pub const AINPUT_EVENT_TYPE_KEY: _bindgen_ty_8 = 1;
#[doc = " Indicates that the input event is a motion event."]
pub const AINPUT_EVENT_TYPE_MOTION: _bindgen_ty_8 = 2;
#[doc = " Focus event"]
pub const AINPUT_EVENT_TYPE_FOCUS: _bindgen_ty_8 = 3;
#[doc = " Capture event"]
pub const AINPUT_EVENT_TYPE_CAPTURE: _bindgen_ty_8 = 4;
#[doc = " Drag event"]
pub const AINPUT_EVENT_TYPE_DRAG: _bindgen_ty_8 = 5;
#[doc = " TouchMode event"]
pub const AINPUT_EVENT_TYPE_TOUCH_MODE: _bindgen_ty_8 = 6;
#[doc = " Input event types."]
pub type _bindgen_ty_8 = ::std::os::raw::c_uint;
#[doc = " The key has been pressed down."]
pub const AKEY_EVENT_ACTION_DOWN: _bindgen_ty_9 = 0;
#[doc = " The key has been released."]
pub const AKEY_EVENT_ACTION_UP: _bindgen_ty_9 = 1;
#[doc = " Multiple duplicate key events have occurred in a row, or a\n complex string is being delivered.  The repeat_count property\n of the key event contains the number of times the given key\n code should be executed."]
pub const AKEY_EVENT_ACTION_MULTIPLE: _bindgen_ty_9 = 2;
#[doc = " Key event actions."]
pub type _bindgen_ty_9 = ::std::os::raw::c_uint;
#[doc = " This mask is set if the device woke because of this key event."]
pub const AKEY_EVENT_FLAG_WOKE_HERE: _bindgen_ty_10 = 1;
#[doc = " This mask is set if the key event was generated by a software keyboard."]
pub const AKEY_EVENT_FLAG_SOFT_KEYBOARD: _bindgen_ty_10 = 2;
#[doc = " This mask is set if we don't want the key event to cause us to leave touch mode."]
pub const AKEY_EVENT_FLAG_KEEP_TOUCH_MODE: _bindgen_ty_10 = 4;
#[doc = " This mask is set if an event was known to come from a trusted\n part of the system.  That is, the event is known to come from\n the user, and could not have been spoofed by a third party\n component."]
pub const AKEY_EVENT_FLAG_FROM_SYSTEM: _bindgen_ty_10 = 8;
#[doc = " This mask is used for compatibility, to identify enter keys that are\n coming from an IME whose enter key has been auto-labelled \"next\" or\n \"done\".  This allows TextView to dispatch these as normal enter keys\n for old applications, but still do the appropriate action when\n receiving them."]
pub const AKEY_EVENT_FLAG_EDITOR_ACTION: _bindgen_ty_10 = 16;
#[doc = " When associated with up key events, this indicates that the key press\n has been canceled.  Typically this is used with virtual touch screen\n keys, where the user can slide from the virtual key area on to the\n display: in that case, the application will receive a canceled up\n event and should not perform the action normally associated with the\n key.  Note that for this to work, the application can not perform an\n action for a key until it receives an up or the long press timeout has\n expired."]
pub const AKEY_EVENT_FLAG_CANCELED: _bindgen_ty_10 = 32;
#[doc = " This key event was generated by a virtual (on-screen) hard key area.\n Typically this is an area of the touchscreen, outside of the regular\n display, dedicated to \"hardware\" buttons."]
pub const AKEY_EVENT_FLAG_VIRTUAL_HARD_KEY: _bindgen_ty_10 = 64;
#[doc = " This flag is set for the first key repeat that occurs after the\n long press timeout."]
pub const AKEY_EVENT_FLAG_LONG_PRESS: _bindgen_ty_10 = 128;
#[doc = " Set when a key event has #AKEY_EVENT_FLAG_CANCELED set because a long\n press action was executed while it was down."]
pub const AKEY_EVENT_FLAG_CANCELED_LONG_PRESS: _bindgen_ty_10 = 256;
#[doc = " Set for #AKEY_EVENT_ACTION_UP when this event's key code is still being\n tracked from its initial down.  That is, somebody requested that tracking\n started on the key down and a long press has not caused\n the tracking to be canceled."]
pub const AKEY_EVENT_FLAG_TRACKING: _bindgen_ty_10 = 512;
#[doc = " Set when a key event has been synthesized to implement default behavior\n for an event that the application did not handle.\n Fallback key events are generated by unhandled trackball motions\n (to emulate a directional keypad) and by certain unhandled key presses\n that are declared in the key map (such as special function numeric keypad\n keys when numlock is off)."]
pub const AKEY_EVENT_FLAG_FALLBACK: _bindgen_ty_10 = 1024;
#[doc = " Key event flags."]
pub type _bindgen_ty_10 = ::std::os::raw::c_uint;
#[doc = " Bit mask of the parts of the action code that are the action itself."]
pub const AMOTION_EVENT_ACTION_MASK: _bindgen_ty_11 = 255;
#[doc = " Bits in the action code that represent a pointer index, used with\n #AMOTION_EVENT_ACTION_POINTER_DOWN and AMOTION_EVENT_ACTION_POINTER_UP.  Shifting\n down by #AMOTION_EVENT_ACTION_POINTER_INDEX_SHIFT provides the actual pointer\n index where the data for the pointer going up or down can be found."]
pub const AMOTION_EVENT_ACTION_POINTER_INDEX_MASK: _bindgen_ty_11 = 65280;
#[doc = " A pressed gesture has started, the motion contains the initial starting location."]
pub const AMOTION_EVENT_ACTION_DOWN: _bindgen_ty_11 = 0;
#[doc = " A pressed gesture has finished, the motion contains the final release location\n as well as any intermediate points since the last down or move event."]
pub const AMOTION_EVENT_ACTION_UP: _bindgen_ty_11 = 1;
#[doc = " A change has happened during a press gesture (between #AMOTION_EVENT_ACTION_DOWN and\n #AMOTION_EVENT_ACTION_UP).  The motion contains the most recent point, as well as\n any intermediate points since the last down or move event."]
pub const AMOTION_EVENT_ACTION_MOVE: _bindgen_ty_11 = 2;
#[doc = " The current gesture has been aborted.\n You will not receive any more points in it.  You should treat this as\n an up event, but not perform any action that you normally would."]
pub const AMOTION_EVENT_ACTION_CANCEL: _bindgen_ty_11 = 3;
#[doc = " A movement has happened outside of the normal bounds of the UI element.\n This does not provide a full gesture, but only the initial location of the movement/touch."]
pub const AMOTION_EVENT_ACTION_OUTSIDE: _bindgen_ty_11 = 4;
#[doc = " A non-primary pointer has gone down.\n The bits in #AMOTION_EVENT_ACTION_POINTER_INDEX_MASK indicate which pointer changed."]
pub const AMOTION_EVENT_ACTION_POINTER_DOWN: _bindgen_ty_11 = 5;
#[doc = " A non-primary pointer has gone up.\n The bits in #AMOTION_EVENT_ACTION_POINTER_INDEX_MASK indicate which pointer changed."]
pub const AMOTION_EVENT_ACTION_POINTER_UP: _bindgen_ty_11 = 6;
#[doc = " A change happened but the pointer is not down (unlike #AMOTION_EVENT_ACTION_MOVE).\n The motion contains the most recent point, as well as any intermediate points since\n the last hover move event."]
pub const AMOTION_EVENT_ACTION_HOVER_MOVE: _bindgen_ty_11 = 7;
#[doc = " The motion event contains relative vertical and/or horizontal scroll offsets.\n Use {@link AMotionEvent_getAxisValue} to retrieve the information from\n #AMOTION_EVENT_AXIS_VSCROLL and #AMOTION_EVENT_AXIS_HSCROLL.\n The pointer may or may not be down when this event is dispatched.\n This action is always delivered to the winder under the pointer, which\n may not be the window currently touched."]
pub const AMOTION_EVENT_ACTION_SCROLL: _bindgen_ty_11 = 8;
#[doc = " The pointer is not down but has entered the boundaries of a window or view."]
pub const AMOTION_EVENT_ACTION_HOVER_ENTER: _bindgen_ty_11 = 9;
#[doc = " The pointer is not down but has exited the boundaries of a window or view."]
pub const AMOTION_EVENT_ACTION_HOVER_EXIT: _bindgen_ty_11 = 10;
#[doc = " The pointer is not down but has exited the boundaries of a window or view."]
pub const AMOTION_EVENT_ACTION_BUTTON_PRESS: _bindgen_ty_11 = 11;
#[doc = " The pointer is not down but has exited the boundaries of a window or view."]
pub const AMOTION_EVENT_ACTION_BUTTON_RELEASE: _bindgen_ty_11 = 12;
#[doc = " Motion event actions"]
pub type _bindgen_ty_11 = ::std::os::raw::c_uint;
#[doc = " This flag indicates that the window that received this motion event is partly\n or wholly obscured by another visible window above it.  This flag is set to true\n even if the event did not directly pass through the obscured area.\n A security sensitive application can check this flag to identify situations in which\n a malicious application may have covered up part of its content for the purpose\n of misleading the user or hijacking touches.  An appropriate response might be\n to drop the suspect touches or to take additional precautions to confirm the user's\n actual intent."]
pub const AMOTION_EVENT_FLAG_WINDOW_IS_OBSCURED: _bindgen_ty_12 = 1;
#[doc = " Motion event flags."]
pub type _bindgen_ty_12 = ::std::os::raw::c_uint;
#[doc = " No edges intersected."]
pub const AMOTION_EVENT_EDGE_FLAG_NONE: _bindgen_ty_13 = 0;
#[doc = " Flag indicating the motion event intersected the top edge of the screen."]
pub const AMOTION_EVENT_EDGE_FLAG_TOP: _bindgen_ty_13 = 1;
#[doc = " Flag indicating the motion event intersected the bottom edge of the screen."]
pub const AMOTION_EVENT_EDGE_FLAG_BOTTOM: _bindgen_ty_13 = 2;
#[doc = " Flag indicating the motion event intersected the left edge of the screen."]
pub const AMOTION_EVENT_EDGE_FLAG_LEFT: _bindgen_ty_13 = 4;
#[doc = " Flag indicating the motion event intersected the right edge of the screen."]
pub const AMOTION_EVENT_EDGE_FLAG_RIGHT: _bindgen_ty_13 = 8;
#[doc = " Motion event edge touch flags."]
pub type _bindgen_ty_13 = ::std::os::raw::c_uint;
#[doc = " Axis constant: X axis of a motion event.\n\n - For a touch screen, reports the absolute X screen position of the center of\n the touch contact area.  The units are display pixels.\n - For a touch pad, reports the absolute X surface position of the center of the touch\n contact area. The units are device-dependent.\n - For a mouse, reports the absolute X screen position of the mouse pointer.\n The units are display pixels.\n - For a trackball, reports the relative horizontal displacement of the trackball.\n The value is normalized to a range from -1.0 (left) to 1.0 (right).\n - For a joystick, reports the absolute X position of the joystick.\n The value is normalized to a range from -1.0 (left) to 1.0 (right)."]
pub const AMOTION_EVENT_AXIS_X: _bindgen_ty_14 = 0;
#[doc = " Axis constant: Y axis of a motion event.\n\n - For a touch screen, reports the absolute Y screen position of the center of\n the touch contact area.  The units are display pixels.\n - For a touch pad, reports the absolute Y surface position of the center of the touch\n contact area. The units are device-dependent.\n - For a mouse, reports the absolute Y screen position of the mouse pointer.\n The units are display pixels.\n - For a trackball, reports the relative vertical displacement of the trackball.\n The value is normalized to a range from -1.0 (up) to 1.0 (down).\n - For a joystick, reports the absolute Y position of the joystick.\n The value is normalized to a range from -1.0 (up or far) to 1.0 (down or near)."]
pub const AMOTION_EVENT_AXIS_Y: _bindgen_ty_14 = 1;
#[doc = " Axis constant: Pressure axis of a motion event.\n\n - For a touch screen or touch pad, reports the approximate pressure applied to the surface\n by a finger or other tool.  The value is normalized to a range from\n 0 (no pressure at all) to 1 (normal pressure), although values higher than 1\n may be generated depending on the calibration of the input device.\n - For a trackball, the value is set to 1 if the trackball button is pressed\n or 0 otherwise.\n - For a mouse, the value is set to 1 if the primary mouse button is pressed\n or 0 otherwise."]
pub const AMOTION_EVENT_AXIS_PRESSURE: _bindgen_ty_14 = 2;
#[doc = " Axis constant: Size axis of a motion event.\n\n - For a touch screen or touch pad, reports the approximate size of the contact area in\n relation to the maximum detectable size for the device.  The value is normalized\n to a range from 0 (smallest detectable size) to 1 (largest detectable size),\n although it is not a linear scale. This value is of limited use.\n To obtain calibrated size information, see\n {@link AMOTION_EVENT_AXIS_TOUCH_MAJOR} or {@link AMOTION_EVENT_AXIS_TOOL_MAJOR}."]
pub const AMOTION_EVENT_AXIS_SIZE: _bindgen_ty_14 = 3;
#[doc = " Axis constant: TouchMajor axis of a motion event.\n\n - For a touch screen, reports the length of the major axis of an ellipse that\n represents the touch area at the point of contact.\n The units are display pixels.\n - For a touch pad, reports the length of the major axis of an ellipse that\n represents the touch area at the point of contact.\n The units are device-dependent."]
pub const AMOTION_EVENT_AXIS_TOUCH_MAJOR: _bindgen_ty_14 = 4;
#[doc = " Axis constant: TouchMinor axis of a motion event.\n\n - For a touch screen, reports the length of the minor axis of an ellipse that\n represents the touch area at the point of contact.\n The units are display pixels.\n - For a touch pad, reports the length of the minor axis of an ellipse that\n represents the touch area at the point of contact.\n The units are device-dependent.\n\n When the touch is circular, the major and minor axis lengths will be equal to one another."]
pub const AMOTION_EVENT_AXIS_TOUCH_MINOR: _bindgen_ty_14 = 5;
#[doc = " Axis constant: ToolMajor axis of a motion event.\n\n - For a touch screen, reports the length of the major axis of an ellipse that\n represents the size of the approaching finger or tool used to make contact.\n - For a touch pad, reports the length of the major axis of an ellipse that\n represents the size of the approaching finger or tool used to make contact.\n The units are device-dependent.\n\n When the touch is circular, the major and minor axis lengths will be equal to one another.\n\n The tool size may be larger than the touch size since the tool may not be fully\n in contact with the touch sensor."]
pub const AMOTION_EVENT_AXIS_TOOL_MAJOR: _bindgen_ty_14 = 6;
#[doc = " Axis constant: ToolMinor axis of a motion event.\n\n - For a touch screen, reports the length of the minor axis of an ellipse that\n represents the size of the approaching finger or tool used to make contact.\n - For a touch pad, reports the length of the minor axis of an ellipse that\n represents the size of the approaching finger or tool used to make contact.\n The units are device-dependent.\n\n When the touch is circular, the major and minor axis lengths will be equal to one another.\n\n The tool size may be larger than the touch size since the tool may not be fully\n in contact with the touch sensor."]
pub const AMOTION_EVENT_AXIS_TOOL_MINOR: _bindgen_ty_14 = 7;
#[doc = " Axis constant: Orientation axis of a motion event.\n\n - For a touch screen or touch pad, reports the orientation of the finger\n or tool in radians relative to the vertical plane of the device.\n An angle of 0 radians indicates that the major axis of contact is oriented\n upwards, is perfectly circular or is of unknown orientation.  A positive angle\n indicates that the major axis of contact is oriented to the right.  A negative angle\n indicates that the major axis of contact is oriented to the left.\n The full range is from -PI/2 radians (finger pointing fully left) to PI/2 radians\n (finger pointing fully right).\n - For a stylus, the orientation indicates the direction in which the stylus\n is pointing in relation to the vertical axis of the current orientation of the screen.\n The range is from -PI radians to PI radians, where 0 is pointing up,\n -PI/2 radians is pointing left, -PI or PI radians is pointing down, and PI/2 radians\n is pointing right.  See also #AMOTION_EVENT_AXIS_TILT."]
pub const AMOTION_EVENT_AXIS_ORIENTATION: _bindgen_ty_14 = 8;
#[doc = " Axis constant: Vertical Scroll axis of a motion event.\n\n - For a mouse, reports the relative movement of the vertical scroll wheel.\n The value is normalized to a range from -1.0 (down) to 1.0 (up).\n\n This axis should be used to scroll views vertically."]
pub const AMOTION_EVENT_AXIS_VSCROLL: _bindgen_ty_14 = 9;
#[doc = " Axis constant: Horizontal Scroll axis of a motion event.\n\n - For a mouse, reports the relative movement of the horizontal scroll wheel.\n The value is normalized to a range from -1.0 (left) to 1.0 (right).\n\n This axis should be used to scroll views horizontally."]
pub const AMOTION_EVENT_AXIS_HSCROLL: _bindgen_ty_14 = 10;
#[doc = " Axis constant: Z axis of a motion event.\n\n - For a joystick, reports the absolute Z position of the joystick.\n The value is normalized to a range from -1.0 (high) to 1.0 (low).\n <em>On game pads with two analog joysticks, this axis is often reinterpreted\n to report the absolute X position of the second joystick instead.</em>"]
pub const AMOTION_EVENT_AXIS_Z: _bindgen_ty_14 = 11;
#[doc = " Axis constant: X Rotation axis of a motion event.\n\n - For a joystick, reports the absolute rotation angle about the X axis.\n The value is normalized to a range from -1.0 (counter-clockwise) to 1.0 (clockwise)."]
pub const AMOTION_EVENT_AXIS_RX: _bindgen_ty_14 = 12;
#[doc = " Axis constant: Y Rotation axis of a motion event.\n\n - For a joystick, reports the absolute rotation angle about the Y axis.\n The value is normalized to a range from -1.0 (counter-clockwise) to 1.0 (clockwise)."]
pub const AMOTION_EVENT_AXIS_RY: _bindgen_ty_14 = 13;
#[doc = " Axis constant: Z Rotation axis of a motion event.\n\n - For a joystick, reports the absolute rotation angle about the Z axis.\n The value is normalized to a range from -1.0 (counter-clockwise) to 1.0 (clockwise).\n On game pads with two analog joysticks, this axis is often reinterpreted\n to report the absolute Y position of the second joystick instead."]
pub const AMOTION_EVENT_AXIS_RZ: _bindgen_ty_14 = 14;
#[doc = " Axis constant: Hat X axis of a motion event.\n\n - For a joystick, reports the absolute X position of the directional hat control.\n The value is normalized to a range from -1.0 (left) to 1.0 (right)."]
pub const AMOTION_EVENT_AXIS_HAT_X: _bindgen_ty_14 = 15;
#[doc = " Axis constant: Hat Y axis of a motion event.\n\n - For a joystick, reports the absolute Y position of the directional hat control.\n The value is normalized to a range from -1.0 (up) to 1.0 (down)."]
pub const AMOTION_EVENT_AXIS_HAT_Y: _bindgen_ty_14 = 16;
#[doc = " Axis constant: Left Trigger axis of a motion event.\n\n - For a joystick, reports the absolute position of the left trigger control.\n The value is normalized to a range from 0.0 (released) to 1.0 (fully pressed)."]
pub const AMOTION_EVENT_AXIS_LTRIGGER: _bindgen_ty_14 = 17;
#[doc = " Axis constant: Right Trigger axis of a motion event.\n\n - For a joystick, reports the absolute position of the right trigger control.\n The value is normalized to a range from 0.0 (released) to 1.0 (fully pressed)."]
pub const AMOTION_EVENT_AXIS_RTRIGGER: _bindgen_ty_14 = 18;
#[doc = " Axis constant: Throttle axis of a motion event.\n\n - For a joystick, reports the absolute position of the throttle control.\n The value is normalized to a range from 0.0 (fully open) to 1.0 (fully closed)."]
pub const AMOTION_EVENT_AXIS_THROTTLE: _bindgen_ty_14 = 19;
#[doc = " Axis constant: Rudder axis of a motion event.\n\n - For a joystick, reports the absolute position of the rudder control.\n The value is normalized to a range from -1.0 (turn left) to 1.0 (turn right)."]
pub const AMOTION_EVENT_AXIS_RUDDER: _bindgen_ty_14 = 20;
#[doc = " Axis constant: Wheel axis of a motion event.\n\n - For a joystick, reports the absolute position of the steering wheel control.\n The value is normalized to a range from -1.0 (turn left) to 1.0 (turn right)."]
pub const AMOTION_EVENT_AXIS_WHEEL: _bindgen_ty_14 = 21;
#[doc = " Axis constant: Gas axis of a motion event.\n\n - For a joystick, reports the absolute position of the gas (accelerator) control.\n The value is normalized to a range from 0.0 (no acceleration)\n to 1.0 (maximum acceleration)."]
pub const AMOTION_EVENT_AXIS_GAS: _bindgen_ty_14 = 22;
#[doc = " Axis constant: Brake axis of a motion event.\n\n - For a joystick, reports the absolute position of the brake control.\n The value is normalized to a range from 0.0 (no braking) to 1.0 (maximum braking)."]
pub const AMOTION_EVENT_AXIS_BRAKE: _bindgen_ty_14 = 23;
#[doc = " Axis constant: Distance axis of a motion event.\n\n - For a stylus, reports the distance of the stylus from the screen.\n A value of 0.0 indicates direct contact and larger values indicate increasing\n distance from the surface."]
pub const AMOTION_EVENT_AXIS_DISTANCE: _bindgen_ty_14 = 24;
#[doc = " Axis constant: Tilt axis of a motion event.\n\n - For a stylus, reports the tilt angle of the stylus in radians where\n 0 radians indicates that the stylus is being held perpendicular to the\n surface, and PI/2 radians indicates that the stylus is being held flat\n against the surface."]
pub const AMOTION_EVENT_AXIS_TILT: _bindgen_ty_14 = 25;
#[doc = " Axis constant:  Generic scroll axis of a motion event.\n\n - This is used for scroll axis motion events that can't be classified as strictly\n   vertical or horizontal. The movement of a rotating scroller is an example of this."]
pub const AMOTION_EVENT_AXIS_SCROLL: _bindgen_ty_14 = 26;
#[doc = " Axis constant: The movement of x position of a motion event.\n\n - For a mouse, reports a difference of x position between the previous position.\n This is useful when pointer is captured, in that case the mouse pointer doesn't\n change the location but this axis reports the difference which allows the app\n to see how the mouse is moved."]
pub const AMOTION_EVENT_AXIS_RELATIVE_X: _bindgen_ty_14 = 27;
#[doc = " Axis constant: The movement of y position of a motion event.\n\n Same as #AMOTION_EVENT_AXIS_RELATIVE_X, but for y position."]
pub const AMOTION_EVENT_AXIS_RELATIVE_Y: _bindgen_ty_14 = 28;
#[doc = " Axis constant: Generic 1 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_1: _bindgen_ty_14 = 32;
#[doc = " Axis constant: Generic 2 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_2: _bindgen_ty_14 = 33;
#[doc = " Axis constant: Generic 3 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_3: _bindgen_ty_14 = 34;
#[doc = " Axis constant: Generic 4 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_4: _bindgen_ty_14 = 35;
#[doc = " Axis constant: Generic 5 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_5: _bindgen_ty_14 = 36;
#[doc = " Axis constant: Generic 6 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_6: _bindgen_ty_14 = 37;
#[doc = " Axis constant: Generic 7 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_7: _bindgen_ty_14 = 38;
#[doc = " Axis constant: Generic 8 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_8: _bindgen_ty_14 = 39;
#[doc = " Axis constant: Generic 9 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_9: _bindgen_ty_14 = 40;
#[doc = " Axis constant: Generic 10 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_10: _bindgen_ty_14 = 41;
#[doc = " Axis constant: Generic 11 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_11: _bindgen_ty_14 = 42;
#[doc = " Axis constant: Generic 12 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_12: _bindgen_ty_14 = 43;
#[doc = " Axis constant: Generic 13 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_13: _bindgen_ty_14 = 44;
#[doc = " Axis constant: Generic 14 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_14: _bindgen_ty_14 = 45;
#[doc = " Axis constant: Generic 15 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_15: _bindgen_ty_14 = 46;
#[doc = " Axis constant: Generic 16 axis of a motion event.\n The interpretation of a generic axis is device-specific."]
pub const AMOTION_EVENT_AXIS_GENERIC_16: _bindgen_ty_14 = 47;
#[doc = " Constants that identify each individual axis of a motion event.\n @anchor AMOTION_EVENT_AXIS"]
pub type _bindgen_ty_14 = ::std::os::raw::c_uint;
#[doc = " primary"]
pub const AMOTION_EVENT_BUTTON_PRIMARY: _bindgen_ty_15 = 1;
#[doc = " secondary"]
pub const AMOTION_EVENT_BUTTON_SECONDARY: _bindgen_ty_15 = 2;
#[doc = " tertiary"]
pub const AMOTION_EVENT_BUTTON_TERTIARY: _bindgen_ty_15 = 4;
#[doc = " back"]
pub const AMOTION_EVENT_BUTTON_BACK: _bindgen_ty_15 = 8;
#[doc = " forward"]
pub const AMOTION_EVENT_BUTTON_FORWARD: _bindgen_ty_15 = 16;
#[doc = " forward"]
pub const AMOTION_EVENT_BUTTON_STYLUS_PRIMARY: _bindgen_ty_15 = 32;
#[doc = " forward"]
pub const AMOTION_EVENT_BUTTON_STYLUS_SECONDARY: _bindgen_ty_15 = 64;
#[doc = " Constants that identify buttons that are associated with motion events.\n Refer to the documentation on the MotionEvent class for descriptions of each button."]
pub type _bindgen_ty_15 = ::std::os::raw::c_uint;
#[doc = " unknown"]
pub const AMOTION_EVENT_TOOL_TYPE_UNKNOWN: _bindgen_ty_16 = 0;
#[doc = " finger"]
pub const AMOTION_EVENT_TOOL_TYPE_FINGER: _bindgen_ty_16 = 1;
#[doc = " stylus"]
pub const AMOTION_EVENT_TOOL_TYPE_STYLUS: _bindgen_ty_16 = 2;
#[doc = " mouse"]
pub const AMOTION_EVENT_TOOL_TYPE_MOUSE: _bindgen_ty_16 = 3;
#[doc = " eraser"]
pub const AMOTION_EVENT_TOOL_TYPE_ERASER: _bindgen_ty_16 = 4;
#[doc = " palm"]
pub const AMOTION_EVENT_TOOL_TYPE_PALM: _bindgen_ty_16 = 5;
#[doc = " Constants that identify tool types.\n Refer to the documentation on the MotionEvent class for descriptions of each tool type."]
pub type _bindgen_ty_16 = ::std::os::raw::c_uint;
#[doc = " Classification constant: None.\n\n No additional information is available about the current motion event stream."]
pub const AMotionClassification_AMOTION_EVENT_CLASSIFICATION_NONE: AMotionClassification = 0;
#[doc = " Classification constant: Ambiguous gesture.\n\n The user's intent with respect to the current event stream is not yet determined. Events\n starting in #AMOTION_EVENT_CLASSIFICATION_AMBIGUOUS_GESTURE will eventually resolve into\n either #AMOTION_EVENT_CLASSIFICATION_DEEP_PRESS or #AMOTION_EVENT_CLASSIFICATION_NONE.\n Gestural actions, such as scrolling, should be inhibited until the classification resolves\n to another value or the event stream ends."]
pub const AMotionClassification_AMOTION_EVENT_CLASSIFICATION_AMBIGUOUS_GESTURE:
    AMotionClassification = 1;
#[doc = " Classification constant: Deep press.\n\n The current event stream represents the user intentionally pressing harder on the screen.\n This classification type should be used to accelerate the long press behaviour."]
pub const AMotionClassification_AMOTION_EVENT_CLASSIFICATION_DEEP_PRESS: AMotionClassification = 2;
#[doc = " Constants that identify different gesture classification types."]
pub type AMotionClassification = u32;
#[doc = " mask"]
pub const AINPUT_SOURCE_CLASS_MASK: _bindgen_ty_17 = 255;
#[doc = " none"]
pub const AINPUT_SOURCE_CLASS_NONE: _bindgen_ty_17 = 0;
#[doc = " button"]
pub const AINPUT_SOURCE_CLASS_BUTTON: _bindgen_ty_17 = 1;
#[doc = " pointer"]
pub const AINPUT_SOURCE_CLASS_POINTER: _bindgen_ty_17 = 2;
#[doc = " navigation"]
pub const AINPUT_SOURCE_CLASS_NAVIGATION: _bindgen_ty_17 = 4;
#[doc = " position"]
pub const AINPUT_SOURCE_CLASS_POSITION: _bindgen_ty_17 = 8;
#[doc = " joystick"]
pub const AINPUT_SOURCE_CLASS_JOYSTICK: _bindgen_ty_17 = 16;
#[doc = " Input source masks.\n\n Refer to the documentation on android.view.InputDevice for more details about input sources\n and their correct interpretation."]
pub type _bindgen_ty_17 = ::std::os::raw::c_uint;
#[doc = " unknown"]
pub const AINPUT_SOURCE_UNKNOWN: _bindgen_ty_18 = 0;
#[doc = " keyboard"]
pub const AINPUT_SOURCE_KEYBOARD: _bindgen_ty_18 = 257;
#[doc = " dpad"]
pub const AINPUT_SOURCE_DPAD: _bindgen_ty_18 = 513;
#[doc = " gamepad"]
pub const AINPUT_SOURCE_GAMEPAD: _bindgen_ty_18 = 1025;
#[doc = " touchscreen"]
pub const AINPUT_SOURCE_TOUCHSCREEN: _bindgen_ty_18 = 4098;
#[doc = " mouse"]
pub const AINPUT_SOURCE_MOUSE: _bindgen_ty_18 = 8194;
#[doc = " stylus"]
pub const AINPUT_SOURCE_STYLUS: _bindgen_ty_18 = 16386;
#[doc = " bluetooth stylus"]
pub const AINPUT_SOURCE_BLUETOOTH_STYLUS: _bindgen_ty_18 = 49154;
#[doc = " trackball"]
pub const AINPUT_SOURCE_TRACKBALL: _bindgen_ty_18 = 65540;
#[doc = " mouse relative"]
pub const AINPUT_SOURCE_MOUSE_RELATIVE: _bindgen_ty_18 = 131076;
#[doc = " touchpad"]
pub const AINPUT_SOURCE_TOUCHPAD: _bindgen_ty_18 = 1048584;
#[doc = " navigation"]
pub const AINPUT_SOURCE_TOUCH_NAVIGATION: _bindgen_ty_18 = 2097152;
#[doc = " joystick"]
pub const AINPUT_SOURCE_JOYSTICK: _bindgen_ty_18 = 16777232;
#[doc = " HDMI"]
pub const AINPUT_SOURCE_HDMI: _bindgen_ty_18 = 33554433;
#[doc = " sensor"]
pub const AINPUT_SOURCE_SENSOR: _bindgen_ty_18 = 67108864;
#[doc = " rotary encoder"]
pub const AINPUT_SOURCE_ROTARY_ENCODER: _bindgen_ty_18 = 4194304;
#[doc = " any"]
pub const AINPUT_SOURCE_ANY: _bindgen_ty_18 = 4294967040;
#[doc = " Input sources."]
pub type _bindgen_ty_18 = ::std::os::raw::c_uint;
#[doc = " none"]
pub const AINPUT_KEYBOARD_TYPE_NONE: _bindgen_ty_19 = 0;
#[doc = " non alphabetic"]
pub const AINPUT_KEYBOARD_TYPE_NON_ALPHABETIC: _bindgen_ty_19 = 1;
#[doc = " alphabetic"]
pub const AINPUT_KEYBOARD_TYPE_ALPHABETIC: _bindgen_ty_19 = 2;
#[doc = " Keyboard types.\n\n Refer to the documentation on android.view.InputDevice for more details.\n Note: When adding a new keyboard type here InputDeviceInfo::setKeyboardType needs to be updated."]
pub type _bindgen_ty_19 = ::std::os::raw::c_uint;
#[doc = " x"]
pub const AINPUT_MOTION_RANGE_X: _bindgen_ty_20 = 0;
#[doc = " y"]
pub const AINPUT_MOTION_RANGE_Y: _bindgen_ty_20 = 1;
#[doc = " pressure"]
pub const AINPUT_MOTION_RANGE_PRESSURE: _bindgen_ty_20 = 2;
#[doc = " size"]
pub const AINPUT_MOTION_RANGE_SIZE: _bindgen_ty_20 = 3;
#[doc = " touch major"]
pub const AINPUT_MOTION_RANGE_TOUCH_MAJOR: _bindgen_ty_20 = 4;
#[doc = " touch minor"]
pub const AINPUT_MOTION_RANGE_TOUCH_MINOR: _bindgen_ty_20 = 5;
#[doc = " tool major"]
pub const AINPUT_MOTION_RANGE_TOOL_MAJOR: _bindgen_ty_20 = 6;
#[doc = " tool minor"]
pub const AINPUT_MOTION_RANGE_TOOL_MINOR: _bindgen_ty_20 = 7;
#[doc = " orientation"]
pub const AINPUT_MOTION_RANGE_ORIENTATION: _bindgen_ty_20 = 8;
#[doc = " Constants used to retrieve information about the range of motion for a particular\n coordinate of a motion event.\n\n Refer to the documentation on android.view.InputDevice for more details about input sources\n and their correct interpretation.\n\n @deprecated These constants are deprecated. Use {@link AMOTION_EVENT_AXIS AMOTION_EVENT_AXIS_*}\n constants instead."]
pub type _bindgen_ty_20 = ::std::os::raw::c_uint;
extern "C" {
    #[doc = " Get the input event type."]
    pub fn AInputEvent_getType(event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the id for the device that an input event came from.\n\n Input events can be generated by multiple different input devices.\n Use the input device id to obtain information about the input\n device that was responsible for generating a particular event.\n\n An input device id of 0 indicates that the event didn't come from a physical device;\n other numbers are arbitrary and you shouldn't depend on the values.\n Use the provided input device query API to obtain information about input devices."]
    pub fn AInputEvent_getDeviceId(event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the input event source."]
    pub fn AInputEvent_getSource(event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Releases interface objects created by {@link AKeyEvent_fromJava()}\n and {@link AMotionEvent_fromJava()}.\n After returning, the specified {@link AInputEvent}* object becomes invalid and should no longer\n be used. The underlying Java object remains valid and does not change its state.\n\n Available since API level 31."]
    pub fn AInputEvent_release(event: *const AInputEvent);
}
extern "C" {
    #[doc = " Get the key event action."]
    pub fn AKeyEvent_getAction(key_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the key event flags."]
    pub fn AKeyEvent_getFlags(key_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the key code of the key event.\n This is the physical key that was pressed, not the Unicode character."]
    pub fn AKeyEvent_getKeyCode(key_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the hardware key id of this key event.\n These values are not reliable and vary from device to device."]
    pub fn AKeyEvent_getScanCode(key_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the meta key state."]
    pub fn AKeyEvent_getMetaState(key_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the repeat count of the event.\n For both key up an key down events, this is the number of times the key has\n repeated with the first down starting at 0 and counting up from there.  For\n multiple key events, this is the number of down/up pairs that have occurred."]
    pub fn AKeyEvent_getRepeatCount(key_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the time of the most recent key down event, in the\n java.lang.System.nanoTime() time base.  If this is a down event,\n this will be the same as eventTime.\n Note that when chording keys, this value is the down time of the most recently\n pressed key, which may not be the same physical key of this event."]
    pub fn AKeyEvent_getDownTime(key_event: *const AInputEvent) -> i64;
}
extern "C" {
    #[doc = " Get the time this event occurred, in the\n java.lang.System.nanoTime() time base."]
    pub fn AKeyEvent_getEventTime(key_event: *const AInputEvent) -> i64;
}
extern "C" {
    #[doc = " Creates a native {@link AInputEvent}* object that is a copy of the specified Java\n android.view.KeyEvent. The result may be used with generic and KeyEvent-specific AInputEvent_*\n functions. The object returned by this function must be disposed using\n {@link AInputEvent_release()}.\n\n Available since API level 31."]
    pub fn AKeyEvent_fromJava(env: *mut JNIEnv, keyEvent: jobject) -> *const AInputEvent;
}
extern "C" {
    #[doc = " Get the combined motion event action code and pointer index."]
    pub fn AMotionEvent_getAction(motion_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the motion event flags."]
    pub fn AMotionEvent_getFlags(motion_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the state of any meta / modifier keys that were in effect when the\n event was generated."]
    pub fn AMotionEvent_getMetaState(motion_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the button state of all buttons that are pressed."]
    pub fn AMotionEvent_getButtonState(motion_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get a bitfield indicating which edges, if any, were touched by this motion event.\n For touch events, clients can use this to determine if the user's finger was\n touching the edge of the display."]
    pub fn AMotionEvent_getEdgeFlags(motion_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Get the time when the user originally pressed down to start a stream of\n position events, in the java.lang.System.nanoTime() time base."]
    pub fn AMotionEvent_getDownTime(motion_event: *const AInputEvent) -> i64;
}
extern "C" {
    #[doc = " Get the time when this specific event was generated,\n in the java.lang.System.nanoTime() time base."]
    pub fn AMotionEvent_getEventTime(motion_event: *const AInputEvent) -> i64;
}
extern "C" {
    #[doc = " Get the X coordinate offset.\n For touch events on the screen, this is the delta that was added to the raw\n screen coordinates to adjust for the absolute position of the containing windows\n and views."]
    pub fn AMotionEvent_getXOffset(motion_event: *const AInputEvent) -> f32;
}
extern "C" {
    #[doc = " Get the Y coordinate offset.\n For touch events on the screen, this is the delta that was added to the raw\n screen coordinates to adjust for the absolute position of the containing windows\n and views."]
    pub fn AMotionEvent_getYOffset(motion_event: *const AInputEvent) -> f32;
}
extern "C" {
    #[doc = " Get the precision of the X coordinates being reported.\n You can multiply this number with an X coordinate sample to find the\n actual hardware value of the X coordinate."]
    pub fn AMotionEvent_getXPrecision(motion_event: *const AInputEvent) -> f32;
}
extern "C" {
    #[doc = " Get the precision of the Y coordinates being reported.\n You can multiply this number with a Y coordinate sample to find the\n actual hardware value of the Y coordinate."]
    pub fn AMotionEvent_getYPrecision(motion_event: *const AInputEvent) -> f32;
}
extern "C" {
    #[doc = " Get the number of pointers of data contained in this event.\n Always >= 1."]
    pub fn AMotionEvent_getPointerCount(motion_event: *const AInputEvent) -> usize;
}
extern "C" {
    #[doc = " Get the pointer identifier associated with a particular pointer\n data index in this event.  The identifier tells you the actual pointer\n number associated with the data, accounting for individual pointers\n going up and down since the start of the current gesture."]
    pub fn AMotionEvent_getPointerId(motion_event: *const AInputEvent, pointer_index: usize)
        -> i32;
}
extern "C" {
    #[doc = " Get the tool type of a pointer for the given pointer index.\n The tool type indicates the type of tool used to make contact such as a\n finger or stylus, if known."]
    pub fn AMotionEvent_getToolType(motion_event: *const AInputEvent, pointer_index: usize) -> i32;
}
extern "C" {
    #[doc = " Get the original raw X coordinate of this event.\n For touch events on the screen, this is the original location of the event\n on the screen, before it had been adjusted for the containing window\n and views."]
    pub fn AMotionEvent_getRawX(motion_event: *const AInputEvent, pointer_index: usize) -> f32;
}
extern "C" {
    #[doc = " Get the original raw X coordinate of this event.\n For touch events on the screen, this is the original location of the event\n on the screen, before it had been adjusted for the containing window\n and views."]
    pub fn AMotionEvent_getRawY(motion_event: *const AInputEvent, pointer_index: usize) -> f32;
}
extern "C" {
    #[doc = " Get the current X coordinate of this event for the given pointer index.\n Whole numbers are pixels; the value may have a fraction for input devices\n that are sub-pixel precise."]
    pub fn AMotionEvent_getX(motion_event: *const AInputEvent, pointer_index: usize) -> f32;
}
extern "C" {
    #[doc = " Get the current Y coordinate of this event for the given pointer index.\n Whole numbers are pixels; the value may have a fraction for input devices\n that are sub-pixel precise."]
    pub fn AMotionEvent_getY(motion_event: *const AInputEvent, pointer_index: usize) -> f32;
}
extern "C" {
    #[doc = " Get the current pressure of this event for the given pointer index.\n The pressure generally ranges from 0 (no pressure at all) to 1 (normal pressure),\n although values higher than 1 may be generated depending on the calibration of\n the input device."]
    pub fn AMotionEvent_getPressure(motion_event: *const AInputEvent, pointer_index: usize) -> f32;
}
extern "C" {
    #[doc = " Get the current scaled value of the approximate size for the given pointer index.\n This represents some approximation of the area of the screen being\n pressed; the actual value in pixels corresponding to the\n touch is normalized with the device specific range of values\n and scaled to a value between 0 and 1.  The value of size can be used to\n determine fat touch events."]
    pub fn AMotionEvent_getSize(motion_event: *const AInputEvent, pointer_index: usize) -> f32;
}
extern "C" {
    #[doc = " Get the current length of the major axis of an ellipse that describes the touch area\n at the point of contact for the given pointer index."]
    pub fn AMotionEvent_getTouchMajor(
        motion_event: *const AInputEvent,
        pointer_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the current length of the minor axis of an ellipse that describes the touch area\n at the point of contact for the given pointer index."]
    pub fn AMotionEvent_getTouchMinor(
        motion_event: *const AInputEvent,
        pointer_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the current length of the major axis of an ellipse that describes the size\n of the approaching tool for the given pointer index.\n The tool area represents the estimated size of the finger or pen that is\n touching the device independent of its actual touch area at the point of contact."]
    pub fn AMotionEvent_getToolMajor(motion_event: *const AInputEvent, pointer_index: usize)
        -> f32;
}
extern "C" {
    #[doc = " Get the current length of the minor axis of an ellipse that describes the size\n of the approaching tool for the given pointer index.\n The tool area represents the estimated size of the finger or pen that is\n touching the device independent of its actual touch area at the point of contact."]
    pub fn AMotionEvent_getToolMinor(motion_event: *const AInputEvent, pointer_index: usize)
        -> f32;
}
extern "C" {
    #[doc = " Get the current orientation of the touch area and tool area in radians clockwise from\n vertical for the given pointer index.\n An angle of 0 degrees indicates that the major axis of contact is oriented\n upwards, is perfectly circular or is of unknown orientation.  A positive angle\n indicates that the major axis of contact is oriented to the right.  A negative angle\n indicates that the major axis of contact is oriented to the left.\n The full range is from -PI/2 radians (finger pointing fully left) to PI/2 radians\n (finger pointing fully right)."]
    pub fn AMotionEvent_getOrientation(
        motion_event: *const AInputEvent,
        pointer_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the value of the request axis for the given pointer index."]
    pub fn AMotionEvent_getAxisValue(
        motion_event: *const AInputEvent,
        axis: i32,
        pointer_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the number of historical points in this event.  These are movements that\n have occurred between this event and the previous event.  This only applies\n to #AMOTION_EVENT_ACTION_MOVE events -- all other actions will have a size of 0.\n Historical samples are indexed from oldest to newest."]
    pub fn AMotionEvent_getHistorySize(motion_event: *const AInputEvent) -> usize;
}
extern "C" {
    #[doc = " Get the time that a historical movement occurred between this event and\n the previous event, in the java.lang.System.nanoTime() time base."]
    pub fn AMotionEvent_getHistoricalEventTime(
        motion_event: *const AInputEvent,
        history_index: usize,
    ) -> i64;
}
extern "C" {
    #[doc = " Get the historical raw X coordinate of this event for the given pointer index that\n occurred between this event and the previous motion event.\n For touch events on the screen, this is the original location of the event\n on the screen, before it had been adjusted for the containing window\n and views.\n Whole numbers are pixels; the value may have a fraction for input devices\n that are sub-pixel precise."]
    pub fn AMotionEvent_getHistoricalRawX(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the historical raw Y coordinate of this event for the given pointer index that\n occurred between this event and the previous motion event.\n For touch events on the screen, this is the original location of the event\n on the screen, before it had been adjusted for the containing window\n and views.\n Whole numbers are pixels; the value may have a fraction for input devices\n that are sub-pixel precise."]
    pub fn AMotionEvent_getHistoricalRawY(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the historical X coordinate of this event for the given pointer index that\n occurred between this event and the previous motion event.\n Whole numbers are pixels; the value may have a fraction for input devices\n that are sub-pixel precise."]
    pub fn AMotionEvent_getHistoricalX(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the historical Y coordinate of this event for the given pointer index that\n occurred between this event and the previous motion event.\n Whole numbers are pixels; the value may have a fraction for input devices\n that are sub-pixel precise."]
    pub fn AMotionEvent_getHistoricalY(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the historical pressure of this event for the given pointer index that\n occurred between this event and the previous motion event.\n The pressure generally ranges from 0 (no pressure at all) to 1 (normal pressure),\n although values higher than 1 may be generated depending on the calibration of\n the input device."]
    pub fn AMotionEvent_getHistoricalPressure(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the current scaled value of the approximate size for the given pointer index that\n occurred between this event and the previous motion event.\n This represents some approximation of the area of the screen being\n pressed; the actual value in pixels corresponding to the\n touch is normalized with the device specific range of values\n and scaled to a value between 0 and 1.  The value of size can be used to\n determine fat touch events."]
    pub fn AMotionEvent_getHistoricalSize(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the historical length of the major axis of an ellipse that describes the touch area\n at the point of contact for the given pointer index that\n occurred between this event and the previous motion event."]
    pub fn AMotionEvent_getHistoricalTouchMajor(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the historical length of the minor axis of an ellipse that describes the touch area\n at the point of contact for the given pointer index that\n occurred between this event and the previous motion event."]
    pub fn AMotionEvent_getHistoricalTouchMinor(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the historical length of the major axis of an ellipse that describes the size\n of the approaching tool for the given pointer index that\n occurred between this event and the previous motion event.\n The tool area represents the estimated size of the finger or pen that is\n touching the device independent of its actual touch area at the point of contact."]
    pub fn AMotionEvent_getHistoricalToolMajor(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the historical length of the minor axis of an ellipse that describes the size\n of the approaching tool for the given pointer index that\n occurred between this event and the previous motion event.\n The tool area represents the estimated size of the finger or pen that is\n touching the device independent of its actual touch area at the point of contact."]
    pub fn AMotionEvent_getHistoricalToolMinor(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the historical orientation of the touch area and tool area in radians clockwise from\n vertical for the given pointer index that\n occurred between this event and the previous motion event.\n An angle of 0 degrees indicates that the major axis of contact is oriented\n upwards, is perfectly circular or is of unknown orientation.  A positive angle\n indicates that the major axis of contact is oriented to the right.  A negative angle\n indicates that the major axis of contact is oriented to the left.\n The full range is from -PI/2 radians (finger pointing fully left) to PI/2 radians\n (finger pointing fully right)."]
    pub fn AMotionEvent_getHistoricalOrientation(
        motion_event: *const AInputEvent,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the historical value of the request axis for the given pointer index\n that occurred between this event and the previous motion event."]
    pub fn AMotionEvent_getHistoricalAxisValue(
        motion_event: *const AInputEvent,
        axis: i32,
        pointer_index: usize,
        history_index: usize,
    ) -> f32;
}
extern "C" {
    #[doc = " Get the action button for the motion event. Returns a valid action button when the\n event is associated with a button press or button release action. For other actions\n the return value is undefined.\n\n @see #AMOTION_EVENT_BUTTON_PRIMARY\n @see #AMOTION_EVENT_BUTTON_SECONDARY\n @see #AMOTION_EVENT_BUTTON_TERTIARY\n @see #AMOTION_EVENT_BUTTON_BACK\n @see #AMOTION_EVENT_BUTTON_FORWARD\n @see #AMOTION_EVENT_BUTTON_STYLUS_PRIMARY\n @see #AMOTION_EVENT_BUTTON_STYLUS_SECONDARY"]
    pub fn AMotionEvent_getActionButton(motion_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Returns the classification for the current gesture.\n The classification may change as more events become available for the same gesture.\n\n @see #AMOTION_EVENT_CLASSIFICATION_NONE\n @see #AMOTION_EVENT_CLASSIFICATION_AMBIGUOUS_GESTURE\n @see #AMOTION_EVENT_CLASSIFICATION_DEEP_PRESS"]
    pub fn AMotionEvent_getClassification(motion_event: *const AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Creates a native {@link AInputEvent}* object that is a copy of the specified Java\n android.view.MotionEvent. The result may be used with generic and MotionEvent-specific\n AInputEvent_* functions. The object returned by this function must be disposed using\n {@link AInputEvent_release()}.\n\n Available since API level 31."]
    pub fn AMotionEvent_fromJava(env: *mut JNIEnv, motionEvent: jobject) -> *const AInputEvent;
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AInputQueue {
    _unused: [u8; 0],
}
extern "C" {
    #[doc = " Add this input queue to a looper for processing.  See\n {@link ALooper_addFd()} for information on the ident, callback, and data params."]
    pub fn AInputQueue_attachLooper(
        queue: *mut AInputQueue,
        looper: *mut ALooper,
        ident: ::std::os::raw::c_int,
        callback: ALooper_callbackFunc,
        data: *mut ::std::os::raw::c_void,
    );
}
extern "C" {
    #[doc = " Remove the input queue from the looper it is currently attached to."]
    pub fn AInputQueue_detachLooper(queue: *mut AInputQueue);
}
extern "C" {
    #[doc = " Returns true if there are one or more events available in the\n input queue.  Returns 1 if the queue has events; 0 if\n it does not have events; and a negative value if there is an error."]
    pub fn AInputQueue_hasEvents(queue: *mut AInputQueue) -> i32;
}
extern "C" {
    #[doc = " Returns the next available event from the queue.  Returns a negative\n value if no events are available or an error has occurred."]
    pub fn AInputQueue_getEvent(queue: *mut AInputQueue, outEvent: *mut *mut AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Sends the key for standard pre-dispatching -- that is, possibly deliver\n it to the current IME to be consumed before the app.  Returns 0 if it\n was not pre-dispatched, meaning you can process it right now.  If non-zero\n is returned, you must abandon the current event processing and allow the\n event to appear again in the event queue (if it does not get consumed during\n pre-dispatching)."]
    pub fn AInputQueue_preDispatchEvent(queue: *mut AInputQueue, event: *mut AInputEvent) -> i32;
}
extern "C" {
    #[doc = " Report that dispatching has finished with the given event.\n This must be called after receiving an event with {@link AInputQueue_getEvent()}."]
    pub fn AInputQueue_finishEvent(
        queue: *mut AInputQueue,
        event: *mut AInputEvent,
        handled: ::std::os::raw::c_int,
    );
}
extern "C" {
    #[doc = " Returns the {@link AInputQueue}* object associated with the supplied Java InputQueue\n object. The returned native object holds a weak reference to the Java object,\n and is only valid as long as the Java object has not yet been disposed. You\n should ensure that there is a strong reference to the Java object and that it\n has not been disposed before using the returned object.\n\n Available since API level 33."]
    pub fn AInputQueue_fromJava(env: *mut JNIEnv, inputQueue: jobject) -> *mut AInputQueue;
}
#[doc = " Default-assumption data space, when not explicitly specified.\n\n It is safest to assume the buffer is an image with sRGB primaries and\n encoding ranges, but the consumer and/or the producer of the data may\n simply be using defaults. No automatic gamma transform should be\n expected, except for a possible display gamma transform when drawn to a\n screen."]
pub const ADataSpace_ADATASPACE_UNKNOWN: ADataSpace = 0;
#[doc = " Standard aspect\n\n Defines the chromaticity coordinates of the source primaries in terms of\n the CIE 1931 definition of x and y specified in ISO 11664-1."]
pub const ADataSpace_STANDARD_MASK: ADataSpace = 4128768;
#[doc = " Chromacity coordinates are unknown or are determined by the application.\n Implementations shall use the following suggested standards:\n\n All YCbCr formats: BT709 if size is 720p or larger (since most video\n                    content is letterboxed this corresponds to width is\n                    1280 or greater, or height is 720 or greater).\n                    BT601_625 if size is smaller than 720p or is JPEG.\n All RGB formats:   BT709.\n\n For all other formats standard is undefined, and implementations should use\n an appropriate standard for the data represented."]
pub const ADataSpace_STANDARD_UNSPECIFIED: ADataSpace = 0;
#[doc = " Primaries:       x       y\n  green           0.300   0.600\n  blue            0.150   0.060\n  red             0.640   0.330\n  white (D65)     0.3127  0.3290\n\n Use the unadjusted KR = 0.2126, KB = 0.0722 luminance interpretation\n for RGB conversion."]
pub const ADataSpace_STANDARD_BT709: ADataSpace = 65536;
#[doc = " Primaries:       x       y\n  green           0.290   0.600\n  blue            0.150   0.060\n  red             0.640   0.330\n  white (D65)     0.3127  0.3290\n\n  KR = 0.299, KB = 0.114. This adjusts the luminance interpretation\n  for RGB conversion from the one purely determined by the primaries\n  to minimize the color shift into RGB space that uses BT.709\n  primaries."]
pub const ADataSpace_STANDARD_BT601_625: ADataSpace = 131072;
#[doc = " Primaries:       x       y\n  green           0.290   0.600\n  blue            0.150   0.060\n  red             0.640   0.330\n  white (D65)     0.3127  0.3290\n\n Use the unadjusted KR = 0.222, KB = 0.071 luminance interpretation\n for RGB conversion."]
pub const ADataSpace_STANDARD_BT601_625_UNADJUSTED: ADataSpace = 196608;
#[doc = " Primaries:       x       y\n  green           0.310   0.595\n  blue            0.155   0.070\n  red             0.630   0.340\n  white (D65)     0.3127  0.3290\n\n  KR = 0.299, KB = 0.114. This adjusts the luminance interpretation\n  for RGB conversion from the one purely determined by the primaries\n  to minimize the color shift into RGB space that uses BT.709\n  primaries."]
pub const ADataSpace_STANDARD_BT601_525: ADataSpace = 262144;
#[doc = " Primaries:       x       y\n  green           0.310   0.595\n  blue            0.155   0.070\n  red             0.630   0.340\n  white (D65)     0.3127  0.3290\n\n Use the unadjusted KR = 0.212, KB = 0.087 luminance interpretation\n for RGB conversion (as in SMPTE 240M)."]
pub const ADataSpace_STANDARD_BT601_525_UNADJUSTED: ADataSpace = 327680;
#[doc = " Primaries:       x       y\n  green           0.170   0.797\n  blue            0.131   0.046\n  red             0.708   0.292\n  white (D65)     0.3127  0.3290\n\n Use the unadjusted KR = 0.2627, KB = 0.0593 luminance interpretation\n for RGB conversion."]
pub const ADataSpace_STANDARD_BT2020: ADataSpace = 393216;
#[doc = " Primaries:       x       y\n  green           0.170   0.797\n  blue            0.131   0.046\n  red             0.708   0.292\n  white (D65)     0.3127  0.3290\n\n Use the unadjusted KR = 0.2627, KB = 0.0593 luminance interpretation\n for RGB conversion using the linear domain."]
pub const ADataSpace_STANDARD_BT2020_CONSTANT_LUMINANCE: ADataSpace = 458752;
#[doc = " Primaries:       x      y\n  green           0.21   0.71\n  blue            0.14   0.08\n  red             0.67   0.33\n  white (C)       0.310  0.316\n\n Use the unadjusted KR = 0.30, KB = 0.11 luminance interpretation\n for RGB conversion."]
pub const ADataSpace_STANDARD_BT470M: ADataSpace = 524288;
#[doc = " Primaries:       x       y\n  green           0.243   0.692\n  blue            0.145   0.049\n  red             0.681   0.319\n  white (C)       0.310   0.316\n\n Use the unadjusted KR = 0.254, KB = 0.068 luminance interpretation\n for RGB conversion."]
pub const ADataSpace_STANDARD_FILM: ADataSpace = 589824;
#[doc = " SMPTE EG 432-1 and SMPTE RP 431-2. (DCI-P3)\n Primaries:       x       y\n  green           0.265   0.690\n  blue            0.150   0.060\n  red             0.680   0.320\n  white (D65)     0.3127  0.3290"]
pub const ADataSpace_STANDARD_DCI_P3: ADataSpace = 655360;
#[doc = " Adobe RGB\n Primaries:       x       y\n  green           0.210   0.710\n  blue            0.150   0.060\n  red             0.640   0.330\n  white (D65)     0.3127  0.3290"]
pub const ADataSpace_STANDARD_ADOBE_RGB: ADataSpace = 720896;
#[doc = " Transfer aspect\n\n Transfer characteristics are the opto-electronic transfer characteristic\n at the source as a function of linear optical intensity (luminance).\n\n For digital signals, E corresponds to the recorded value. Normally, the\n transfer function is applied in RGB space to each of the R, G and B\n components independently. This may result in color shift that can be\n minized by applying the transfer function in Lab space only for the L\n component. Implementation may apply the transfer function in RGB space\n for all pixel formats if desired."]
pub const ADataSpace_TRANSFER_MASK: ADataSpace = 130023424;
#[doc = " Transfer characteristics are unknown or are determined by the\n application.\n\n Implementations should use the following transfer functions:\n\n For YCbCr formats: use TRANSFER_SMPTE_170M\n For RGB formats: use TRANSFER_SRGB\n\n For all other formats transfer function is undefined, and implementations\n should use an appropriate standard for the data represented."]
pub const ADataSpace_TRANSFER_UNSPECIFIED: ADataSpace = 0;
#[doc = " Transfer characteristic curve:\n  E = L\n      L - luminance of image 0 <= L <= 1 for conventional colorimetry\n      E - corresponding electrical signal"]
pub const ADataSpace_TRANSFER_LINEAR: ADataSpace = 4194304;
#[doc = " Transfer characteristic curve:\n\n E = 1.055 * L^(1/2.4) - 0.055  for 0.0031308 <= L <= 1\n   = 12.92 * L                  for 0 <= L < 0.0031308\n     L - luminance of image 0 <= L <= 1 for conventional colorimetry\n     E - corresponding electrical signal"]
pub const ADataSpace_TRANSFER_SRGB: ADataSpace = 8388608;
#[doc = " BT.601 525, BT.601 625, BT.709, BT.2020\n\n Transfer characteristic curve:\n  E = 1.099 * L ^ 0.45 - 0.099  for 0.018 <= L <= 1\n    = 4.500 * L                 for 0 <= L < 0.018\n      L - luminance of image 0 <= L <= 1 for conventional colorimetry\n      E - corresponding electrical signal"]
pub const ADataSpace_TRANSFER_SMPTE_170M: ADataSpace = 12582912;
#[doc = " Assumed display gamma 2.2.\n\n Transfer characteristic curve:\n  E = L ^ (1/2.2)\n      L - luminance of image 0 <= L <= 1 for conventional colorimetry\n      E - corresponding electrical signal"]
pub const ADataSpace_TRANSFER_GAMMA2_2: ADataSpace = 16777216;
#[doc = "  display gamma 2.6.\n\n Transfer characteristic curve:\n  E = L ^ (1/2.6)\n      L - luminance of image 0 <= L <= 1 for conventional colorimetry\n      E - corresponding electrical signal"]
pub const ADataSpace_TRANSFER_GAMMA2_6: ADataSpace = 20971520;
#[doc = "  display gamma 2.8.\n\n Transfer characteristic curve:\n  E = L ^ (1/2.8)\n      L - luminance of image 0 <= L <= 1 for conventional colorimetry\n      E - corresponding electrical signal"]
pub const ADataSpace_TRANSFER_GAMMA2_8: ADataSpace = 25165824;
#[doc = " SMPTE ST 2084 (Dolby Perceptual Quantizer)\n\n Transfer characteristic curve:\n  E = ((c1 + c2 * L^n) / (1 + c3 * L^n)) ^ m\n  c1 = c3 - c2 + 1 = 3424 / 4096 = 0.8359375\n  c2 = 32 * 2413 / 4096 = 18.8515625\n  c3 = 32 * 2392 / 4096 = 18.6875\n  m = 128 * 2523 / 4096 = 78.84375\n  n = 0.25 * 2610 / 4096 = 0.1593017578125\n      L - luminance of image 0 <= L <= 1 for HDR colorimetry.\n          L = 1 corresponds to 10000 cd/m2\n      E - corresponding electrical signal"]
pub const ADataSpace_TRANSFER_ST2084: ADataSpace = 29360128;
#[doc = " ARIB STD-B67 Hybrid Log Gamma\n\n Transfer characteristic curve:\n  E = r * L^0.5                 for 0 <= L <= 1\n    = a * ln(L - b) + c         for 1 < L\n  a = 0.17883277\n  b = 0.28466892\n  c = 0.55991073\n  r = 0.5\n      L - luminance of image 0 <= L for HDR colorimetry. L = 1 corresponds\n          to reference white level of 100 cd/m2\n      E - corresponding electrical signal"]
pub const ADataSpace_TRANSFER_HLG: ADataSpace = 33554432;
#[doc = " Range aspect\n\n Defines the range of values corresponding to the unit range of 0-1.\n This is defined for YCbCr only, but can be expanded to RGB space."]
pub const ADataSpace_RANGE_MASK: ADataSpace = 939524096;
#[doc = " Range is unknown or are determined by the application.  Implementations\n shall use the following suggested ranges:\n\n All YCbCr formats: limited range.\n All RGB or RGBA formats (including RAW and Bayer): full range.\n All Y formats: full range\n\n For all other formats range is undefined, and implementations should use\n an appropriate range for the data represented."]
pub const ADataSpace_RANGE_UNSPECIFIED: ADataSpace = 0;
#[doc = " Full range uses all values for Y, Cb and Cr from\n 0 to 2^b-1, where b is the bit depth of the color format."]
pub const ADataSpace_RANGE_FULL: ADataSpace = 134217728;
#[doc = " Limited range uses values 16/256*2^b to 235/256*2^b for Y, and\n 1/16*2^b to 15/16*2^b for Cb, Cr, R, G and B, where b is the bit depth of\n the color format.\n\n E.g. For 8-bit-depth formats:\n Luma (Y) samples should range from 16 to 235, inclusive\n Chroma (Cb, Cr) samples should range from 16 to 240, inclusive\n\n For 10-bit-depth formats:\n Luma (Y) samples should range from 64 to 940, inclusive\n Chroma (Cb, Cr) samples should range from 64 to 960, inclusive"]
pub const ADataSpace_RANGE_LIMITED: ADataSpace = 268435456;
#[doc = " Extended range is used for scRGB. Intended for use with\n floating point pixel formats. [0.0 - 1.0] is the standard\n sRGB space. Values outside the range 0.0 - 1.0 can encode\n color outside the sRGB gamut.\n Used to blend / merge multiple dataspaces on a single display."]
pub const ADataSpace_RANGE_EXTENDED: ADataSpace = 402653184;
#[doc = " scRGB linear encoding:\n\n The red, green, and blue components are stored in extended sRGB space,\n but are linear, not gamma-encoded.\n The RGB primaries and the white point are the same as BT.709.\n\n The values are floating point.\n A pixel value of 1.0, 1.0, 1.0 corresponds to sRGB white (D65) at 80 nits.\n Values beyond the range [0.0 - 1.0] would correspond to other colors\n spaces and/or HDR content."]
pub const ADataSpace_ADATASPACE_SCRGB_LINEAR: ADataSpace = 406913024;
#[doc = " sRGB gamma encoding:\n\n The red, green and blue components are stored in sRGB space, and\n converted to linear space when read, using the SRGB transfer function\n for each of the R, G and B components. When written, the inverse\n transformation is performed.\n\n The alpha component, if present, is always stored in linear space and\n is left unmodified when read or written.\n\n Use full range and BT.709 standard."]
pub const ADataSpace_ADATASPACE_SRGB: ADataSpace = 142671872;
#[doc = " scRGB:\n\n The red, green, and blue components are stored in extended sRGB space,\n and gamma-encoded using the SRGB transfer function.\n The RGB primaries and the white point are the same as BT.709.\n\n The values are floating point.\n A pixel value of 1.0, 1.0, 1.0 corresponds to sRGB white (D65) at 80 nits.\n Values beyond the range [0.0 - 1.0] would correspond to other colors\n spaces and/or HDR content."]
pub const ADataSpace_ADATASPACE_SCRGB: ADataSpace = 411107328;
#[doc = " Display P3\n\n Use same primaries and white-point as DCI-P3\n but sRGB transfer function."]
pub const ADataSpace_ADATASPACE_DISPLAY_P3: ADataSpace = 143261696;
#[doc = " ITU-R Recommendation 2020 (BT.2020)\n\n Ultra High-definition television\n\n Use full range, SMPTE 2084 (PQ) transfer and BT2020 standard"]
pub const ADataSpace_ADATASPACE_BT2020_PQ: ADataSpace = 163971072;
#[doc = " ITU-R Recommendation 2020 (BT.2020)\n\n Ultra High-definition television\n\n Use limited range, SMPTE 2084 (PQ) transfer and BT2020 standard"]
pub const ADataSpace_ADATASPACE_BT2020_ITU_PQ: ADataSpace = 298188800;
#[doc = " Adobe RGB\n\n Use full range, gamma 2.2 transfer and Adobe RGB primaries\n Note: Application is responsible for gamma encoding the data as\n a 2.2 gamma encoding is not supported in HW."]
pub const ADataSpace_ADATASPACE_ADOBE_RGB: ADataSpace = 151715840;
#[doc = " JPEG File Interchange Format (JFIF)\n\n Same model as BT.601-625, but all values (Y, Cb, Cr) range from 0 to 255\n\n Use full range, SMPTE 170M transfer and BT.601_625 standard."]
pub const ADataSpace_ADATASPACE_JFIF: ADataSpace = 146931712;
#[doc = " ITU-R Recommendation 601 (BT.601) - 525-line\n\n Standard-definition television, 525 Lines (NTSC)\n\n Use limited range, SMPTE 170M transfer and BT.601_525 standard."]
pub const ADataSpace_ADATASPACE_BT601_625: ADataSpace = 281149440;
#[doc = " ITU-R Recommendation 709 (BT.709)\n\n High-definition television\n\n Use limited range, SMPTE 170M transfer and BT.709 standard."]
pub const ADataSpace_ADATASPACE_BT601_525: ADataSpace = 281280512;
#[doc = " ITU-R Recommendation 2020 (BT.2020)\n\n Ultra High-definition television\n\n Use full range, BT.709 transfer and BT2020 standard"]
pub const ADataSpace_ADATASPACE_BT2020: ADataSpace = 147193856;
#[doc = " ITU-R Recommendation 709 (BT.709)\n\n High-definition television\n\n Use limited range, BT.709 transfer and BT.709 standard."]
pub const ADataSpace_ADATASPACE_BT709: ADataSpace = 281083904;
#[doc = " SMPTE EG 432-1 and SMPTE RP 431-2.\n\n Digital Cinema DCI-P3\n\n Use full range, gamma 2.6 transfer and D65 DCI-P3 standard\n Note: Application is responsible for gamma encoding the data as\n a 2.6 gamma encoding is not supported in HW."]
pub const ADataSpace_ADATASPACE_DCI_P3: ADataSpace = 155844608;
#[doc = " sRGB linear encoding:\n\n The red, green, and blue components are stored in sRGB space, but\n are linear, not gamma-encoded.\n The RGB primaries and the white point are the same as BT.709.\n\n The values are encoded using the full range ([0,255] for 8-bit) for all\n components."]
pub const ADataSpace_ADATASPACE_SRGB_LINEAR: ADataSpace = 138477568;
#[doc = " Hybrid Log Gamma encoding:\n\n Use full range, hybrid log gamma transfer and BT2020 standard."]
pub const ADataSpace_ADATASPACE_BT2020_HLG: ADataSpace = 168165376;
#[doc = " ITU Hybrid Log Gamma encoding:\n\n Use limited range, hybrid log gamma transfer and BT2020 standard."]
pub const ADataSpace_ADATASPACE_BT2020_ITU_HLG: ADataSpace = 302383104;
#[doc = " Depth:\n\n This value is valid with formats HAL_PIXEL_FORMAT_Y16 and HAL_PIXEL_FORMAT_BLOB."]
pub const ADataSpace_DEPTH: ADataSpace = 4096;
#[doc = " ISO 16684-1:2011(E) Dynamic Depth:\n\n Embedded depth metadata following the dynamic depth specification."]
pub const ADataSpace_DYNAMIC_DEPTH: ADataSpace = 4098;
#[doc = " ADataSpace."]
pub type ADataSpace = ::std::os::raw::c_uint;
#[doc = " Red: 8 bits, Green: 8 bits, Blue: 8 bits, Alpha: 8 bits."]
pub const ANativeWindow_LegacyFormat_WINDOW_FORMAT_RGBA_8888: ANativeWindow_LegacyFormat = 1;
#[doc = " Red: 8 bits, Green: 8 bits, Blue: 8 bits, Unused: 8 bits."]
pub const ANativeWindow_LegacyFormat_WINDOW_FORMAT_RGBX_8888: ANativeWindow_LegacyFormat = 2;
#[doc = " Red: 5 bits, Green: 6 bits, Blue: 5 bits."]
pub const ANativeWindow_LegacyFormat_WINDOW_FORMAT_RGB_565: ANativeWindow_LegacyFormat = 4;
#[doc = " Legacy window pixel format names, kept for backwards compatibility.\n New code and APIs should use AHARDWAREBUFFER_FORMAT_*."]
pub type ANativeWindow_LegacyFormat = ::std::os::raw::c_uint;
pub const ANativeWindowTransform_ANATIVEWINDOW_TRANSFORM_IDENTITY: ANativeWindowTransform = 0;
pub const ANativeWindowTransform_ANATIVEWINDOW_TRANSFORM_MIRROR_HORIZONTAL: ANativeWindowTransform =
    1;
pub const ANativeWindowTransform_ANATIVEWINDOW_TRANSFORM_MIRROR_VERTICAL: ANativeWindowTransform =
    2;
pub const ANativeWindowTransform_ANATIVEWINDOW_TRANSFORM_ROTATE_90: ANativeWindowTransform = 4;
pub const ANativeWindowTransform_ANATIVEWINDOW_TRANSFORM_ROTATE_180: ANativeWindowTransform = 3;
pub const ANativeWindowTransform_ANATIVEWINDOW_TRANSFORM_ROTATE_270: ANativeWindowTransform = 7;
#[doc = " Transforms that can be applied to buffers as they are displayed to a window.\n\n Supported transforms are any combination of horizontal mirror, vertical\n mirror, and clockwise 90 degree rotation, in that order. Rotations of 180\n and 270 degrees are made up of those basic transforms."]
pub type ANativeWindowTransform = ::std::os::raw::c_uint;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ANativeWindow {
    _unused: [u8; 0],
}
#[doc = " Struct that represents a windows buffer.\n\n A pointer can be obtained using {@link ANativeWindow_lock()}."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ANativeWindow_Buffer {
    #[doc = " The number of pixels that are shown horizontally."]
    pub width: i32,
    #[doc = " The number of pixels that are shown vertically."]
    pub height: i32,
    #[doc = " The number of *pixels* that a line in the buffer takes in\n memory. This may be >= width."]
    pub stride: i32,
    #[doc = " The format of the buffer. One of AHardwareBuffer_Format."]
    pub format: i32,
    #[doc = " The actual bits."]
    pub bits: *mut ::std::os::raw::c_void,
    #[doc = " Do not touch."]
    pub reserved: [u32; 6usize],
}
#[test]
fn bindgen_test_layout_ANativeWindow_Buffer() {
    const UNINIT: ::std::mem::MaybeUninit<ANativeWindow_Buffer> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ANativeWindow_Buffer>(),
        48usize,
        concat!("Size of: ", stringify!(ANativeWindow_Buffer))
    );
    assert_eq!(
        ::std::mem::align_of::<ANativeWindow_Buffer>(),
        8usize,
        concat!("Alignment of ", stringify!(ANativeWindow_Buffer))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).width) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeWindow_Buffer),
            "::",
            stringify!(width)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).height) as usize - ptr as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeWindow_Buffer),
            "::",
            stringify!(height)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).stride) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeWindow_Buffer),
            "::",
            stringify!(stride)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).format) as usize - ptr as usize },
        12usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeWindow_Buffer),
            "::",
            stringify!(format)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).bits) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeWindow_Buffer),
            "::",
            stringify!(bits)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reserved) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeWindow_Buffer),
            "::",
            stringify!(reserved)
        )
    );
}
extern "C" {
    #[doc = " Acquire a reference on the given {@link ANativeWindow} object. This prevents the object\n from being deleted until the reference is removed."]
    pub fn ANativeWindow_acquire(window: *mut ANativeWindow);
}
extern "C" {
    #[doc = " Remove a reference that was previously acquired with {@link ANativeWindow_acquire()}."]
    pub fn ANativeWindow_release(window: *mut ANativeWindow);
}
extern "C" {
    #[doc = " Return the current width in pixels of the window surface.\n\n \\return negative value on error."]
    pub fn ANativeWindow_getWidth(window: *mut ANativeWindow) -> i32;
}
extern "C" {
    #[doc = " Return the current height in pixels of the window surface.\n\n \\return a negative value on error."]
    pub fn ANativeWindow_getHeight(window: *mut ANativeWindow) -> i32;
}
extern "C" {
    #[doc = " Return the current pixel format (AHARDWAREBUFFER_FORMAT_*) of the window surface.\n\n \\return a negative value on error."]
    pub fn ANativeWindow_getFormat(window: *mut ANativeWindow) -> i32;
}
extern "C" {
    #[doc = " Change the format and size of the window buffers.\n\n The width and height control the number of pixels in the buffers, not the\n dimensions of the window on screen. If these are different than the\n window's physical size, then its buffer will be scaled to match that size\n when compositing it to the screen. The width and height must be either both zero\n or both non-zero.\n\n For all of these parameters, if 0 is supplied then the window's base\n value will come back in force.\n\n \\param window pointer to an ANativeWindow object.\n \\param width width of the buffers in pixels.\n \\param height height of the buffers in pixels.\n \\param format one of the AHardwareBuffer_Format constants.\n \\return 0 for success, or a negative value on error."]
    pub fn ANativeWindow_setBuffersGeometry(
        window: *mut ANativeWindow,
        width: i32,
        height: i32,
        format: i32,
    ) -> i32;
}
extern "C" {
    #[doc = " Lock the window's next drawing surface for writing.\n inOutDirtyBounds is used as an in/out parameter, upon entering the\n function, it contains the dirty region, that is, the region the caller\n intends to redraw. When the function returns, inOutDirtyBounds is updated\n with the actual area the caller needs to redraw -- this region is often\n extended by {@link ANativeWindow_lock}.\n\n \\return 0 for success, or a negative value on error."]
    pub fn ANativeWindow_lock(
        window: *mut ANativeWindow,
        outBuffer: *mut ANativeWindow_Buffer,
        inOutDirtyBounds: *mut ARect,
    ) -> i32;
}
extern "C" {
    #[doc = " Unlock the window's drawing surface after previously locking it,\n posting the new buffer to the display.\n\n \\return 0 for success, or a negative value on error."]
    pub fn ANativeWindow_unlockAndPost(window: *mut ANativeWindow) -> i32;
}
extern "C" {
    #[doc = " Set a transform that will be applied to future buffers posted to the window.\n\n Available since API level 26.\n\n \\param window pointer to an ANativeWindow object.\n \\param transform combination of {@link ANativeWindowTransform} flags\n \\return 0 for success, or -EINVAL if \\p transform is invalid"]
    pub fn ANativeWindow_setBuffersTransform(window: *mut ANativeWindow, transform: i32) -> i32;
}
extern "C" {
    #[doc = " All buffers queued after this call will be associated with the dataSpace\n parameter specified.\n\n dataSpace specifies additional information about the buffer.\n For example, it can be used to convey the color space of the image data in\n the buffer, or it can be used to indicate that the buffers contain depth\n measurement data instead of color images. The default dataSpace is 0,\n ADATASPACE_UNKNOWN, unless it has been overridden by the producer.\n\n Available since API level 28.\n\n \\param window pointer to an ANativeWindow object.\n \\param dataSpace data space of all buffers queued after this call.\n \\return 0 for success, -EINVAL if window is invalid or the dataspace is not\n supported."]
    pub fn ANativeWindow_setBuffersDataSpace(window: *mut ANativeWindow, dataSpace: i32) -> i32;
}
extern "C" {
    #[doc = " Get the dataspace of the buffers in window.\n\n Available since API level 28.\n\n \\return the dataspace of buffers in window, ADATASPACE_UNKNOWN is returned if\n dataspace is unknown, or -EINVAL if window is invalid."]
    pub fn ANativeWindow_getBuffersDataSpace(window: *mut ANativeWindow) -> i32;
}
#[doc = " There are no inherent restrictions on the frame rate of this window. When\n the system selects a frame rate other than what the app requested, the\n app will be able to run at the system frame rate without requiring pull\n down. This value should be used when displaying game content, UIs, and\n anything that isn't video."]
pub const ANativeWindow_FrameRateCompatibility_ANATIVEWINDOW_FRAME_RATE_COMPATIBILITY_DEFAULT:
    ANativeWindow_FrameRateCompatibility = 0;
#[doc = " This window is being used to display content with an inherently fixed\n frame rate, e.g.\\ a video that has a specific frame rate. When the system\n selects a frame rate other than what the app requested, the app will need\n to do pull down or use some other technique to adapt to the system's\n frame rate. The user experience is likely to be worse (e.g. more frame\n stuttering) than it would be if the system had chosen the app's requested\n frame rate. This value should be used for video content."]
pub const ANativeWindow_FrameRateCompatibility_ANATIVEWINDOW_FRAME_RATE_COMPATIBILITY_FIXED_SOURCE : ANativeWindow_FrameRateCompatibility = 1 ;
#[doc = " Compatibility value for ANativeWindow_setFrameRate."]
pub type ANativeWindow_FrameRateCompatibility = ::std::os::raw::c_uint;
extern "C" {
    #[doc = " Same as ANativeWindow_setFrameRateWithChangeStrategy(window, frameRate, compatibility,\n ANATIVEWINDOW_CHANGE_FRAME_RATE_ONLY_IF_SEAMLESS).\n\n See ANativeWindow_setFrameRateWithChangeStrategy().\n\n Available since API level 30."]
    pub fn ANativeWindow_setFrameRate(
        window: *mut ANativeWindow,
        frameRate: f32,
        compatibility: i8,
    ) -> i32;
}
extern "C" {
    #[doc = " Provides a hint to the window that buffers should be preallocated ahead of\n time. Note that the window implementation is not guaranteed to preallocate\n any buffers, for instance if an implementation disallows allocation of new\n buffers, or if there is insufficient memory in the system to preallocate\n additional buffers\n\n Available since API level 30."]
    pub fn ANativeWindow_tryAllocateBuffers(window: *mut ANativeWindow);
}
#[doc = " Change the frame rate only if the transition is going to be seamless."]
pub const ANativeWindow_ChangeFrameRateStrategy_ANATIVEWINDOW_CHANGE_FRAME_RATE_ONLY_IF_SEAMLESS:
    ANativeWindow_ChangeFrameRateStrategy = 0;
#[doc = " Change the frame rate even if the transition is going to be non-seamless,\n i.e. with visual interruptions for the user."]
pub const ANativeWindow_ChangeFrameRateStrategy_ANATIVEWINDOW_CHANGE_FRAME_RATE_ALWAYS:
    ANativeWindow_ChangeFrameRateStrategy = 1;
#[doc = " Change frame rate strategy value for ANativeWindow_setFrameRate."]
pub type ANativeWindow_ChangeFrameRateStrategy = ::std::os::raw::c_uint;
extern "C" {
    #[doc = " Sets the intended frame rate for this window.\n\n On devices that are capable of running the display at different refresh\n rates, the system may choose a display refresh rate to better match this\n window's frame rate. Usage of this API won't introduce frame rate throttling,\n or affect other aspects of the application's frame production\n pipeline. However, because the system may change the display refresh rate,\n calls to this function may result in changes to Choreographer callback\n timings, and changes to the time interval at which the system releases\n buffers back to the application.\n\n Note that this only has an effect for windows presented on the display. If\n this ANativeWindow is consumed by something other than the system compositor,\n e.g. a media codec, this call has no effect.\n\n You can register for changes in the refresh rate using\n \\a AChoreographer_registerRefreshRateCallback.\n\n Available since API level 31.\n\n \\param window pointer to an ANativeWindow object.\n\n \\param frameRate The intended frame rate of this window, in frames per\n second. 0 is a special value that indicates the app will accept the system's\n choice for the display frame rate, which is the default behavior if this\n function isn't called. The frameRate param does <em>not</em> need to be a\n valid refresh rate for this device's display - e.g., it's fine to pass 30fps\n to a device that can only run the display at 60fps.\n\n \\param compatibility The frame rate compatibility of this window. The\n compatibility value may influence the system's choice of display refresh\n rate. See the ANATIVEWINDOW_FRAME_RATE_COMPATIBILITY_* values for more info.\n This parameter is ignored when frameRate is 0.\n\n \\param changeFrameRateStrategy Whether display refresh rate transitions caused by this\n window should be seamless.\n A seamless transition is one that doesn't have any visual interruptions, such as a black\n screen for a second or two. See the ANATIVEWINDOW_CHANGE_FRAME_RATE_* values.\n This parameter is ignored when frameRate is 0.\n\n \\return 0 for success, -EINVAL if the window, frame rate, or compatibility\n value are invalid."]
    pub fn ANativeWindow_setFrameRateWithChangeStrategy(
        window: *mut ANativeWindow,
        frameRate: f32,
        compatibility: i8,
        changeFrameRateStrategy: i8,
    ) -> i32;
}
#[doc = " This structure defines the native side of an android.app.NativeActivity.\n It is created by the framework, and handed to the application's native\n code as it is being launched."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ANativeActivity {
    #[doc = " Pointer to the callback function table of the native application.\n You can set the functions here to your own callbacks.  The callbacks\n pointer itself here should not be changed; it is allocated and managed\n for you by the framework."]
    pub callbacks: *mut ANativeActivityCallbacks,
    #[doc = " The global handle on the process's Java VM."]
    pub vm: *mut JavaVM,
    #[doc = " JNI context for the main thread of the app.  Note that this field\n can ONLY be used from the main thread of the process; that is, the\n thread that calls into the ANativeActivityCallbacks."]
    pub env: *mut JNIEnv,
    #[doc = " The NativeActivity object handle.\n\n IMPORTANT NOTE: This member is mis-named. It should really be named\n 'activity' instead of 'clazz', since it's a reference to the\n NativeActivity instance created by the system for you.\n\n We unfortunately cannot change this without breaking NDK\n source-compatibility."]
    pub clazz: jobject,
    #[doc = " Path to this application's internal data directory."]
    pub internalDataPath: *const ::std::os::raw::c_char,
    #[doc = " Path to this application's external (removable/mountable) data directory."]
    pub externalDataPath: *const ::std::os::raw::c_char,
    #[doc = " The platform's SDK version code."]
    pub sdkVersion: i32,
    #[doc = " This is the native instance of the application.  It is not used by\n the framework, but can be set by the application to its own instance\n state."]
    pub instance: *mut ::std::os::raw::c_void,
    #[doc = " Pointer to the Asset Manager instance for the application.  The application\n uses this to access binary assets bundled inside its own .apk file."]
    pub assetManager: *mut AAssetManager,
    #[doc = " Available starting with Honeycomb: path to the directory containing\n the application's OBB files (if any).  If the app doesn't have any\n OBB files, this directory may not exist."]
    pub obbPath: *const ::std::os::raw::c_char,
}
#[test]
fn bindgen_test_layout_ANativeActivity() {
    const UNINIT: ::std::mem::MaybeUninit<ANativeActivity> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ANativeActivity>(),
        80usize,
        concat!("Size of: ", stringify!(ANativeActivity))
    );
    assert_eq!(
        ::std::mem::align_of::<ANativeActivity>(),
        8usize,
        concat!("Alignment of ", stringify!(ANativeActivity))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).callbacks) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivity),
            "::",
            stringify!(callbacks)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).vm) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivity),
            "::",
            stringify!(vm)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).env) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivity),
            "::",
            stringify!(env)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).clazz) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivity),
            "::",
            stringify!(clazz)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).internalDataPath) as usize - ptr as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivity),
            "::",
            stringify!(internalDataPath)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).externalDataPath) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivity),
            "::",
            stringify!(externalDataPath)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).sdkVersion) as usize - ptr as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivity),
            "::",
            stringify!(sdkVersion)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).instance) as usize - ptr as usize },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivity),
            "::",
            stringify!(instance)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).assetManager) as usize - ptr as usize },
        64usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivity),
            "::",
            stringify!(assetManager)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).obbPath) as usize - ptr as usize },
        72usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivity),
            "::",
            stringify!(obbPath)
        )
    );
}
#[doc = " {@link ANativeActivityCallbacks}"]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ANativeActivityCallbacks {
    #[doc = " NativeActivity has started.  See Java documentation for Activity.onStart()\n for more information."]
    pub onStart: ::std::option::Option<unsafe extern "C" fn(activity: *mut ANativeActivity)>,
    #[doc = " NativeActivity has resumed.  See Java documentation for Activity.onResume()\n for more information."]
    pub onResume: ::std::option::Option<unsafe extern "C" fn(activity: *mut ANativeActivity)>,
    #[doc = " Framework is asking NativeActivity to save its current instance state.\n See Java documentation for Activity.onSaveInstanceState() for more\n information.  The returned pointer needs to be created with malloc();\n the framework will call free() on it for you.  You also must fill in\n outSize with the number of bytes in the allocation.  Note that the\n saved state will be persisted, so it can not contain any active\n entities (pointers to memory, file descriptors, etc)."]
    pub onSaveInstanceState: ::std::option::Option<
        unsafe extern "C" fn(
            activity: *mut ANativeActivity,
            outSize: *mut usize,
        ) -> *mut ::std::os::raw::c_void,
    >,
    #[doc = " NativeActivity has paused.  See Java documentation for Activity.onPause()\n for more information."]
    pub onPause: ::std::option::Option<unsafe extern "C" fn(activity: *mut ANativeActivity)>,
    #[doc = " NativeActivity has stopped.  See Java documentation for Activity.onStop()\n for more information."]
    pub onStop: ::std::option::Option<unsafe extern "C" fn(activity: *mut ANativeActivity)>,
    #[doc = " NativeActivity is being destroyed.  See Java documentation for Activity.onDestroy()\n for more information."]
    pub onDestroy: ::std::option::Option<unsafe extern "C" fn(activity: *mut ANativeActivity)>,
    #[doc = " Focus has changed in this NativeActivity's window.  This is often used,\n for example, to pause a game when it loses input focus."]
    pub onWindowFocusChanged: ::std::option::Option<
        unsafe extern "C" fn(activity: *mut ANativeActivity, hasFocus: ::std::os::raw::c_int),
    >,
    #[doc = " The drawing window for this native activity has been created.  You\n can use the given native window object to start drawing."]
    pub onNativeWindowCreated: ::std::option::Option<
        unsafe extern "C" fn(activity: *mut ANativeActivity, window: *mut ANativeWindow),
    >,
    #[doc = " The drawing window for this native activity has been resized.  You should\n retrieve the new size from the window and ensure that your rendering in\n it now matches."]
    pub onNativeWindowResized: ::std::option::Option<
        unsafe extern "C" fn(activity: *mut ANativeActivity, window: *mut ANativeWindow),
    >,
    #[doc = " The drawing window for this native activity needs to be redrawn.  To avoid\n transient artifacts during screen changes (such resizing after rotation),\n applications should not return from this function until they have finished\n drawing their window in its current state."]
    pub onNativeWindowRedrawNeeded: ::std::option::Option<
        unsafe extern "C" fn(activity: *mut ANativeActivity, window: *mut ANativeWindow),
    >,
    #[doc = " The drawing window for this native activity is going to be destroyed.\n You MUST ensure that you do not touch the window object after returning\n from this function: in the common case of drawing to the window from\n another thread, that means the implementation of this callback must\n properly synchronize with the other thread to stop its drawing before\n returning from here."]
    pub onNativeWindowDestroyed: ::std::option::Option<
        unsafe extern "C" fn(activity: *mut ANativeActivity, window: *mut ANativeWindow),
    >,
    #[doc = " The input queue for this native activity's window has been created.\n You can use the given input queue to start retrieving input events."]
    pub onInputQueueCreated: ::std::option::Option<
        unsafe extern "C" fn(activity: *mut ANativeActivity, queue: *mut AInputQueue),
    >,
    #[doc = " The input queue for this native activity's window is being destroyed.\n You should no longer try to reference this object upon returning from this\n function."]
    pub onInputQueueDestroyed: ::std::option::Option<
        unsafe extern "C" fn(activity: *mut ANativeActivity, queue: *mut AInputQueue),
    >,
    #[doc = " The rectangle in the window in which content should be placed has changed."]
    pub onContentRectChanged: ::std::option::Option<
        unsafe extern "C" fn(activity: *mut ANativeActivity, rect: *const ARect),
    >,
    #[doc = " The current device AConfiguration has changed.  The new configuration can\n be retrieved from assetManager."]
    pub onConfigurationChanged:
        ::std::option::Option<unsafe extern "C" fn(activity: *mut ANativeActivity)>,
    #[doc = " The system is running low on memory.  Use this callback to release\n resources you do not need, to help the system avoid killing more\n important processes."]
    pub onLowMemory: ::std::option::Option<unsafe extern "C" fn(activity: *mut ANativeActivity)>,
}
#[test]
fn bindgen_test_layout_ANativeActivityCallbacks() {
    const UNINIT: ::std::mem::MaybeUninit<ANativeActivityCallbacks> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ANativeActivityCallbacks>(),
        128usize,
        concat!("Size of: ", stringify!(ANativeActivityCallbacks))
    );
    assert_eq!(
        ::std::mem::align_of::<ANativeActivityCallbacks>(),
        8usize,
        concat!("Alignment of ", stringify!(ANativeActivityCallbacks))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onStart) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onStart)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onResume) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onResume)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onSaveInstanceState) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onSaveInstanceState)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onPause) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onPause)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onStop) as usize - ptr as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onStop)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onDestroy) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onDestroy)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onWindowFocusChanged) as usize - ptr as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onWindowFocusChanged)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onNativeWindowCreated) as usize - ptr as usize },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onNativeWindowCreated)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onNativeWindowResized) as usize - ptr as usize },
        64usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onNativeWindowResized)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onNativeWindowRedrawNeeded) as usize - ptr as usize },
        72usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onNativeWindowRedrawNeeded)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onNativeWindowDestroyed) as usize - ptr as usize },
        80usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onNativeWindowDestroyed)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onInputQueueCreated) as usize - ptr as usize },
        88usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onInputQueueCreated)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onInputQueueDestroyed) as usize - ptr as usize },
        96usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onInputQueueDestroyed)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onContentRectChanged) as usize - ptr as usize },
        104usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onContentRectChanged)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onConfigurationChanged) as usize - ptr as usize },
        112usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onConfigurationChanged)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onLowMemory) as usize - ptr as usize },
        120usize,
        concat!(
            "Offset of field: ",
            stringify!(ANativeActivityCallbacks),
            "::",
            stringify!(onLowMemory)
        )
    );
}
#[doc = " This is the function that must be in the native code to instantiate the\n application's native activity.  It is called with the activity instance (see\n above); if the code is being instantiated from a previously saved instance,\n the savedState will be non-NULL and point to the saved data.  You must make\n any copy of this data you need -- it will be released after you return from\n this function."]
pub type ANativeActivity_createFunc = ::std::option::Option<
    unsafe extern "C" fn(
        activity: *mut ANativeActivity,
        savedState: *mut ::std::os::raw::c_void,
        savedStateSize: usize,
    ),
>;
extern "C" {
    #[doc = " The name of the function that NativeInstance looks for when launching its\n native code.  This is the default function that is used, you can specify\n \"android.app.func_name\" string meta-data in your manifest to use a different\n function."]
    pub fn ANativeActivity_onCreate(
        activity: *mut ANativeActivity,
        savedState: *mut ::std::os::raw::c_void,
        savedStateSize: usize,
    );
}
extern "C" {
    #[doc = " Finish the given activity.  Its finish() method will be called, causing it\n to be stopped and destroyed.  Note that this method can be called from\n *any* thread; it will send a message to the main thread of the process\n where the Java finish call will take place."]
    pub fn ANativeActivity_finish(activity: *mut ANativeActivity);
}
extern "C" {
    #[doc = " Change the window format of the given activity.  Calls getWindow().setFormat()\n of the given activity.  Note that this method can be called from\n *any* thread; it will send a message to the main thread of the process\n where the Java finish call will take place."]
    pub fn ANativeActivity_setWindowFormat(activity: *mut ANativeActivity, format: i32);
}
extern "C" {
    #[doc = " Change the window flags of the given activity.  Calls getWindow().setFlags()\n of the given activity.  Note that this method can be called from\n *any* thread; it will send a message to the main thread of the process\n where the Java finish call will take place.  See window.h for flag constants."]
    pub fn ANativeActivity_setWindowFlags(
        activity: *mut ANativeActivity,
        addFlags: u32,
        removeFlags: u32,
    );
}
#[doc = " Implicit request to show the input window, not as the result\n of a direct request by the user."]
pub const ANATIVEACTIVITY_SHOW_SOFT_INPUT_IMPLICIT: _bindgen_ty_21 = 1;
#[doc = " The user has forced the input method open (such as by\n long-pressing menu) so it should not be closed until they\n explicitly do so."]
pub const ANATIVEACTIVITY_SHOW_SOFT_INPUT_FORCED: _bindgen_ty_21 = 2;
#[doc = " Flags for ANativeActivity_showSoftInput; see the Java InputMethodManager\n API for documentation."]
pub type _bindgen_ty_21 = ::std::os::raw::c_uint;
extern "C" {
    #[doc = " Show the IME while in the given activity.  Calls InputMethodManager.showSoftInput()\n for the given activity.  Note that this method can be called from\n *any* thread; it will send a message to the main thread of the process\n where the Java finish call will take place."]
    pub fn ANativeActivity_showSoftInput(activity: *mut ANativeActivity, flags: u32);
}
#[doc = " The soft input window should only be hidden if it was not\n explicitly shown by the user."]
pub const ANATIVEACTIVITY_HIDE_SOFT_INPUT_IMPLICIT_ONLY: _bindgen_ty_22 = 1;
#[doc = " The soft input window should normally be hidden, unless it was\n originally shown with {@link ANATIVEACTIVITY_SHOW_SOFT_INPUT_FORCED}."]
pub const ANATIVEACTIVITY_HIDE_SOFT_INPUT_NOT_ALWAYS: _bindgen_ty_22 = 2;
#[doc = " Flags for ANativeActivity_hideSoftInput; see the Java InputMethodManager\n API for documentation."]
pub type _bindgen_ty_22 = ::std::os::raw::c_uint;
extern "C" {
    #[doc = " Hide the IME while in the given activity.  Calls InputMethodManager.hideSoftInput()\n for the given activity.  Note that this method can be called from\n *any* thread; it will send a message to the main thread of the process\n where the Java finish call will take place."]
    pub fn ANativeActivity_hideSoftInput(activity: *mut ANativeActivity, flags: u32);
}
extern "C" {
    #[doc = " Return the ANativeWindow associated with a Java Surface object,\n for interacting with it through native code.  This acquires a reference\n on the ANativeWindow that is returned; be sure to use ANativeWindow_release()\n when done with it so that it doesn't leak."]
    pub fn ANativeWindow_fromSurface(env: *mut JNIEnv, surface: jobject) -> *mut ANativeWindow;
}
extern "C" {
    #[doc = " Return a Java Surface object derived from the ANativeWindow, for interacting\n with it through Java code. The returned Java object acquires a reference on\n the ANativeWindow; maintains it through general Java object's life cycle;\n and will automatically release the reference when the Java object gets garbage\n collected.\n\n Available since API level 26."]
    pub fn ANativeWindow_toSurface(env: *mut JNIEnv, window: *mut ANativeWindow) -> jobject;
}
#[doc = " Camera operation has succeeded."]
pub const camera_status_t_ACAMERA_OK: camera_status_t = 0;
#[doc = " Camera operation has succeeded."]
pub const camera_status_t_ACAMERA_ERROR_BASE: camera_status_t = -10000;
#[doc = " Camera operation has failed due to an unspecified cause."]
pub const camera_status_t_ACAMERA_ERROR_UNKNOWN: camera_status_t = -10000;
#[doc = " Camera operation has failed due to an invalid parameter being passed to the method."]
pub const camera_status_t_ACAMERA_ERROR_INVALID_PARAMETER: camera_status_t = -10001;
#[doc = " Camera operation has failed because the camera device has been closed, possibly because a\n higher-priority client has taken ownership of the camera device."]
pub const camera_status_t_ACAMERA_ERROR_CAMERA_DISCONNECTED: camera_status_t = -10002;
#[doc = " Camera operation has failed due to insufficient memory."]
pub const camera_status_t_ACAMERA_ERROR_NOT_ENOUGH_MEMORY: camera_status_t = -10003;
#[doc = " Camera operation has failed due to the requested metadata tag cannot be found in input\n {@link ACameraMetadata} or {@link ACaptureRequest}."]
pub const camera_status_t_ACAMERA_ERROR_METADATA_NOT_FOUND: camera_status_t = -10004;
#[doc = " Camera operation has failed and the camera device has encountered a fatal error and needs to\n be re-opened before it can be used again."]
pub const camera_status_t_ACAMERA_ERROR_CAMERA_DEVICE: camera_status_t = -10005;
#[doc = " Camera operation has failed and the camera service has encountered a fatal error.\n\n <p>The Android device may need to be shut down and restarted to restore\n camera function, or there may be a persistent hardware problem.</p>\n\n <p>An attempt at recovery may be possible by closing the\n ACameraDevice and the ACameraManager, and trying to acquire all resources\n again from scratch.</p>"]
pub const camera_status_t_ACAMERA_ERROR_CAMERA_SERVICE: camera_status_t = -10006;
#[doc = " The {@link ACameraCaptureSession} has been closed and cannnot perform any operation other\n than {@link ACameraCaptureSession_close}."]
pub const camera_status_t_ACAMERA_ERROR_SESSION_CLOSED: camera_status_t = -10007;
#[doc = " Camera operation has failed due to an invalid internal operation. Usually this is due to a\n low-level problem that may resolve itself on retry"]
pub const camera_status_t_ACAMERA_ERROR_INVALID_OPERATION: camera_status_t = -10008;
#[doc = " Camera device does not support the stream configuration provided by application in\n {@link ACameraDevice_createCaptureSession} or {@link\n ACameraDevice_isSessionConfigurationSupported}."]
pub const camera_status_t_ACAMERA_ERROR_STREAM_CONFIGURE_FAIL: camera_status_t = -10009;
#[doc = " Camera device is being used by another higher priority camera API client."]
pub const camera_status_t_ACAMERA_ERROR_CAMERA_IN_USE: camera_status_t = -10010;
#[doc = " The system-wide limit for number of open cameras or camera resources has been reached, and\n more camera devices cannot be opened until previous instances are closed."]
pub const camera_status_t_ACAMERA_ERROR_MAX_CAMERA_IN_USE: camera_status_t = -10011;
#[doc = " The camera is disabled due to a device policy, and cannot be opened."]
pub const camera_status_t_ACAMERA_ERROR_CAMERA_DISABLED: camera_status_t = -10012;
#[doc = " The application does not have permission to open camera."]
pub const camera_status_t_ACAMERA_ERROR_PERMISSION_DENIED: camera_status_t = -10013;
#[doc = " The operation is not supported by the camera device."]
pub const camera_status_t_ACAMERA_ERROR_UNSUPPORTED_OPERATION: camera_status_t = -10014;
#[doc = " Camera status enum types."]
pub type camera_status_t = ::std::os::raw::c_int;
pub const acamera_metadata_section_ACAMERA_COLOR_CORRECTION: acamera_metadata_section = 0;
pub const acamera_metadata_section_ACAMERA_CONTROL: acamera_metadata_section = 1;
pub const acamera_metadata_section_ACAMERA_DEMOSAIC: acamera_metadata_section = 2;
pub const acamera_metadata_section_ACAMERA_EDGE: acamera_metadata_section = 3;
pub const acamera_metadata_section_ACAMERA_FLASH: acamera_metadata_section = 4;
pub const acamera_metadata_section_ACAMERA_FLASH_INFO: acamera_metadata_section = 5;
pub const acamera_metadata_section_ACAMERA_HOT_PIXEL: acamera_metadata_section = 6;
pub const acamera_metadata_section_ACAMERA_JPEG: acamera_metadata_section = 7;
pub const acamera_metadata_section_ACAMERA_LENS: acamera_metadata_section = 8;
pub const acamera_metadata_section_ACAMERA_LENS_INFO: acamera_metadata_section = 9;
pub const acamera_metadata_section_ACAMERA_NOISE_REDUCTION: acamera_metadata_section = 10;
pub const acamera_metadata_section_ACAMERA_QUIRKS: acamera_metadata_section = 11;
pub const acamera_metadata_section_ACAMERA_REQUEST: acamera_metadata_section = 12;
pub const acamera_metadata_section_ACAMERA_SCALER: acamera_metadata_section = 13;
pub const acamera_metadata_section_ACAMERA_SENSOR: acamera_metadata_section = 14;
pub const acamera_metadata_section_ACAMERA_SENSOR_INFO: acamera_metadata_section = 15;
pub const acamera_metadata_section_ACAMERA_SHADING: acamera_metadata_section = 16;
pub const acamera_metadata_section_ACAMERA_STATISTICS: acamera_metadata_section = 17;
pub const acamera_metadata_section_ACAMERA_STATISTICS_INFO: acamera_metadata_section = 18;
pub const acamera_metadata_section_ACAMERA_TONEMAP: acamera_metadata_section = 19;
pub const acamera_metadata_section_ACAMERA_LED: acamera_metadata_section = 20;
pub const acamera_metadata_section_ACAMERA_INFO: acamera_metadata_section = 21;
pub const acamera_metadata_section_ACAMERA_BLACK_LEVEL: acamera_metadata_section = 22;
pub const acamera_metadata_section_ACAMERA_SYNC: acamera_metadata_section = 23;
pub const acamera_metadata_section_ACAMERA_REPROCESS: acamera_metadata_section = 24;
pub const acamera_metadata_section_ACAMERA_DEPTH: acamera_metadata_section = 25;
pub const acamera_metadata_section_ACAMERA_LOGICAL_MULTI_CAMERA: acamera_metadata_section = 26;
pub const acamera_metadata_section_ACAMERA_DISTORTION_CORRECTION: acamera_metadata_section = 27;
pub const acamera_metadata_section_ACAMERA_HEIC: acamera_metadata_section = 28;
pub const acamera_metadata_section_ACAMERA_HEIC_INFO: acamera_metadata_section = 29;
pub const acamera_metadata_section_ACAMERA_AUTOMOTIVE: acamera_metadata_section = 30;
pub const acamera_metadata_section_ACAMERA_AUTOMOTIVE_LENS: acamera_metadata_section = 31;
pub const acamera_metadata_section_ACAMERA_SECTION_COUNT: acamera_metadata_section = 32;
pub const acamera_metadata_section_ACAMERA_VENDOR: acamera_metadata_section = 32768;
pub type acamera_metadata_section = ::std::os::raw::c_uint;
pub use self::acamera_metadata_section as acamera_metadata_section_t;
pub const acamera_metadata_section_start_ACAMERA_COLOR_CORRECTION_START:
    acamera_metadata_section_start = 0;
pub const acamera_metadata_section_start_ACAMERA_CONTROL_START: acamera_metadata_section_start =
    65536;
pub const acamera_metadata_section_start_ACAMERA_DEMOSAIC_START: acamera_metadata_section_start =
    131072;
pub const acamera_metadata_section_start_ACAMERA_EDGE_START: acamera_metadata_section_start =
    196608;
pub const acamera_metadata_section_start_ACAMERA_FLASH_START: acamera_metadata_section_start =
    262144;
pub const acamera_metadata_section_start_ACAMERA_FLASH_INFO_START: acamera_metadata_section_start =
    327680;
pub const acamera_metadata_section_start_ACAMERA_HOT_PIXEL_START: acamera_metadata_section_start =
    393216;
pub const acamera_metadata_section_start_ACAMERA_JPEG_START: acamera_metadata_section_start =
    458752;
pub const acamera_metadata_section_start_ACAMERA_LENS_START: acamera_metadata_section_start =
    524288;
pub const acamera_metadata_section_start_ACAMERA_LENS_INFO_START: acamera_metadata_section_start =
    589824;
pub const acamera_metadata_section_start_ACAMERA_NOISE_REDUCTION_START:
    acamera_metadata_section_start = 655360;
pub const acamera_metadata_section_start_ACAMERA_QUIRKS_START: acamera_metadata_section_start =
    720896;
pub const acamera_metadata_section_start_ACAMERA_REQUEST_START: acamera_metadata_section_start =
    786432;
pub const acamera_metadata_section_start_ACAMERA_SCALER_START: acamera_metadata_section_start =
    851968;
pub const acamera_metadata_section_start_ACAMERA_SENSOR_START: acamera_metadata_section_start =
    917504;
pub const acamera_metadata_section_start_ACAMERA_SENSOR_INFO_START: acamera_metadata_section_start =
    983040;
pub const acamera_metadata_section_start_ACAMERA_SHADING_START: acamera_metadata_section_start =
    1048576;
pub const acamera_metadata_section_start_ACAMERA_STATISTICS_START: acamera_metadata_section_start =
    1114112;
pub const acamera_metadata_section_start_ACAMERA_STATISTICS_INFO_START:
    acamera_metadata_section_start = 1179648;
pub const acamera_metadata_section_start_ACAMERA_TONEMAP_START: acamera_metadata_section_start =
    1245184;
pub const acamera_metadata_section_start_ACAMERA_LED_START: acamera_metadata_section_start =
    1310720;
pub const acamera_metadata_section_start_ACAMERA_INFO_START: acamera_metadata_section_start =
    1376256;
pub const acamera_metadata_section_start_ACAMERA_BLACK_LEVEL_START: acamera_metadata_section_start =
    1441792;
pub const acamera_metadata_section_start_ACAMERA_SYNC_START: acamera_metadata_section_start =
    1507328;
pub const acamera_metadata_section_start_ACAMERA_REPROCESS_START: acamera_metadata_section_start =
    1572864;
pub const acamera_metadata_section_start_ACAMERA_DEPTH_START: acamera_metadata_section_start =
    1638400;
pub const acamera_metadata_section_start_ACAMERA_LOGICAL_MULTI_CAMERA_START:
    acamera_metadata_section_start = 1703936;
pub const acamera_metadata_section_start_ACAMERA_DISTORTION_CORRECTION_START:
    acamera_metadata_section_start = 1769472;
pub const acamera_metadata_section_start_ACAMERA_HEIC_START: acamera_metadata_section_start =
    1835008;
pub const acamera_metadata_section_start_ACAMERA_HEIC_INFO_START: acamera_metadata_section_start =
    1900544;
pub const acamera_metadata_section_start_ACAMERA_AUTOMOTIVE_START: acamera_metadata_section_start =
    1966080;
pub const acamera_metadata_section_start_ACAMERA_AUTOMOTIVE_LENS_START:
    acamera_metadata_section_start = 2031616;
pub const acamera_metadata_section_start_ACAMERA_VENDOR_START: acamera_metadata_section_start =
    -2147483648;
#[doc = " Hierarchy positions in enum space."]
pub type acamera_metadata_section_start = ::std::os::raw::c_int;
#[doc = " Hierarchy positions in enum space."]
pub use self::acamera_metadata_section_start as acamera_metadata_section_start_t;
#[doc = " <p>The mode control selects how the image data is converted from the\n sensor's native color into linear sRGB color.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_color_correction_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>When auto-white balance (AWB) is enabled with ACAMERA_CONTROL_AWB_MODE, this\n control is overridden by the AWB routine. When AWB is disabled, the\n application controls how the color mapping is performed.</p>\n <p>We define the expected processing pipeline below. For consistency\n across devices, this is always the case with TRANSFORM_MATRIX.</p>\n <p>When either FAST or HIGH_QUALITY is used, the camera device may\n do additional processing but ACAMERA_COLOR_CORRECTION_GAINS and\n ACAMERA_COLOR_CORRECTION_TRANSFORM will still be provided by the\n camera device (in the results) and be roughly correct.</p>\n <p>Switching to TRANSFORM_MATRIX and using the data provided from\n FAST or HIGH_QUALITY will yield a picture with the same white point\n as what was produced by the camera device in the earlier frame.</p>\n <p>The expected processing pipeline is as follows:</p>\n <p><img alt=\"White balance processing pipeline\" src=\"../images/camera2/metadata/android.colorCorrection.mode/processing_pipeline.png\" /></p>\n <p>The white balance is encoded by two values, a 4-channel white-balance\n gain vector (applied in the Bayer domain), and a 3x3 color transform\n matrix (applied after demosaic).</p>\n <p>The 4-channel white-balance gains are defined as:</p>\n <pre><code>ACAMERA_COLOR_CORRECTION_GAINS = [ R G_even G_odd B ]\n </code></pre>\n <p>where <code>G_even</code> is the gain for green pixels on even rows of the\n output, and <code>G_odd</code> is the gain for green pixels on the odd rows.\n These may be identical for a given camera device implementation; if\n the camera device does not support a separate gain for even/odd green\n channels, it will use the <code>G_even</code> value, and write <code>G_odd</code> equal to\n <code>G_even</code> in the output result metadata.</p>\n <p>The matrices for color transforms are defined as a 9-entry vector:</p>\n <pre><code>ACAMERA_COLOR_CORRECTION_TRANSFORM = [ I0 I1 I2 I3 I4 I5 I6 I7 I8 ]\n </code></pre>\n <p>which define a transform from input sensor colors, <code>P_in = [ r g b ]</code>,\n to output linear sRGB, <code>P_out = [ r' g' b' ]</code>,</p>\n <p>with colors as follows:</p>\n <pre><code>r' = I0r + I1g + I2b\n g' = I3r + I4g + I5b\n b' = I6r + I7g + I8b\n </code></pre>\n <p>Both the input and output value ranges must match. Overflow/underflow\n values are clipped to fit within the range.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM\n @see ACAMERA_CONTROL_AWB_MODE"]
pub const acamera_metadata_tag_ACAMERA_COLOR_CORRECTION_MODE: acamera_metadata_tag = 0;
#[doc = " <p>A color transform matrix to use to transform\n from sensor RGB color space to output linear sRGB color space.</p>\n\n <p>Type: rational[3*3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This matrix is either set by the camera device when the request\n ACAMERA_COLOR_CORRECTION_MODE is not TRANSFORM_MATRIX, or\n directly by the application in the request when the\n ACAMERA_COLOR_CORRECTION_MODE is TRANSFORM_MATRIX.</p>\n <p>In the latter case, the camera device may round the matrix to account\n for precision issues; the final rounded matrix should be reported back\n in this matrix result metadata. The transform should keep the magnitude\n of the output color values within <code>[0, 1.0]</code> (assuming input color\n values is within the normalized range <code>[0, 1.0]</code>), or clipping may occur.</p>\n <p>The valid range of each matrix element varies on different devices, but\n values within [-1.5, 3.0] are guaranteed not to be clipped.</p>\n\n @see ACAMERA_COLOR_CORRECTION_MODE"]
pub const acamera_metadata_tag_ACAMERA_COLOR_CORRECTION_TRANSFORM: acamera_metadata_tag = 1;
#[doc = " <p>Gains applying to Bayer raw color channels for\n white-balance.</p>\n\n <p>Type: float[4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>These per-channel gains are either set by the camera device\n when the request ACAMERA_COLOR_CORRECTION_MODE is not\n TRANSFORM_MATRIX, or directly by the application in the\n request when the ACAMERA_COLOR_CORRECTION_MODE is\n TRANSFORM_MATRIX.</p>\n <p>The gains in the result metadata are the gains actually\n applied by the camera device to the current frame.</p>\n <p>The valid range of gains varies on different devices, but gains\n between [1.0, 3.0] are guaranteed not to be clipped. Even if a given\n device allows gains below 1.0, this is usually not recommended because\n this can create color artifacts.</p>\n\n @see ACAMERA_COLOR_CORRECTION_MODE"]
pub const acamera_metadata_tag_ACAMERA_COLOR_CORRECTION_GAINS: acamera_metadata_tag = 2;
#[doc = " <p>Mode of operation for the chromatic aberration correction algorithm.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_color_correction_aberration_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Chromatic (color) aberration is caused by the fact that different wavelengths of light\n can not focus on the same point after exiting from the lens. This metadata defines\n the high level control of chromatic aberration correction algorithm, which aims to\n minimize the chromatic artifacts that may occur along the object boundaries in an\n image.</p>\n <p>FAST/HIGH_QUALITY both mean that camera device determined aberration\n correction will be applied. HIGH_QUALITY mode indicates that the camera device will\n use the highest-quality aberration correction algorithms, even if it slows down\n capture rate. FAST means the camera device will not slow down capture rate when\n applying aberration correction.</p>\n <p>LEGACY devices will always be in FAST mode.</p>"]
pub const acamera_metadata_tag_ACAMERA_COLOR_CORRECTION_ABERRATION_MODE: acamera_metadata_tag = 3;
#[doc = " <p>List of aberration correction modes for ACAMERA_COLOR_CORRECTION_ABERRATION_MODE that are\n supported by this camera device.</p>\n\n @see ACAMERA_COLOR_CORRECTION_ABERRATION_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This key lists the valid modes for ACAMERA_COLOR_CORRECTION_ABERRATION_MODE.  If no\n aberration correction modes are available for a device, this list will solely include\n OFF mode. All camera devices will support either OFF or FAST mode.</p>\n <p>Camera devices that support the MANUAL_POST_PROCESSING capability will always list\n OFF mode. This includes all FULL level devices.</p>\n <p>LEGACY devices will always only support FAST mode.</p>\n\n @see ACAMERA_COLOR_CORRECTION_ABERRATION_MODE"]
pub const acamera_metadata_tag_ACAMERA_COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES:
    acamera_metadata_tag = 4;
#[doc = " <p>List of aberration correction modes for ACAMERA_COLOR_CORRECTION_ABERRATION_MODE that are\n supported by this camera device.</p>\n\n @see ACAMERA_COLOR_CORRECTION_ABERRATION_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This key lists the valid modes for ACAMERA_COLOR_CORRECTION_ABERRATION_MODE.  If no\n aberration correction modes are available for a device, this list will solely include\n OFF mode. All camera devices will support either OFF or FAST mode.</p>\n <p>Camera devices that support the MANUAL_POST_PROCESSING capability will always list\n OFF mode. This includes all FULL level devices.</p>\n <p>LEGACY devices will always only support FAST mode.</p>\n\n @see ACAMERA_COLOR_CORRECTION_ABERRATION_MODE"]
pub const acamera_metadata_tag_ACAMERA_COLOR_CORRECTION_END: acamera_metadata_tag = 5;
#[doc = " <p>The desired setting for the camera device's auto-exposure\n algorithm's antibanding compensation.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_ae_antibanding_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Some kinds of lighting fixtures, such as some fluorescent\n lights, flicker at the rate of the power supply frequency\n (60Hz or 50Hz, depending on country). While this is\n typically not noticeable to a person, it can be visible to\n a camera device. If a camera sets its exposure time to the\n wrong value, the flicker may become visible in the\n viewfinder as flicker or in a final captured image, as a\n set of variable-brightness bands across the image.</p>\n <p>Therefore, the auto-exposure routines of camera devices\n include antibanding routines that ensure that the chosen\n exposure value will not cause such banding. The choice of\n exposure time depends on the rate of flicker, which the\n camera device can detect automatically, or the expected\n rate can be selected by the application using this\n control.</p>\n <p>A given camera device may not support all of the possible\n options for the antibanding mode. The\n ACAMERA_CONTROL_AE_AVAILABLE_ANTIBANDING_MODES key contains\n the available modes for a given camera device.</p>\n <p>AUTO mode is the default if it is available on given\n camera device. When AUTO mode is not available, the\n default will be either 50HZ or 60HZ, and both 50HZ\n and 60HZ will be available.</p>\n <p>If manual exposure control is enabled (by setting\n ACAMERA_CONTROL_AE_MODE or ACAMERA_CONTROL_MODE to OFF),\n then this setting has no effect, and the application must\n ensure it selects exposure times that do not cause banding\n issues. The ACAMERA_STATISTICS_SCENE_FLICKER key can assist\n the application in this.</p>\n\n @see ACAMERA_CONTROL_AE_AVAILABLE_ANTIBANDING_MODES\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_MODE\n @see ACAMERA_STATISTICS_SCENE_FLICKER"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_ANTIBANDING_MODE: acamera_metadata_tag = 65536;
#[doc = " <p>Adjustment to auto-exposure (AE) target image\n brightness.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>The adjustment is measured as a count of steps, with the\n step size defined by ACAMERA_CONTROL_AE_COMPENSATION_STEP and the\n allowed range by ACAMERA_CONTROL_AE_COMPENSATION_RANGE.</p>\n <p>For example, if the exposure value (EV) step is 0.333, '6'\n will mean an exposure compensation of +2 EV; -3 will mean an\n exposure compensation of -1 EV. One EV represents a doubling\n of image brightness. Note that this control will only be\n effective if ACAMERA_CONTROL_AE_MODE <code>!=</code> OFF. This control\n will take effect even when ACAMERA_CONTROL_AE_LOCK <code>== true</code>.</p>\n <p>In the event of exposure compensation value being changed, camera device\n may take several frames to reach the newly requested exposure target.\n During that time, ACAMERA_CONTROL_AE_STATE field will be in the SEARCHING\n state. Once the new exposure target is reached, ACAMERA_CONTROL_AE_STATE will\n change from SEARCHING to either CONVERGED, LOCKED (if AE lock is enabled), or\n FLASH_REQUIRED (if the scene is too dark for still capture).</p>\n\n @see ACAMERA_CONTROL_AE_COMPENSATION_RANGE\n @see ACAMERA_CONTROL_AE_COMPENSATION_STEP\n @see ACAMERA_CONTROL_AE_LOCK\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_AE_STATE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_EXPOSURE_COMPENSATION: acamera_metadata_tag =
    65537;
#[doc = " <p>Whether auto-exposure (AE) is currently locked to its latest\n calculated values.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_ae_lock_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>When set to <code>true</code> (ON), the AE algorithm is locked to its latest parameters,\n and will not change exposure settings until the lock is set to <code>false</code> (OFF).</p>\n <p>Note that even when AE is locked, the flash may be fired if\n the ACAMERA_CONTROL_AE_MODE is ON_AUTO_FLASH /\n ON_ALWAYS_FLASH / ON_AUTO_FLASH_REDEYE.</p>\n <p>When ACAMERA_CONTROL_AE_EXPOSURE_COMPENSATION is changed, even if the AE lock\n is ON, the camera device will still adjust its exposure value.</p>\n <p>If AE precapture is triggered (see ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER)\n when AE is already locked, the camera device will not change the exposure time\n (ACAMERA_SENSOR_EXPOSURE_TIME) and sensitivity (ACAMERA_SENSOR_SENSITIVITY)\n parameters. The flash may be fired if the ACAMERA_CONTROL_AE_MODE\n is ON_AUTO_FLASH/ON_AUTO_FLASH_REDEYE and the scene is too dark. If the\n ACAMERA_CONTROL_AE_MODE is ON_ALWAYS_FLASH, the scene may become overexposed.\n Similarly, AE precapture trigger CANCEL has no effect when AE is already locked.</p>\n <p>When an AE precapture sequence is triggered, AE unlock will not be able to unlock\n the AE if AE is locked by the camera device internally during precapture metering\n sequence In other words, submitting requests with AE unlock has no effect for an\n ongoing precapture metering sequence. Otherwise, the precapture metering sequence\n will never succeed in a sequence of preview requests where AE lock is always set\n to <code>false</code>.</p>\n <p>Since the camera device has a pipeline of in-flight requests, the settings that\n get locked do not necessarily correspond to the settings that were present in the\n latest capture result received from the camera device, since additional captures\n and AE updates may have occurred even before the result was sent out. If an\n application is switching between automatic and manual control and wishes to eliminate\n any flicker during the switch, the following procedure is recommended:</p>\n <ol>\n <li>Starting in auto-AE mode:</li>\n <li>Lock AE</li>\n <li>Wait for the first result to be output that has the AE locked</li>\n <li>Copy exposure settings from that result into a request, set the request to manual AE</li>\n <li>Submit the capture request, proceed to run manual AE as desired.</li>\n </ol>\n <p>See ACAMERA_CONTROL_AE_STATE for AE lock related state transition details.</p>\n\n @see ACAMERA_CONTROL_AE_EXPOSURE_COMPENSATION\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER\n @see ACAMERA_CONTROL_AE_STATE\n @see ACAMERA_SENSOR_EXPOSURE_TIME\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_LOCK: acamera_metadata_tag = 65538;
#[doc = " <p>The desired mode for the camera device's\n auto-exposure routine.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_ae_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This control is only effective if ACAMERA_CONTROL_MODE is\n AUTO.</p>\n <p>When set to any of the ON modes, the camera device's\n auto-exposure routine is enabled, overriding the\n application's selected exposure time, sensor sensitivity,\n and frame duration (ACAMERA_SENSOR_EXPOSURE_TIME,\n ACAMERA_SENSOR_SENSITIVITY, and\n ACAMERA_SENSOR_FRAME_DURATION). If one of the FLASH modes\n is selected, the camera device's flash unit controls are\n also overridden.</p>\n <p>The FLASH modes are only available if the camera device\n has a flash unit (ACAMERA_FLASH_INFO_AVAILABLE is <code>true</code>).</p>\n <p>If flash TORCH mode is desired, this field must be set to\n ON or OFF, and ACAMERA_FLASH_MODE set to TORCH.</p>\n <p>When set to any of the ON modes, the values chosen by the\n camera device auto-exposure routine for the overridden\n fields for a given capture will be available in its\n CaptureResult.</p>\n\n @see ACAMERA_CONTROL_MODE\n @see ACAMERA_FLASH_INFO_AVAILABLE\n @see ACAMERA_FLASH_MODE\n @see ACAMERA_SENSOR_EXPOSURE_TIME\n @see ACAMERA_SENSOR_FRAME_DURATION\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_MODE: acamera_metadata_tag = 65539;
#[doc = " <p>List of metering areas to use for auto-exposure adjustment.</p>\n\n <p>Type: int32[5*area_count]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Not available if android.control.maxRegionsAe is 0.\n Otherwise will always be present.</p>\n <p>The maximum number of regions supported by the device is determined by the value\n of android.control.maxRegionsAe.</p>\n <p>For devices not supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system always follows that of ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with (0,0) being\n the top-left pixel in the active pixel array, and\n (ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.width - 1,\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.height - 1) being the bottom-right pixel in the\n active pixel array.</p>\n <p>For devices supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system depends on the mode being set.\n When the distortion correction mode is OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE, with\n <code>(0, 0)</code> being the top-left pixel of the pre-correction active array, and\n (ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE.width - 1,\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE.height - 1) being the bottom-right\n pixel in the pre-correction active pixel array.\n When the distortion correction mode is not OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with\n <code>(0, 0)</code> being the top-left pixel of the active array, and\n (ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.width - 1,\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.height - 1) being the bottom-right pixel in the\n active pixel array.</p>\n <p>The weight must be within <code>[0, 1000]</code>, and represents a weight\n for every pixel in the area. This means that a large metering area\n with the same weight as a smaller area will have more effect in\n the metering result. Metering areas can partially overlap and the\n camera device will add the weights in the overlap region.</p>\n <p>The weights are relative to weights of other exposure metering regions, so if only one\n region is used, all non-zero weights will have the same effect. A region with 0\n weight is ignored.</p>\n <p>If all regions have 0 weight, then no specific metering area needs to be used by the\n camera device.</p>\n <p>If the metering region is outside the used ACAMERA_SCALER_CROP_REGION returned in\n capture result metadata, the camera device will ignore the sections outside the crop\n region and output only the intersection rectangle as the metering region in the result\n metadata.  If the region is entirely outside the crop region, it will be ignored and\n not reported in the result metadata.</p>\n <p>When setting the AE metering regions, the application must consider the additional\n crop resulted from the aspect ratio differences between the preview stream and\n ACAMERA_SCALER_CROP_REGION. For example, if the ACAMERA_SCALER_CROP_REGION is the full\n active array size with 4:3 aspect ratio, and the preview stream is 16:9,\n the boundary of AE regions will be [0, y_crop] and\n [active_width, active_height - 2 * y_crop] rather than [0, 0] and\n [active_width, active_height], where y_crop is the additional crop due to aspect ratio\n mismatch.</p>\n <p>Starting from API level 30, the coordinate system of activeArraySize or\n preCorrectionActiveArraySize is used to represent post-zoomRatio field of view, not\n pre-zoom field of view. This means that the same aeRegions values at different\n ACAMERA_CONTROL_ZOOM_RATIO represent different parts of the scene. The aeRegions\n coordinates are relative to the activeArray/preCorrectionActiveArray representing the\n zoomed field of view. If ACAMERA_CONTROL_ZOOM_RATIO is set to 1.0 (default), the same\n aeRegions at different ACAMERA_SCALER_CROP_REGION still represent the same parts of the\n scene as they do before. See ACAMERA_CONTROL_ZOOM_RATIO for details. Whether to use\n activeArraySize or preCorrectionActiveArraySize still depends on distortion correction\n mode.</p>\n <p>For camera devices with the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability,\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION /\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION must be used as the\n coordinate system for requests where ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n <p>The data representation is <code>int[5 * area_count]</code>.\n Every five elements represent a metering region of <code>(xmin, ymin, xmax, ymax, weight)</code>.\n The rectangle is defined to be inclusive on xmin and ymin, but exclusive on xmax and\n ymax.</p>\n\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_DISTORTION_CORRECTION_MODE\n @see ACAMERA_SCALER_CROP_REGION\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_REGIONS: acamera_metadata_tag = 65540;
#[doc = " <p>Range over which the auto-exposure routine can\n adjust the capture frame rate to maintain good\n exposure.</p>\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Only constrains auto-exposure (AE) algorithm, not\n manual control of ACAMERA_SENSOR_EXPOSURE_TIME and\n ACAMERA_SENSOR_FRAME_DURATION.</p>\n\n @see ACAMERA_SENSOR_EXPOSURE_TIME\n @see ACAMERA_SENSOR_FRAME_DURATION"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_TARGET_FPS_RANGE: acamera_metadata_tag = 65541;
#[doc = " <p>Whether the camera device will trigger a precapture\n metering sequence when it processes this request.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_ae_precapture_trigger_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This entry is normally set to IDLE, or is not\n included at all in the request settings. When included and\n set to START, the camera device will trigger the auto-exposure (AE)\n precapture metering sequence.</p>\n <p>When set to CANCEL, the camera device will cancel any active\n precapture metering trigger, and return to its initial AE state.\n If a precapture metering sequence is already completed, and the camera\n device has implicitly locked the AE for subsequent still capture, the\n CANCEL trigger will unlock the AE and return to its initial AE state.</p>\n <p>The precapture sequence should be triggered before starting a\n high-quality still capture for final metering decisions to\n be made, and for firing pre-capture flash pulses to estimate\n scene brightness and required final capture flash power, when\n the flash is enabled.</p>\n <p>Normally, this entry should be set to START for only a\n single request, and the application should wait until the\n sequence completes before starting a new one.</p>\n <p>When a precapture metering sequence is finished, the camera device\n may lock the auto-exposure routine internally to be able to accurately expose the\n subsequent still capture image (<code>ACAMERA_CONTROL_CAPTURE_INTENT == STILL_CAPTURE</code>).\n For this case, the AE may not resume normal scan if no subsequent still capture is\n submitted. To ensure that the AE routine restarts normal scan, the application should\n submit a request with <code>ACAMERA_CONTROL_AE_LOCK == true</code>, followed by a request\n with <code>ACAMERA_CONTROL_AE_LOCK == false</code>, if the application decides not to submit a\n still capture request after the precapture sequence completes. Alternatively, for\n API level 23 or newer devices, the CANCEL can be used to unlock the camera device\n internally locked AE if the application doesn't submit a still capture request after\n the AE precapture trigger. Note that, the CANCEL was added in API level 23, and must not\n be used in devices that have earlier API levels.</p>\n <p>The exact effect of auto-exposure (AE) precapture trigger\n depends on the current AE mode and state; see\n ACAMERA_CONTROL_AE_STATE for AE precapture state transition\n details.</p>\n <p>On LEGACY-level devices, the precapture trigger is not supported;\n capturing a high-resolution JPEG image will automatically trigger a\n precapture sequence before the high-resolution capture, including\n potentially firing a pre-capture flash.</p>\n <p>Using the precapture trigger and the auto-focus trigger ACAMERA_CONTROL_AF_TRIGGER\n simultaneously is allowed. However, since these triggers often require cooperation between\n the auto-focus and auto-exposure routines (for example, the may need to be enabled for a\n focus sweep), the camera device may delay acting on a later trigger until the previous\n trigger has been fully handled. This may lead to longer intervals between the trigger and\n changes to ACAMERA_CONTROL_AE_STATE indicating the start of the precapture sequence, for\n example.</p>\n <p>If both the precapture and the auto-focus trigger are activated on the same request, then\n the camera device will complete them in the optimal order for that device.</p>\n\n @see ACAMERA_CONTROL_AE_LOCK\n @see ACAMERA_CONTROL_AE_STATE\n @see ACAMERA_CONTROL_AF_TRIGGER\n @see ACAMERA_CONTROL_CAPTURE_INTENT"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER: acamera_metadata_tag = 65542;
#[doc = " <p>Whether auto-focus (AF) is currently enabled, and what\n mode it is set to.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_af_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Only effective if ACAMERA_CONTROL_MODE = AUTO and the lens is not fixed focus\n (i.e. <code>ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE &gt; 0</code>). Also note that\n when ACAMERA_CONTROL_AE_MODE is OFF, the behavior of AF is device\n dependent. It is recommended to lock AF by using ACAMERA_CONTROL_AF_TRIGGER before\n setting ACAMERA_CONTROL_AE_MODE to OFF, or set AF mode to OFF when AE is OFF.</p>\n <p>If the lens is controlled by the camera device auto-focus algorithm,\n the camera device will report the current AF status in ACAMERA_CONTROL_AF_STATE\n in result metadata.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_AF_STATE\n @see ACAMERA_CONTROL_AF_TRIGGER\n @see ACAMERA_CONTROL_MODE\n @see ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AF_MODE: acamera_metadata_tag = 65543;
#[doc = " <p>List of metering areas to use for auto-focus.</p>\n\n <p>Type: int32[5*area_count]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Not available if android.control.maxRegionsAf is 0.\n Otherwise will always be present.</p>\n <p>The maximum number of focus areas supported by the device is determined by the value\n of android.control.maxRegionsAf.</p>\n <p>For devices not supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system always follows that of ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with (0,0) being\n the top-left pixel in the active pixel array, and\n (ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.width - 1,\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.height - 1) being the bottom-right pixel in the\n active pixel array.</p>\n <p>For devices supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system depends on the mode being set.\n When the distortion correction mode is OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE, with\n <code>(0, 0)</code> being the top-left pixel of the pre-correction active array, and\n (ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE.width - 1,\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE.height - 1) being the bottom-right\n pixel in the pre-correction active pixel array.\n When the distortion correction mode is not OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with\n <code>(0, 0)</code> being the top-left pixel of the active array, and\n (ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.width - 1,\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.height - 1) being the bottom-right pixel in the\n active pixel array.</p>\n <p>The weight must be within <code>[0, 1000]</code>, and represents a weight\n for every pixel in the area. This means that a large metering area\n with the same weight as a smaller area will have more effect in\n the metering result. Metering areas can partially overlap and the\n camera device will add the weights in the overlap region.</p>\n <p>The weights are relative to weights of other metering regions, so if only one region\n is used, all non-zero weights will have the same effect. A region with 0 weight is\n ignored.</p>\n <p>If all regions have 0 weight, then no specific metering area needs to be used by the\n camera device. The capture result will either be a zero weight region as well, or\n the region selected by the camera device as the focus area of interest.</p>\n <p>If the metering region is outside the used ACAMERA_SCALER_CROP_REGION returned in\n capture result metadata, the camera device will ignore the sections outside the crop\n region and output only the intersection rectangle as the metering region in the result\n metadata. If the region is entirely outside the crop region, it will be ignored and\n not reported in the result metadata.</p>\n <p>When setting the AF metering regions, the application must consider the additional\n crop resulted from the aspect ratio differences between the preview stream and\n ACAMERA_SCALER_CROP_REGION. For example, if the ACAMERA_SCALER_CROP_REGION is the full\n active array size with 4:3 aspect ratio, and the preview stream is 16:9,\n the boundary of AF regions will be [0, y_crop] and\n [active_width, active_height - 2 * y_crop] rather than [0, 0] and\n [active_width, active_height], where y_crop is the additional crop due to aspect ratio\n mismatch.</p>\n <p>Starting from API level 30, the coordinate system of activeArraySize or\n preCorrectionActiveArraySize is used to represent post-zoomRatio field of view, not\n pre-zoom field of view. This means that the same afRegions values at different\n ACAMERA_CONTROL_ZOOM_RATIO represent different parts of the scene. The afRegions\n coordinates are relative to the activeArray/preCorrectionActiveArray representing the\n zoomed field of view. If ACAMERA_CONTROL_ZOOM_RATIO is set to 1.0 (default), the same\n afRegions at different ACAMERA_SCALER_CROP_REGION still represent the same parts of the\n scene as they do before. See ACAMERA_CONTROL_ZOOM_RATIO for details. Whether to use\n activeArraySize or preCorrectionActiveArraySize still depends on distortion correction\n mode.</p>\n <p>For camera devices with the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability, ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION /\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION must be used as the\n coordinate system for requests where ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n <p>The data representation is <code>int[5 * area_count]</code>.\n Every five elements represent a metering region of <code>(xmin, ymin, xmax, ymax, weight)</code>.\n The rectangle is defined to be inclusive on xmin and ymin, but exclusive on xmax and\n ymax.</p>\n\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_DISTORTION_CORRECTION_MODE\n @see ACAMERA_SCALER_CROP_REGION\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AF_REGIONS: acamera_metadata_tag = 65544;
#[doc = " <p>Whether the camera device will trigger autofocus for this request.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_af_trigger_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This entry is normally set to IDLE, or is not\n included at all in the request settings.</p>\n <p>When included and set to START, the camera device will trigger the\n autofocus algorithm. If autofocus is disabled, this trigger has no effect.</p>\n <p>When set to CANCEL, the camera device will cancel any active trigger,\n and return to its initial AF state.</p>\n <p>Generally, applications should set this entry to START or CANCEL for only a\n single capture, and then return it to IDLE (or not set at all). Specifying\n START for multiple captures in a row means restarting the AF operation over\n and over again.</p>\n <p>See ACAMERA_CONTROL_AF_STATE for what the trigger means for each AF mode.</p>\n <p>Using the autofocus trigger and the precapture trigger ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER\n simultaneously is allowed. However, since these triggers often require cooperation between\n the auto-focus and auto-exposure routines (for example, the may need to be enabled for a\n focus sweep), the camera device may delay acting on a later trigger until the previous\n trigger has been fully handled. This may lead to longer intervals between the trigger and\n changes to ACAMERA_CONTROL_AF_STATE, for example.</p>\n\n @see ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER\n @see ACAMERA_CONTROL_AF_STATE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AF_TRIGGER: acamera_metadata_tag = 65545;
#[doc = " <p>Whether auto-white balance (AWB) is currently locked to its\n latest calculated values.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_awb_lock_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>When set to <code>true</code> (ON), the AWB algorithm is locked to its latest parameters,\n and will not change color balance settings until the lock is set to <code>false</code> (OFF).</p>\n <p>Since the camera device has a pipeline of in-flight requests, the settings that\n get locked do not necessarily correspond to the settings that were present in the\n latest capture result received from the camera device, since additional captures\n and AWB updates may have occurred even before the result was sent out. If an\n application is switching between automatic and manual control and wishes to eliminate\n any flicker during the switch, the following procedure is recommended:</p>\n <ol>\n <li>Starting in auto-AWB mode:</li>\n <li>Lock AWB</li>\n <li>Wait for the first result to be output that has the AWB locked</li>\n <li>Copy AWB settings from that result into a request, set the request to manual AWB</li>\n <li>Submit the capture request, proceed to run manual AWB as desired.</li>\n </ol>\n <p>Note that AWB lock is only meaningful when\n ACAMERA_CONTROL_AWB_MODE is in the AUTO mode; in other modes,\n AWB is already fixed to a specific setting.</p>\n <p>Some LEGACY devices may not support ON; the value is then overridden to OFF.</p>\n\n @see ACAMERA_CONTROL_AWB_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AWB_LOCK: acamera_metadata_tag = 65546;
#[doc = " <p>Whether auto-white balance (AWB) is currently setting the color\n transform fields, and what its illumination target\n is.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_awb_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This control is only effective if ACAMERA_CONTROL_MODE is AUTO.</p>\n <p>When set to the AUTO mode, the camera device's auto-white balance\n routine is enabled, overriding the application's selected\n ACAMERA_COLOR_CORRECTION_TRANSFORM, ACAMERA_COLOR_CORRECTION_GAINS and\n ACAMERA_COLOR_CORRECTION_MODE. Note that when ACAMERA_CONTROL_AE_MODE\n is OFF, the behavior of AWB is device dependent. It is recommended to\n also set AWB mode to OFF or lock AWB by using ACAMERA_CONTROL_AWB_LOCK before\n setting AE mode to OFF.</p>\n <p>When set to the OFF mode, the camera device's auto-white balance\n routine is disabled. The application manually controls the white\n balance by ACAMERA_COLOR_CORRECTION_TRANSFORM, ACAMERA_COLOR_CORRECTION_GAINS\n and ACAMERA_COLOR_CORRECTION_MODE.</p>\n <p>When set to any other modes, the camera device's auto-white\n balance routine is disabled. The camera device uses each\n particular illumination target for white balance\n adjustment. The application's values for\n ACAMERA_COLOR_CORRECTION_TRANSFORM,\n ACAMERA_COLOR_CORRECTION_GAINS and\n ACAMERA_COLOR_CORRECTION_MODE are ignored.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_MODE\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_AWB_LOCK\n @see ACAMERA_CONTROL_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AWB_MODE: acamera_metadata_tag = 65547;
#[doc = " <p>List of metering areas to use for auto-white-balance illuminant\n estimation.</p>\n\n <p>Type: int32[5*area_count]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Not available if android.control.maxRegionsAwb is 0.\n Otherwise will always be present.</p>\n <p>The maximum number of regions supported by the device is determined by the value\n of android.control.maxRegionsAwb.</p>\n <p>For devices not supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system always follows that of ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with (0,0) being\n the top-left pixel in the active pixel array, and\n (ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.width - 1,\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.height - 1) being the bottom-right pixel in the\n active pixel array.</p>\n <p>For devices supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system depends on the mode being set.\n When the distortion correction mode is OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE, with\n <code>(0, 0)</code> being the top-left pixel of the pre-correction active array, and\n (ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE.width - 1,\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE.height - 1) being the bottom-right\n pixel in the pre-correction active pixel array.\n When the distortion correction mode is not OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with\n <code>(0, 0)</code> being the top-left pixel of the active array, and\n (ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.width - 1,\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.height - 1) being the bottom-right pixel in the\n active pixel array.</p>\n <p>The weight must range from 0 to 1000, and represents a weight\n for every pixel in the area. This means that a large metering area\n with the same weight as a smaller area will have more effect in\n the metering result. Metering areas can partially overlap and the\n camera device will add the weights in the overlap region.</p>\n <p>The weights are relative to weights of other white balance metering regions, so if\n only one region is used, all non-zero weights will have the same effect. A region with\n 0 weight is ignored.</p>\n <p>If all regions have 0 weight, then no specific metering area needs to be used by the\n camera device.</p>\n <p>If the metering region is outside the used ACAMERA_SCALER_CROP_REGION returned in\n capture result metadata, the camera device will ignore the sections outside the crop\n region and output only the intersection rectangle as the metering region in the result\n metadata.  If the region is entirely outside the crop region, it will be ignored and\n not reported in the result metadata.</p>\n <p>When setting the AWB metering regions, the application must consider the additional\n crop resulted from the aspect ratio differences between the preview stream and\n ACAMERA_SCALER_CROP_REGION. For example, if the ACAMERA_SCALER_CROP_REGION is the full\n active array size with 4:3 aspect ratio, and the preview stream is 16:9,\n the boundary of AWB regions will be [0, y_crop] and\n [active_width, active_height - 2 * y_crop] rather than [0, 0] and\n [active_width, active_height], where y_crop is the additional crop due to aspect ratio\n mismatch.</p>\n <p>Starting from API level 30, the coordinate system of activeArraySize or\n preCorrectionActiveArraySize is used to represent post-zoomRatio field of view, not\n pre-zoom field of view. This means that the same awbRegions values at different\n ACAMERA_CONTROL_ZOOM_RATIO represent different parts of the scene. The awbRegions\n coordinates are relative to the activeArray/preCorrectionActiveArray representing the\n zoomed field of view. If ACAMERA_CONTROL_ZOOM_RATIO is set to 1.0 (default), the same\n awbRegions at different ACAMERA_SCALER_CROP_REGION still represent the same parts of\n the scene as they do before. See ACAMERA_CONTROL_ZOOM_RATIO for details. Whether to use\n activeArraySize or preCorrectionActiveArraySize still depends on distortion correction\n mode.</p>\n <p>For camera devices with the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability, ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION /\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION must be used as the\n coordinate system for requests where ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n <p>The data representation is <code>int[5 * area_count]</code>.\n Every five elements represent a metering region of <code>(xmin, ymin, xmax, ymax, weight)</code>.\n The rectangle is defined to be inclusive on xmin and ymin, but exclusive on xmax and\n ymax.</p>\n\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_DISTORTION_CORRECTION_MODE\n @see ACAMERA_SCALER_CROP_REGION\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AWB_REGIONS: acamera_metadata_tag = 65548;
#[doc = " <p>Information to the camera device 3A (auto-exposure,\n auto-focus, auto-white balance) routines about the purpose\n of this capture, to help the camera device to decide optimal 3A\n strategy.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_capture_intent_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This control (except for MANUAL) is only effective if\n <code>ACAMERA_CONTROL_MODE != OFF</code> and any 3A routine is active.</p>\n <p>All intents are supported by all devices, except that:</p>\n <ul>\n <li>ZERO_SHUTTER_LAG will be supported if ACAMERA_REQUEST_AVAILABLE_CAPABILITIES contains\n PRIVATE_REPROCESSING or YUV_REPROCESSING.</li>\n <li>MANUAL will be supported if ACAMERA_REQUEST_AVAILABLE_CAPABILITIES contains\n MANUAL_SENSOR.</li>\n <li>MOTION_TRACKING will be supported if ACAMERA_REQUEST_AVAILABLE_CAPABILITIES contains\n MOTION_TRACKING.</li>\n </ul>\n\n @see ACAMERA_CONTROL_MODE\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_CAPTURE_INTENT: acamera_metadata_tag = 65549;
#[doc = " <p>A special color effect to apply.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_effect_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>When this mode is set, a color effect will be applied\n to images produced by the camera device. The interpretation\n and implementation of these color effects is left to the\n implementor of the camera device, and should not be\n depended on to be consistent (or present) across all\n devices.</p>"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_EFFECT_MODE: acamera_metadata_tag = 65550;
#[doc = " <p>Overall mode of 3A (auto-exposure, auto-white-balance, auto-focus) control\n routines.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This is a top-level 3A control switch. When set to OFF, all 3A control\n by the camera device is disabled. The application must set the fields for\n capture parameters itself.</p>\n <p>When set to AUTO, the individual algorithm controls in\n ACAMERA_CONTROL_* are in effect, such as ACAMERA_CONTROL_AF_MODE.</p>\n <p>When set to USE_SCENE_MODE or USE_EXTENDED_SCENE_MODE, the individual controls in\n ACAMERA_CONTROL_* are mostly disabled, and the camera device\n implements one of the scene mode or extended scene mode settings (such as ACTION,\n SUNSET, PARTY, or BOKEH) as it wishes. The camera device scene mode\n 3A settings are provided by {@link ACameraCaptureSession_captureCallback_result capture results}.</p>\n <p>When set to OFF_KEEP_STATE, it is similar to OFF mode, the only difference\n is that this frame will not be used by camera device background 3A statistics\n update, as if this frame is never captured. This mode can be used in the scenario\n where the application doesn't want a 3A manual control capture to affect\n the subsequent auto 3A capture results.</p>\n\n @see ACAMERA_CONTROL_AF_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_MODE: acamera_metadata_tag = 65551;
#[doc = " <p>Control for which scene mode is currently active.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_scene_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Scene modes are custom camera modes optimized for a certain set of conditions and\n capture settings.</p>\n <p>This is the mode that that is active when\n <code>ACAMERA_CONTROL_MODE == USE_SCENE_MODE</code>. Aside from FACE_PRIORITY, these modes will\n disable ACAMERA_CONTROL_AE_MODE, ACAMERA_CONTROL_AWB_MODE, and ACAMERA_CONTROL_AF_MODE\n while in use.</p>\n <p>The interpretation and implementation of these scene modes is left\n to the implementor of the camera device. Their behavior will not be\n consistent across all devices, and any given device may only implement\n a subset of these modes.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_AF_MODE\n @see ACAMERA_CONTROL_AWB_MODE\n @see ACAMERA_CONTROL_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_SCENE_MODE: acamera_metadata_tag = 65552;
#[doc = " <p>Whether video stabilization is\n active.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_video_stabilization_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Video stabilization automatically warps images from\n the camera in order to stabilize motion between consecutive frames.</p>\n <p>If enabled, video stabilization can modify the\n ACAMERA_SCALER_CROP_REGION to keep the video stream stabilized.</p>\n <p>Switching between different video stabilization modes may take several\n frames to initialize, the camera device will report the current mode\n in capture result metadata. For example, When \"ON\" mode is requested,\n the video stabilization modes in the first several capture results may\n still be \"OFF\", and it will become \"ON\" when the initialization is\n done.</p>\n <p>In addition, not all recording sizes or frame rates may be supported for\n stabilization by a device that reports stabilization support. It is guaranteed\n that an output targeting a MediaRecorder or MediaCodec will be stabilized if\n the recording resolution is less than or equal to 1920 x 1080 (width less than\n or equal to 1920, height less than or equal to 1080), and the recording\n frame rate is less than or equal to 30fps.  At other sizes, the CaptureResult\n ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE field will return\n OFF if the recording output is not stabilized, or if there are no output\n Surface types that can be stabilized.</p>\n <p>If a camera device supports both this mode and OIS\n (ACAMERA_LENS_OPTICAL_STABILIZATION_MODE), turning both modes on may\n produce undesirable interaction, so it is recommended not to enable\n both at the same time.</p>\n <p>If video stabilization is set to \"PREVIEW_STABILIZATION\",\n ACAMERA_LENS_OPTICAL_STABILIZATION_MODE is overridden. The camera sub-system may choose\n to turn on hardware based image stabilization in addition to software based stabilization\n if it deems that appropriate.\n This key may be a part of the available session keys, which camera clients may\n query via\n {@link ACameraManager_getCameraCharacteristics }.\n If this is the case, changing this key over the life-time of a capture session may\n cause delays / glitches.</p>\n\n @see ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE\n @see ACAMERA_LENS_OPTICAL_STABILIZATION_MODE\n @see ACAMERA_SCALER_CROP_REGION"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE: acamera_metadata_tag =
    65553;
#[doc = " <p>List of auto-exposure antibanding modes for ACAMERA_CONTROL_AE_ANTIBANDING_MODE that are\n supported by this camera device.</p>\n\n @see ACAMERA_CONTROL_AE_ANTIBANDING_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Not all of the auto-exposure anti-banding modes may be\n supported by a given camera device. This field lists the\n valid anti-banding modes that the application may request\n for this camera device with the\n ACAMERA_CONTROL_AE_ANTIBANDING_MODE control.</p>\n\n @see ACAMERA_CONTROL_AE_ANTIBANDING_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_AVAILABLE_ANTIBANDING_MODES:
    acamera_metadata_tag = 65554;
#[doc = " <p>List of auto-exposure modes for ACAMERA_CONTROL_AE_MODE that are supported by this camera\n device.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Not all the auto-exposure modes may be supported by a\n given camera device, especially if no flash unit is\n available. This entry lists the valid modes for\n ACAMERA_CONTROL_AE_MODE for this camera device.</p>\n <p>All camera devices support ON, and all camera devices with flash\n units support ON_AUTO_FLASH and ON_ALWAYS_FLASH.</p>\n <p>FULL mode camera devices always support OFF mode,\n which enables application control of camera exposure time,\n sensitivity, and frame duration.</p>\n <p>LEGACY mode camera devices never support OFF mode.\n LIMITED mode devices support OFF if they support the MANUAL_SENSOR\n capability.</p>\n\n @see ACAMERA_CONTROL_AE_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_AVAILABLE_MODES: acamera_metadata_tag = 65555;
#[doc = " <p>List of frame rate ranges for ACAMERA_CONTROL_AE_TARGET_FPS_RANGE supported by\n this camera device.</p>\n\n @see ACAMERA_CONTROL_AE_TARGET_FPS_RANGE\n\n <p>Type: int32[2*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>For devices at the LEGACY level or above:</p>\n <ul>\n <li>\n <p>For constant-framerate recording, for each normal\n <a href=\"https://developer.android.com/reference/android/media/CamcorderProfile.html\">CamcorderProfile</a>, that is, a\n <a href=\"https://developer.android.com/reference/android/media/CamcorderProfile.html\">CamcorderProfile</a> that has\n <a href=\"https://developer.android.com/reference/android/media/CamcorderProfile.html#quality\">quality</a> in\n the range [<a href=\"https://developer.android.com/reference/android/media/CamcorderProfile.html#QUALITY_LOW\">QUALITY_LOW</a>,\n <a href=\"https://developer.android.com/reference/android/media/CamcorderProfile.html#QUALITY_2160P\">QUALITY_2160P</a>], if the profile is\n supported by the device and has\n <a href=\"https://developer.android.com/reference/android/media/CamcorderProfile.html#videoFrameRate\">videoFrameRate</a> <code>x</code>, this list will\n always include (<code>x</code>,<code>x</code>).</p>\n </li>\n <li>\n <p>Also, a camera device must either not support any\n <a href=\"https://developer.android.com/reference/android/media/CamcorderProfile.html\">CamcorderProfile</a>,\n or support at least one\n normal <a href=\"https://developer.android.com/reference/android/media/CamcorderProfile.html\">CamcorderProfile</a> that has\n <a href=\"https://developer.android.com/reference/android/media/CamcorderProfile.html#videoFrameRate\">videoFrameRate</a> <code>x</code> &gt;= 24.</p>\n </li>\n </ul>\n <p>For devices at the LIMITED level or above:</p>\n <ul>\n <li>For devices that advertise NIR color filter arrangement in\n ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT, this list will always include\n (<code>max</code>, <code>max</code>) where <code>max</code> = the maximum output frame rate of the maximum YUV_420_888\n output size.</li>\n <li>For devices advertising any color filter arrangement other than NIR, or devices not\n advertising color filter arrangement, this list will always include (<code>min</code>, <code>max</code>) and\n (<code>max</code>, <code>max</code>) where <code>min</code> &lt;= 15 and <code>max</code> = the maximum output frame rate of the\n maximum YUV_420_888 output size.</li>\n </ul>\n\n @see ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES:
    acamera_metadata_tag = 65556;
#[doc = " <p>Maximum and minimum exposure compensation values for\n ACAMERA_CONTROL_AE_EXPOSURE_COMPENSATION, in counts of ACAMERA_CONTROL_AE_COMPENSATION_STEP,\n that are supported by this camera device.</p>\n\n @see ACAMERA_CONTROL_AE_COMPENSATION_STEP\n @see ACAMERA_CONTROL_AE_EXPOSURE_COMPENSATION\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_COMPENSATION_RANGE: acamera_metadata_tag = 65557;
#[doc = " <p>Smallest step by which the exposure compensation\n can be changed.</p>\n\n <p>Type: rational</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This is the unit for ACAMERA_CONTROL_AE_EXPOSURE_COMPENSATION. For example, if this key has\n a value of <code>1/2</code>, then a setting of <code>-2</code> for ACAMERA_CONTROL_AE_EXPOSURE_COMPENSATION means\n that the target EV offset for the auto-exposure routine is -1 EV.</p>\n <p>One unit of EV compensation changes the brightness of the captured image by a factor\n of two. +1 EV doubles the image brightness, while -1 EV halves the image brightness.</p>\n\n @see ACAMERA_CONTROL_AE_EXPOSURE_COMPENSATION"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_COMPENSATION_STEP: acamera_metadata_tag = 65558;
#[doc = " <p>List of auto-focus (AF) modes for ACAMERA_CONTROL_AF_MODE that are\n supported by this camera device.</p>\n\n @see ACAMERA_CONTROL_AF_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Not all the auto-focus modes may be supported by a\n given camera device. This entry lists the valid modes for\n ACAMERA_CONTROL_AF_MODE for this camera device.</p>\n <p>All LIMITED and FULL mode camera devices will support OFF mode, and all\n camera devices with adjustable focuser units\n (<code>ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE &gt; 0</code>) will support AUTO mode.</p>\n <p>LEGACY devices will support OFF mode only if they support\n focusing to infinity (by also setting ACAMERA_LENS_FOCUS_DISTANCE to\n <code>0.0f</code>).</p>\n\n @see ACAMERA_CONTROL_AF_MODE\n @see ACAMERA_LENS_FOCUS_DISTANCE\n @see ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AF_AVAILABLE_MODES: acamera_metadata_tag = 65559;
#[doc = " <p>List of color effects for ACAMERA_CONTROL_EFFECT_MODE that are supported by this camera\n device.</p>\n\n @see ACAMERA_CONTROL_EFFECT_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This list contains the color effect modes that can be applied to\n images produced by the camera device.\n Implementations are not expected to be consistent across all devices.\n If no color effect modes are available for a device, this will only list\n OFF.</p>\n <p>A color effect will only be applied if\n ACAMERA_CONTROL_MODE != OFF.  OFF is always included in this list.</p>\n <p>This control has no effect on the operation of other control routines such\n as auto-exposure, white balance, or focus.</p>\n\n @see ACAMERA_CONTROL_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AVAILABLE_EFFECTS: acamera_metadata_tag = 65560;
#[doc = " <p>List of scene modes for ACAMERA_CONTROL_SCENE_MODE that are supported by this camera\n device.</p>\n\n @see ACAMERA_CONTROL_SCENE_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This list contains scene modes that can be set for the camera device.\n Only scene modes that have been fully implemented for the\n camera device may be included here. Implementations are not expected\n to be consistent across all devices.</p>\n <p>If no scene modes are supported by the camera device, this\n will be set to DISABLED. Otherwise DISABLED will not be listed.</p>\n <p>FACE_PRIORITY is always listed if face detection is\n supported (i.e.<code>ACAMERA_STATISTICS_INFO_MAX_FACE_COUNT &gt;\n 0</code>).</p>\n\n @see ACAMERA_STATISTICS_INFO_MAX_FACE_COUNT"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AVAILABLE_SCENE_MODES: acamera_metadata_tag = 65561;
#[doc = " <p>List of video stabilization modes for ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE\n that are supported by this camera device.</p>\n\n @see ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>OFF will always be listed.</p>"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES:
    acamera_metadata_tag = 65562;
#[doc = " <p>List of auto-white-balance modes for ACAMERA_CONTROL_AWB_MODE that are supported by this\n camera device.</p>\n\n @see ACAMERA_CONTROL_AWB_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Not all the auto-white-balance modes may be supported by a\n given camera device. This entry lists the valid modes for\n ACAMERA_CONTROL_AWB_MODE for this camera device.</p>\n <p>All camera devices will support ON mode.</p>\n <p>Camera devices that support the MANUAL_POST_PROCESSING capability will always support OFF\n mode, which enables application control of white balance, by using\n ACAMERA_COLOR_CORRECTION_TRANSFORM and ACAMERA_COLOR_CORRECTION_GAINS(ACAMERA_COLOR_CORRECTION_MODE must be set to TRANSFORM_MATRIX). This includes all FULL\n mode camera devices.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_MODE\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM\n @see ACAMERA_CONTROL_AWB_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AWB_AVAILABLE_MODES: acamera_metadata_tag = 65563;
#[doc = " <p>List of the maximum number of regions that can be used for metering in\n auto-exposure (AE), auto-white balance (AWB), and auto-focus (AF);\n this corresponds to the maximum number of elements in\n ACAMERA_CONTROL_AE_REGIONS, ACAMERA_CONTROL_AWB_REGIONS,\n and ACAMERA_CONTROL_AF_REGIONS.</p>\n\n @see ACAMERA_CONTROL_AE_REGIONS\n @see ACAMERA_CONTROL_AF_REGIONS\n @see ACAMERA_CONTROL_AWB_REGIONS\n\n <p>Type: int32[3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_MAX_REGIONS: acamera_metadata_tag = 65564;
#[doc = " <p>Current state of the auto-exposure (AE) algorithm.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_ae_state_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>Switching between or enabling AE modes (ACAMERA_CONTROL_AE_MODE) always\n resets the AE state to INACTIVE. Similarly, switching between ACAMERA_CONTROL_MODE,\n or ACAMERA_CONTROL_SCENE_MODE if <code>ACAMERA_CONTROL_MODE == USE_SCENE_MODE</code> resets all\n the algorithm states to INACTIVE.</p>\n <p>The camera device can do several state transitions between two results, if it is\n allowed by the state transition table. For example: INACTIVE may never actually be\n seen in a result.</p>\n <p>The state in the result is the state for this image (in sync with this image): if\n AE state becomes CONVERGED, then the image data associated with this result should\n be good to use.</p>\n <p>Below are state transition tables for different AE modes.</p>\n <p>State       | Transition Cause | New State | Notes\n :------------:|:----------------:|:---------:|:-----------------------:\n INACTIVE      |                  | INACTIVE  | Camera device auto exposure algorithm is disabled</p>\n <p>When ACAMERA_CONTROL_AE_MODE is AE_MODE_ON*:</p>\n <p>State        | Transition Cause                             | New State      | Notes\n :-------------:|:--------------------------------------------:|:--------------:|:-----------------:\n INACTIVE       | Camera device initiates AE scan              | SEARCHING      | Values changing\n INACTIVE       | ACAMERA_CONTROL_AE_LOCK is ON                 | LOCKED         | Values locked\n SEARCHING      | Camera device finishes AE scan               | CONVERGED      | Good values, not changing\n SEARCHING      | Camera device finishes AE scan               | FLASH_REQUIRED | Converged but too dark w/o flash\n SEARCHING      | ACAMERA_CONTROL_AE_LOCK is ON                 | LOCKED         | Values locked\n CONVERGED      | Camera device initiates AE scan              | SEARCHING      | Values changing\n CONVERGED      | ACAMERA_CONTROL_AE_LOCK is ON                 | LOCKED         | Values locked\n FLASH_REQUIRED | Camera device initiates AE scan              | SEARCHING      | Values changing\n FLASH_REQUIRED | ACAMERA_CONTROL_AE_LOCK is ON                 | LOCKED         | Values locked\n LOCKED         | ACAMERA_CONTROL_AE_LOCK is OFF                | SEARCHING      | Values not good after unlock\n LOCKED         | ACAMERA_CONTROL_AE_LOCK is OFF                | CONVERGED      | Values good after unlock\n LOCKED         | ACAMERA_CONTROL_AE_LOCK is OFF                | FLASH_REQUIRED | Exposure good, but too dark\n PRECAPTURE     | Sequence done. ACAMERA_CONTROL_AE_LOCK is OFF | CONVERGED      | Ready for high-quality capture\n PRECAPTURE     | Sequence done. ACAMERA_CONTROL_AE_LOCK is ON  | LOCKED         | Ready for high-quality capture\n LOCKED         | aeLock is ON and aePrecaptureTrigger is START | LOCKED        | Precapture trigger is ignored when AE is already locked\n LOCKED         | aeLock is ON and aePrecaptureTrigger is CANCEL| LOCKED        | Precapture trigger is ignored when AE is already locked\n Any state (excluding LOCKED) | ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER is START | PRECAPTURE     | Start AE precapture metering sequence\n Any state (excluding LOCKED) | ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER is CANCEL| INACTIVE       | Currently active precapture metering sequence is canceled</p>\n <p>If the camera device supports AE external flash mode (ON_EXTERNAL_FLASH is included in\n ACAMERA_CONTROL_AE_AVAILABLE_MODES), ACAMERA_CONTROL_AE_STATE must be FLASH_REQUIRED after\n the camera device finishes AE scan and it's too dark without flash.</p>\n <p>For the above table, the camera device may skip reporting any state changes that happen\n without application intervention (i.e. mode switch, trigger, locking). Any state that\n can be skipped in that manner is called a transient state.</p>\n <p>For example, for above AE modes (AE_MODE_ON*), in addition to the state transitions\n listed in above table, it is also legal for the camera device to skip one or more\n transient states between two results. See below table for examples:</p>\n <p>State        | Transition Cause                                            | New State      | Notes\n :-------------:|:-----------------------------------------------------------:|:--------------:|:-----------------:\n INACTIVE       | Camera device finished AE scan                              | CONVERGED      | Values are already good, transient states are skipped by camera device.\n Any state (excluding LOCKED) | ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER is START, sequence done | FLASH_REQUIRED | Converged but too dark w/o flash after a precapture sequence, transient states are skipped by camera device.\n Any state (excluding LOCKED) | ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER is START, sequence done | CONVERGED      | Converged after a precapture sequence, transient states are skipped by camera device.\n Any state (excluding LOCKED) | ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER is CANCEL, converged    | FLASH_REQUIRED | Converged but too dark w/o flash after a precapture sequence is canceled, transient states are skipped by camera device.\n Any state (excluding LOCKED) | ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER is CANCEL, converged    | CONVERGED      | Converged after a precapture sequences canceled, transient states are skipped by camera device.\n CONVERGED      | Camera device finished AE scan                              | FLASH_REQUIRED | Converged but too dark w/o flash after a new scan, transient states are skipped by camera device.\n FLASH_REQUIRED | Camera device finished AE scan                              | CONVERGED      | Converged after a new scan, transient states are skipped by camera device.</p>\n\n @see ACAMERA_CONTROL_AE_AVAILABLE_MODES\n @see ACAMERA_CONTROL_AE_LOCK\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER\n @see ACAMERA_CONTROL_AE_STATE\n @see ACAMERA_CONTROL_MODE\n @see ACAMERA_CONTROL_SCENE_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_STATE: acamera_metadata_tag = 65567;
#[doc = " <p>Current state of auto-focus (AF) algorithm.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_af_state_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>Switching between or enabling AF modes (ACAMERA_CONTROL_AF_MODE) always\n resets the AF state to INACTIVE. Similarly, switching between ACAMERA_CONTROL_MODE,\n or ACAMERA_CONTROL_SCENE_MODE if <code>ACAMERA_CONTROL_MODE == USE_SCENE_MODE</code> resets all\n the algorithm states to INACTIVE.</p>\n <p>The camera device can do several state transitions between two results, if it is\n allowed by the state transition table. For example: INACTIVE may never actually be\n seen in a result.</p>\n <p>The state in the result is the state for this image (in sync with this image): if\n AF state becomes FOCUSED, then the image data associated with this result should\n be sharp.</p>\n <p>Below are state transition tables for different AF modes.</p>\n <p>When ACAMERA_CONTROL_AF_MODE is AF_MODE_OFF or AF_MODE_EDOF:</p>\n <p>State       | Transition Cause | New State | Notes\n :------------:|:----------------:|:---------:|:-----------:\n INACTIVE      |                  | INACTIVE  | Never changes</p>\n <p>When ACAMERA_CONTROL_AF_MODE is AF_MODE_AUTO or AF_MODE_MACRO:</p>\n <p>State            | Transition Cause | New State          | Notes\n :-----------------:|:----------------:|:------------------:|:--------------:\n INACTIVE           | AF_TRIGGER       | ACTIVE_SCAN        | Start AF sweep, Lens now moving\n ACTIVE_SCAN        | AF sweep done    | FOCUSED_LOCKED     | Focused, Lens now locked\n ACTIVE_SCAN        | AF sweep done    | NOT_FOCUSED_LOCKED | Not focused, Lens now locked\n ACTIVE_SCAN        | AF_CANCEL        | INACTIVE           | Cancel/reset AF, Lens now locked\n FOCUSED_LOCKED     | AF_CANCEL        | INACTIVE           | Cancel/reset AF\n FOCUSED_LOCKED     | AF_TRIGGER       | ACTIVE_SCAN        | Start new sweep, Lens now moving\n NOT_FOCUSED_LOCKED | AF_CANCEL        | INACTIVE           | Cancel/reset AF\n NOT_FOCUSED_LOCKED | AF_TRIGGER       | ACTIVE_SCAN        | Start new sweep, Lens now moving\n Any state          | Mode change      | INACTIVE           |</p>\n <p>For the above table, the camera device may skip reporting any state changes that happen\n without application intervention (i.e. mode switch, trigger, locking). Any state that\n can be skipped in that manner is called a transient state.</p>\n <p>For example, for these AF modes (AF_MODE_AUTO and AF_MODE_MACRO), in addition to the\n state transitions listed in above table, it is also legal for the camera device to skip\n one or more transient states between two results. See below table for examples:</p>\n <p>State            | Transition Cause | New State          | Notes\n :-----------------:|:----------------:|:------------------:|:--------------:\n INACTIVE           | AF_TRIGGER       | FOCUSED_LOCKED     | Focus is already good or good after a scan, lens is now locked.\n INACTIVE           | AF_TRIGGER       | NOT_FOCUSED_LOCKED | Focus failed after a scan, lens is now locked.\n FOCUSED_LOCKED     | AF_TRIGGER       | FOCUSED_LOCKED     | Focus is already good or good after a scan, lens is now locked.\n NOT_FOCUSED_LOCKED | AF_TRIGGER       | FOCUSED_LOCKED     | Focus is good after a scan, lens is not locked.</p>\n <p>When ACAMERA_CONTROL_AF_MODE is AF_MODE_CONTINUOUS_VIDEO:</p>\n <p>State            | Transition Cause                    | New State          | Notes\n :-----------------:|:-----------------------------------:|:------------------:|:--------------:\n INACTIVE           | Camera device initiates new scan    | PASSIVE_SCAN       | Start AF scan, Lens now moving\n INACTIVE           | AF_TRIGGER                          | NOT_FOCUSED_LOCKED | AF state query, Lens now locked\n PASSIVE_SCAN       | Camera device completes current scan| PASSIVE_FOCUSED    | End AF scan, Lens now locked\n PASSIVE_SCAN       | Camera device fails current scan    | PASSIVE_UNFOCUSED  | End AF scan, Lens now locked\n PASSIVE_SCAN       | AF_TRIGGER                          | FOCUSED_LOCKED     | Immediate transition, if focus is good. Lens now locked\n PASSIVE_SCAN       | AF_TRIGGER                          | NOT_FOCUSED_LOCKED | Immediate transition, if focus is bad. Lens now locked\n PASSIVE_SCAN       | AF_CANCEL                           | INACTIVE           | Reset lens position, Lens now locked\n PASSIVE_FOCUSED    | Camera device initiates new scan    | PASSIVE_SCAN       | Start AF scan, Lens now moving\n PASSIVE_UNFOCUSED  | Camera device initiates new scan    | PASSIVE_SCAN       | Start AF scan, Lens now moving\n PASSIVE_FOCUSED    | AF_TRIGGER                          | FOCUSED_LOCKED     | Immediate transition, lens now locked\n PASSIVE_UNFOCUSED  | AF_TRIGGER                          | NOT_FOCUSED_LOCKED | Immediate transition, lens now locked\n FOCUSED_LOCKED     | AF_TRIGGER                          | FOCUSED_LOCKED     | No effect\n FOCUSED_LOCKED     | AF_CANCEL                           | INACTIVE           | Restart AF scan\n NOT_FOCUSED_LOCKED | AF_TRIGGER                          | NOT_FOCUSED_LOCKED | No effect\n NOT_FOCUSED_LOCKED | AF_CANCEL                           | INACTIVE           | Restart AF scan</p>\n <p>When ACAMERA_CONTROL_AF_MODE is AF_MODE_CONTINUOUS_PICTURE:</p>\n <p>State            | Transition Cause                     | New State          | Notes\n :-----------------:|:------------------------------------:|:------------------:|:--------------:\n INACTIVE           | Camera device initiates new scan     | PASSIVE_SCAN       | Start AF scan, Lens now moving\n INACTIVE           | AF_TRIGGER                           | NOT_FOCUSED_LOCKED | AF state query, Lens now locked\n PASSIVE_SCAN       | Camera device completes current scan | PASSIVE_FOCUSED    | End AF scan, Lens now locked\n PASSIVE_SCAN       | Camera device fails current scan     | PASSIVE_UNFOCUSED  | End AF scan, Lens now locked\n PASSIVE_SCAN       | AF_TRIGGER                           | FOCUSED_LOCKED     | Eventual transition once the focus is good. Lens now locked\n PASSIVE_SCAN       | AF_TRIGGER                           | NOT_FOCUSED_LOCKED | Eventual transition if cannot find focus. Lens now locked\n PASSIVE_SCAN       | AF_CANCEL                            | INACTIVE           | Reset lens position, Lens now locked\n PASSIVE_FOCUSED    | Camera device initiates new scan     | PASSIVE_SCAN       | Start AF scan, Lens now moving\n PASSIVE_UNFOCUSED  | Camera device initiates new scan     | PASSIVE_SCAN       | Start AF scan, Lens now moving\n PASSIVE_FOCUSED    | AF_TRIGGER                           | FOCUSED_LOCKED     | Immediate trans. Lens now locked\n PASSIVE_UNFOCUSED  | AF_TRIGGER                           | NOT_FOCUSED_LOCKED | Immediate trans. Lens now locked\n FOCUSED_LOCKED     | AF_TRIGGER                           | FOCUSED_LOCKED     | No effect\n FOCUSED_LOCKED     | AF_CANCEL                            | INACTIVE           | Restart AF scan\n NOT_FOCUSED_LOCKED | AF_TRIGGER                           | NOT_FOCUSED_LOCKED | No effect\n NOT_FOCUSED_LOCKED | AF_CANCEL                            | INACTIVE           | Restart AF scan</p>\n <p>When switch between AF_MODE_CONTINUOUS_* (CAF modes) and AF_MODE_AUTO/AF_MODE_MACRO\n (AUTO modes), the initial INACTIVE or PASSIVE_SCAN states may be skipped by the\n camera device. When a trigger is included in a mode switch request, the trigger\n will be evaluated in the context of the new mode in the request.\n See below table for examples:</p>\n <p>State      | Transition Cause                       | New State                                | Notes\n :-----------:|:--------------------------------------:|:----------------------------------------:|:--------------:\n any state    | CAF--&gt;AUTO mode switch                 | INACTIVE                                 | Mode switch without trigger, initial state must be INACTIVE\n any state    | CAF--&gt;AUTO mode switch with AF_TRIGGER | trigger-reachable states from INACTIVE   | Mode switch with trigger, INACTIVE is skipped\n any state    | AUTO--&gt;CAF mode switch                 | passively reachable states from INACTIVE | Mode switch without trigger, passive transient state is skipped</p>\n\n @see ACAMERA_CONTROL_AF_MODE\n @see ACAMERA_CONTROL_MODE\n @see ACAMERA_CONTROL_SCENE_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AF_STATE: acamera_metadata_tag = 65568;
#[doc = " <p>Current state of auto-white balance (AWB) algorithm.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_awb_state_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>Switching between or enabling AWB modes (ACAMERA_CONTROL_AWB_MODE) always\n resets the AWB state to INACTIVE. Similarly, switching between ACAMERA_CONTROL_MODE,\n or ACAMERA_CONTROL_SCENE_MODE if <code>ACAMERA_CONTROL_MODE == USE_SCENE_MODE</code> resets all\n the algorithm states to INACTIVE.</p>\n <p>The camera device can do several state transitions between two results, if it is\n allowed by the state transition table. So INACTIVE may never actually be seen in\n a result.</p>\n <p>The state in the result is the state for this image (in sync with this image): if\n AWB state becomes CONVERGED, then the image data associated with this result should\n be good to use.</p>\n <p>Below are state transition tables for different AWB modes.</p>\n <p>When <code>ACAMERA_CONTROL_AWB_MODE != AWB_MODE_AUTO</code>:</p>\n <p>State       | Transition Cause | New State | Notes\n :------------:|:----------------:|:---------:|:-----------------------:\n INACTIVE      |                  |INACTIVE   |Camera device auto white balance algorithm is disabled</p>\n <p>When ACAMERA_CONTROL_AWB_MODE is AWB_MODE_AUTO:</p>\n <p>State        | Transition Cause                 | New State     | Notes\n :-------------:|:--------------------------------:|:-------------:|:-----------------:\n INACTIVE       | Camera device initiates AWB scan | SEARCHING     | Values changing\n INACTIVE       | ACAMERA_CONTROL_AWB_LOCK is ON    | LOCKED        | Values locked\n SEARCHING      | Camera device finishes AWB scan  | CONVERGED     | Good values, not changing\n SEARCHING      | ACAMERA_CONTROL_AWB_LOCK is ON    | LOCKED        | Values locked\n CONVERGED      | Camera device initiates AWB scan | SEARCHING     | Values changing\n CONVERGED      | ACAMERA_CONTROL_AWB_LOCK is ON    | LOCKED        | Values locked\n LOCKED         | ACAMERA_CONTROL_AWB_LOCK is OFF   | SEARCHING     | Values not good after unlock</p>\n <p>For the above table, the camera device may skip reporting any state changes that happen\n without application intervention (i.e. mode switch, trigger, locking). Any state that\n can be skipped in that manner is called a transient state.</p>\n <p>For example, for this AWB mode (AWB_MODE_AUTO), in addition to the state transitions\n listed in above table, it is also legal for the camera device to skip one or more\n transient states between two results. See below table for examples:</p>\n <p>State        | Transition Cause                 | New State     | Notes\n :-------------:|:--------------------------------:|:-------------:|:-----------------:\n INACTIVE       | Camera device finished AWB scan  | CONVERGED     | Values are already good, transient states are skipped by camera device.\n LOCKED         | ACAMERA_CONTROL_AWB_LOCK is OFF   | CONVERGED     | Values good after unlock, transient states are skipped by camera device.</p>\n\n @see ACAMERA_CONTROL_AWB_LOCK\n @see ACAMERA_CONTROL_AWB_MODE\n @see ACAMERA_CONTROL_MODE\n @see ACAMERA_CONTROL_SCENE_MODE"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AWB_STATE: acamera_metadata_tag = 65570;
#[doc = " <p>Whether the camera device supports ACAMERA_CONTROL_AE_LOCK</p>\n\n @see ACAMERA_CONTROL_AE_LOCK\n\n <p>Type: byte (acamera_metadata_enum_android_control_ae_lock_available_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Devices with MANUAL_SENSOR capability or BURST_CAPTURE capability will always\n list <code>true</code>. This includes FULL devices.</p>"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AE_LOCK_AVAILABLE: acamera_metadata_tag = 65572;
#[doc = " <p>Whether the camera device supports ACAMERA_CONTROL_AWB_LOCK</p>\n\n @see ACAMERA_CONTROL_AWB_LOCK\n\n <p>Type: byte (acamera_metadata_enum_android_control_awb_lock_available_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Devices with MANUAL_POST_PROCESSING capability or BURST_CAPTURE capability will\n always list <code>true</code>. This includes FULL devices.</p>"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AWB_LOCK_AVAILABLE: acamera_metadata_tag = 65573;
#[doc = " <p>List of control modes for ACAMERA_CONTROL_MODE that are supported by this camera\n device.</p>\n\n @see ACAMERA_CONTROL_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This list contains control modes that can be set for the camera device.\n LEGACY mode devices will always support AUTO mode. LIMITED and FULL\n devices will always support OFF, AUTO modes.</p>"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AVAILABLE_MODES: acamera_metadata_tag = 65574;
#[doc = " <p>Range of boosts for ACAMERA_CONTROL_POST_RAW_SENSITIVITY_BOOST supported\n by this camera device.</p>\n\n @see ACAMERA_CONTROL_POST_RAW_SENSITIVITY_BOOST\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Devices support post RAW sensitivity boost  will advertise\n ACAMERA_CONTROL_POST_RAW_SENSITIVITY_BOOST key for controlling\n post RAW sensitivity boost.</p>\n <p>This key will be <code>null</code> for devices that do not support any RAW format\n outputs. For devices that do support RAW format outputs, this key will always\n present, and if a device does not support post RAW sensitivity boost, it will\n list <code>(100, 100)</code> in this key.</p>\n\n @see ACAMERA_CONTROL_POST_RAW_SENSITIVITY_BOOST"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_POST_RAW_SENSITIVITY_BOOST_RANGE:
    acamera_metadata_tag = 65575;
#[doc = " <p>The amount of additional sensitivity boost applied to output images\n after RAW sensor data is captured.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Some camera devices support additional digital sensitivity boosting in the\n camera processing pipeline after sensor RAW image is captured.\n Such a boost will be applied to YUV/JPEG format output images but will not\n have effect on RAW output formats like RAW_SENSOR, RAW10, RAW12 or RAW_OPAQUE.</p>\n <p>This key will be <code>null</code> for devices that do not support any RAW format\n outputs. For devices that do support RAW format outputs, this key will always\n present, and if a device does not support post RAW sensitivity boost, it will\n list <code>100</code> in this key.</p>\n <p>If the camera device cannot apply the exact boost requested, it will reduce the\n boost to the nearest supported value.\n The final boost value used will be available in the output capture result.</p>\n <p>For devices that support post RAW sensitivity boost, the YUV/JPEG output images\n of such device will have the total sensitivity of\n <code>ACAMERA_SENSOR_SENSITIVITY * ACAMERA_CONTROL_POST_RAW_SENSITIVITY_BOOST / 100</code>\n The sensitivity of RAW format images will always be <code>ACAMERA_SENSOR_SENSITIVITY</code></p>\n <p>This control is only effective if ACAMERA_CONTROL_AE_MODE or ACAMERA_CONTROL_MODE is set to\n OFF; otherwise the auto-exposure algorithm will override this value.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_MODE\n @see ACAMERA_CONTROL_POST_RAW_SENSITIVITY_BOOST\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_POST_RAW_SENSITIVITY_BOOST: acamera_metadata_tag =
    65576;
#[doc = " <p>Allow camera device to enable zero-shutter-lag mode for requests with\n ACAMERA_CONTROL_CAPTURE_INTENT == STILL_CAPTURE.</p>\n\n @see ACAMERA_CONTROL_CAPTURE_INTENT\n\n <p>Type: byte (acamera_metadata_enum_android_control_enable_zsl_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>If enableZsl is <code>true</code>, the camera device may enable zero-shutter-lag mode for requests with\n STILL_CAPTURE capture intent. The camera device may use images captured in the past to\n produce output images for a zero-shutter-lag request. The result metadata including the\n ACAMERA_SENSOR_TIMESTAMP reflects the source frames used to produce output images.\n Therefore, the contents of the output images and the result metadata may be out of order\n compared to previous regular requests. enableZsl does not affect requests with other\n capture intents.</p>\n <p>For example, when requests are submitted in the following order:\n   Request A: enableZsl is ON, ACAMERA_CONTROL_CAPTURE_INTENT is PREVIEW\n   Request B: enableZsl is ON, ACAMERA_CONTROL_CAPTURE_INTENT is STILL_CAPTURE</p>\n <p>The output images for request B may have contents captured before the output images for\n request A, and the result metadata for request B may be older than the result metadata for\n request A.</p>\n <p>Note that when enableZsl is <code>true</code>, it is not guaranteed to get output images captured in\n the past for requests with STILL_CAPTURE capture intent.</p>\n <p>For applications targeting SDK versions O and newer, the value of enableZsl in\n TEMPLATE_STILL_CAPTURE template may be <code>true</code>. The value in other templates is always\n <code>false</code> if present.</p>\n <p>For applications targeting SDK versions older than O, the value of enableZsl in all\n capture templates is always <code>false</code> if present.</p>\n <p>For application-operated ZSL, use CAMERA3_TEMPLATE_ZERO_SHUTTER_LAG template.</p>\n\n @see ACAMERA_CONTROL_CAPTURE_INTENT\n @see ACAMERA_SENSOR_TIMESTAMP"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_ENABLE_ZSL: acamera_metadata_tag = 65577;
#[doc = " <p>Whether a significant scene change is detected within the currently-set AF\n region(s).</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_af_scene_change_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>When the camera focus routine detects a change in the scene it is looking at,\n such as a large shift in camera viewpoint, significant motion in the scene, or a\n significant illumination change, this value will be set to DETECTED for a single capture\n result. Otherwise the value will be NOT_DETECTED. The threshold for detection is similar\n to what would trigger a new passive focus scan to begin in CONTINUOUS autofocus modes.</p>\n <p>This key will be available if the camera device advertises this key via {@link ACAMERA_REQUEST_AVAILABLE_RESULT_KEYS }.</p>"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AF_SCENE_CHANGE: acamera_metadata_tag = 65578;
#[doc = " <p>The list of extended scene modes for ACAMERA_CONTROL_EXTENDED_SCENE_MODE that are supported\n by this camera device, and each extended scene mode's maximum streaming (non-stall) size\n with  effect.</p>\n\n @see ACAMERA_CONTROL_EXTENDED_SCENE_MODE\n\n <p>Type: int32[3*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>For DISABLED mode, the camera behaves normally with no extended scene mode enabled.</p>\n <p>For BOKEH_STILL_CAPTURE mode, the maximum streaming dimension specifies the limit\n under which bokeh is effective when capture intent is PREVIEW. Note that when capture\n intent is PREVIEW, the bokeh effect may not be as high in quality compared to\n STILL_CAPTURE intent in order to maintain reasonable frame rate. The maximum streaming\n dimension must be one of the YUV_420_888 or PRIVATE resolutions in\n availableStreamConfigurations, or (0, 0) if preview bokeh is not supported. If the\n application configures a stream larger than the maximum streaming dimension, bokeh\n effect may not be applied for this stream for PREVIEW intent.</p>\n <p>For BOKEH_CONTINUOUS mode, the maximum streaming dimension specifies the limit under\n which bokeh is effective. This dimension must be one of the YUV_420_888 or PRIVATE\n resolutions in availableStreamConfigurations, and if the sensor maximum resolution is\n larger than or equal to 1080p, the maximum streaming dimension must be at least 1080p.\n If the application configures a stream with larger dimension, the stream may not have\n bokeh effect applied.</p>"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AVAILABLE_EXTENDED_SCENE_MODE_MAX_SIZES:
    acamera_metadata_tag = 65579;
#[doc = " <p>The ranges of supported zoom ratio for non-DISABLED ACAMERA_CONTROL_EXTENDED_SCENE_MODE.</p>\n\n @see ACAMERA_CONTROL_EXTENDED_SCENE_MODE\n\n <p>Type: float[2*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>When extended scene mode is set, the camera device may have limited range of zoom ratios\n compared to when extended scene mode is DISABLED. This tag lists the zoom ratio ranges\n for all supported non-DISABLED extended scene modes, in the same order as in\n android.control.availableExtended.</p>\n <p>Range [1.0, 1.0] means that no zoom (optical or digital) is supported.</p>"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_AVAILABLE_EXTENDED_SCENE_MODE_ZOOM_RATIO_RANGES:
    acamera_metadata_tag = 65580;
#[doc = " <p>Whether extended scene mode is enabled for a particular capture request.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_control_extended_scene_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>With bokeh mode, the camera device may blur out the parts of scene that are not in\n focus, creating a bokeh (or shallow depth of field) effect for people or objects.</p>\n <p>When set to BOKEH_STILL_CAPTURE mode with STILL_CAPTURE capture intent, due to the extra\n processing needed for high quality bokeh effect, the stall may be longer than when\n capture intent is not STILL_CAPTURE.</p>\n <p>When set to BOKEH_STILL_CAPTURE mode with PREVIEW capture intent,</p>\n <ul>\n <li>If the camera device has BURST_CAPTURE capability, the frame rate requirement of\n BURST_CAPTURE must still be met.</li>\n <li>All streams not larger than the maximum streaming dimension for BOKEH_STILL_CAPTURE mode\n (queried via {@link ACAMERA_CONTROL_AVAILABLE_EXTENDED_SCENE_MODE_MAX_SIZES })\n will have preview bokeh effect applied.</li>\n </ul>\n <p>When set to BOKEH_CONTINUOUS mode, configured streams dimension should not exceed this mode's\n maximum streaming dimension in order to have bokeh effect applied. Bokeh effect may not\n be available for streams larger than the maximum streaming dimension.</p>\n <p>Switching between different extended scene modes may involve reconfiguration of the camera\n pipeline, resulting in long latency. The application should check this key against the\n available session keys queried via\n {@link ACameraManager_getCameraCharacteristics }.</p>\n <p>For a logical multi-camera, bokeh may be implemented by stereo vision from sub-cameras\n with different field of view. As a result, when bokeh mode is enabled, the camera device\n may override ACAMERA_SCALER_CROP_REGION or ACAMERA_CONTROL_ZOOM_RATIO, and the field of\n view may be smaller than when bokeh mode is off.</p>\n\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_SCALER_CROP_REGION"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_EXTENDED_SCENE_MODE: acamera_metadata_tag = 65581;
#[doc = " <p>Minimum and maximum zoom ratios supported by this camera device.</p>\n\n <p>Type: float[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If the camera device supports zoom-out from 1x zoom, minZoom will be less than 1.0, and\n setting ACAMERA_CONTROL_ZOOM_RATIO to values less than 1.0 increases the camera's field\n of view.</p>\n\n @see ACAMERA_CONTROL_ZOOM_RATIO"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_ZOOM_RATIO_RANGE: acamera_metadata_tag = 65582;
#[doc = " <p>The desired zoom ratio</p>\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Instead of using ACAMERA_SCALER_CROP_REGION for zoom, the application can now choose to\n use this tag to specify the desired zoom level.</p>\n <p>By using this control, the application gains a simpler way to control zoom, which can\n be a combination of optical and digital zoom. For example, a multi-camera system may\n contain more than one lens with different focal lengths, and the user can use optical\n zoom by switching between lenses. Using zoomRatio has benefits in the scenarios below:</p>\n <ul>\n <li>Zooming in from a wide-angle lens to a telephoto lens: A floating-point ratio provides\n   better precision compared to an integer value of ACAMERA_SCALER_CROP_REGION.</li>\n <li>Zooming out from a wide lens to an ultrawide lens: zoomRatio supports zoom-out whereas\n   ACAMERA_SCALER_CROP_REGION doesn't.</li>\n </ul>\n <p>To illustrate, here are several scenarios of different zoom ratios, crop regions,\n and output streams, for a hypothetical camera device with an active array of size\n <code>(2000,1500)</code>.</p>\n <ul>\n <li>Camera Configuration:<ul>\n <li>Active array size: <code>2000x1500</code> (3 MP, 4:3 aspect ratio)</li>\n <li>Output stream #1: <code>640x480</code> (VGA, 4:3 aspect ratio)</li>\n <li>Output stream #2: <code>1280x720</code> (720p, 16:9 aspect ratio)</li>\n </ul>\n </li>\n <li>Case #1: 4:3 crop region with 2.0x zoom ratio<ul>\n <li>Zoomed field of view: 1/4 of original field of view</li>\n <li>Crop region: <code>Rect(0, 0, 2000, 1500) // (left, top, right, bottom)</code> (post zoom)</li>\n </ul>\n </li>\n <li><img alt=\"4:3 aspect ratio crop diagram\" src=\"../images/camera2/metadata/android.control.zoomRatio/zoom-ratio-2-crop-43.png\" /><ul>\n <li><code>640x480</code> stream source area: <code>(0, 0, 2000, 1500)</code> (equal to crop region)</li>\n <li><code>1280x720</code> stream source area: <code>(0, 187, 2000, 1312)</code> (letterboxed)</li>\n </ul>\n </li>\n <li>Case #2: 16:9 crop region with 2.0x zoom.<ul>\n <li>Zoomed field of view: 1/4 of original field of view</li>\n <li>Crop region: <code>Rect(0, 187, 2000, 1312)</code></li>\n <li><img alt=\"16:9 aspect ratio crop diagram\" src=\"../images/camera2/metadata/android.control.zoomRatio/zoom-ratio-2-crop-169.png\" /></li>\n <li><code>640x480</code> stream source area: <code>(250, 187, 1750, 1312)</code> (pillarboxed)</li>\n <li><code>1280x720</code> stream source area: <code>(0, 187, 2000, 1312)</code> (equal to crop region)</li>\n </ul>\n </li>\n <li>Case #3: 1:1 crop region with 0.5x zoom out to ultrawide lens.<ul>\n <li>Zoomed field of view: 4x of original field of view (switched from wide lens to ultrawide lens)</li>\n <li>Crop region: <code>Rect(250, 0, 1750, 1500)</code></li>\n <li><img alt=\"1:1 aspect ratio crop diagram\" src=\"../images/camera2/metadata/android.control.zoomRatio/zoom-ratio-0.5-crop-11.png\" /></li>\n <li><code>640x480</code> stream source area: <code>(250, 187, 1750, 1312)</code> (letterboxed)</li>\n <li><code>1280x720</code> stream source area: <code>(250, 328, 1750, 1172)</code> (letterboxed)</li>\n </ul>\n </li>\n </ul>\n <p>As seen from the graphs above, the coordinate system of cropRegion now changes to the\n effective after-zoom field-of-view, and is represented by the rectangle of (0, 0,\n activeArrayWith, activeArrayHeight). The same applies to AE/AWB/AF regions, and faces.\n This coordinate system change isn't applicable to RAW capture and its related\n metadata such as intrinsicCalibration and lensShadingMap.</p>\n <p>Using the same hypothetical example above, and assuming output stream #1 (640x480) is\n the viewfinder stream, the application can achieve 2.0x zoom in one of two ways:</p>\n <ul>\n <li>zoomRatio = 2.0, scaler.cropRegion = (0, 0, 2000, 1500)</li>\n <li>zoomRatio = 1.0 (default), scaler.cropRegion = (500, 375, 1500, 1125)</li>\n </ul>\n <p>If the application intends to set aeRegions to be top-left quarter of the viewfinder\n field-of-view, the ACAMERA_CONTROL_AE_REGIONS should be set to (0, 0, 1000, 750) with\n zoomRatio set to 2.0. Alternatively, the application can set aeRegions to the equivalent\n region of (500, 375, 1000, 750) for zoomRatio of 1.0. If the application doesn't\n explicitly set ACAMERA_CONTROL_ZOOM_RATIO, its value defaults to 1.0.</p>\n <p>One limitation of controlling zoom using zoomRatio is that the ACAMERA_SCALER_CROP_REGION\n must only be used for letterboxing or pillarboxing of the sensor active array, and no\n FREEFORM cropping can be used with ACAMERA_CONTROL_ZOOM_RATIO other than 1.0. If\n ACAMERA_CONTROL_ZOOM_RATIO is not 1.0, and ACAMERA_SCALER_CROP_REGION is set to be\n windowboxing, the camera framework will override the ACAMERA_SCALER_CROP_REGION to be\n the active array.</p>\n <p>In the capture request, if the application sets ACAMERA_CONTROL_ZOOM_RATIO to a\n value != 1.0, the ACAMERA_CONTROL_ZOOM_RATIO tag in the capture result reflects the\n effective zoom ratio achieved by the camera device, and the ACAMERA_SCALER_CROP_REGION\n adjusts for additional crops that are not zoom related. Otherwise, if the application\n sets ACAMERA_CONTROL_ZOOM_RATIO to 1.0, or does not set it at all, the\n ACAMERA_CONTROL_ZOOM_RATIO tag in the result metadata will also be 1.0.</p>\n <p>When the application requests a physical stream for a logical multi-camera, the\n ACAMERA_CONTROL_ZOOM_RATIO in the physical camera result metadata will be 1.0, and\n the ACAMERA_SCALER_CROP_REGION tag reflects the amount of zoom and crop done by the\n physical camera device.</p>\n\n @see ACAMERA_CONTROL_AE_REGIONS\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_SCALER_CROP_REGION"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_ZOOM_RATIO: acamera_metadata_tag = 65583;
#[doc = " <p>The desired zoom ratio</p>\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Instead of using ACAMERA_SCALER_CROP_REGION for zoom, the application can now choose to\n use this tag to specify the desired zoom level.</p>\n <p>By using this control, the application gains a simpler way to control zoom, which can\n be a combination of optical and digital zoom. For example, a multi-camera system may\n contain more than one lens with different focal lengths, and the user can use optical\n zoom by switching between lenses. Using zoomRatio has benefits in the scenarios below:</p>\n <ul>\n <li>Zooming in from a wide-angle lens to a telephoto lens: A floating-point ratio provides\n   better precision compared to an integer value of ACAMERA_SCALER_CROP_REGION.</li>\n <li>Zooming out from a wide lens to an ultrawide lens: zoomRatio supports zoom-out whereas\n   ACAMERA_SCALER_CROP_REGION doesn't.</li>\n </ul>\n <p>To illustrate, here are several scenarios of different zoom ratios, crop regions,\n and output streams, for a hypothetical camera device with an active array of size\n <code>(2000,1500)</code>.</p>\n <ul>\n <li>Camera Configuration:<ul>\n <li>Active array size: <code>2000x1500</code> (3 MP, 4:3 aspect ratio)</li>\n <li>Output stream #1: <code>640x480</code> (VGA, 4:3 aspect ratio)</li>\n <li>Output stream #2: <code>1280x720</code> (720p, 16:9 aspect ratio)</li>\n </ul>\n </li>\n <li>Case #1: 4:3 crop region with 2.0x zoom ratio<ul>\n <li>Zoomed field of view: 1/4 of original field of view</li>\n <li>Crop region: <code>Rect(0, 0, 2000, 1500) // (left, top, right, bottom)</code> (post zoom)</li>\n </ul>\n </li>\n <li><img alt=\"4:3 aspect ratio crop diagram\" src=\"../images/camera2/metadata/android.control.zoomRatio/zoom-ratio-2-crop-43.png\" /><ul>\n <li><code>640x480</code> stream source area: <code>(0, 0, 2000, 1500)</code> (equal to crop region)</li>\n <li><code>1280x720</code> stream source area: <code>(0, 187, 2000, 1312)</code> (letterboxed)</li>\n </ul>\n </li>\n <li>Case #2: 16:9 crop region with 2.0x zoom.<ul>\n <li>Zoomed field of view: 1/4 of original field of view</li>\n <li>Crop region: <code>Rect(0, 187, 2000, 1312)</code></li>\n <li><img alt=\"16:9 aspect ratio crop diagram\" src=\"../images/camera2/metadata/android.control.zoomRatio/zoom-ratio-2-crop-169.png\" /></li>\n <li><code>640x480</code> stream source area: <code>(250, 187, 1750, 1312)</code> (pillarboxed)</li>\n <li><code>1280x720</code> stream source area: <code>(0, 187, 2000, 1312)</code> (equal to crop region)</li>\n </ul>\n </li>\n <li>Case #3: 1:1 crop region with 0.5x zoom out to ultrawide lens.<ul>\n <li>Zoomed field of view: 4x of original field of view (switched from wide lens to ultrawide lens)</li>\n <li>Crop region: <code>Rect(250, 0, 1750, 1500)</code></li>\n <li><img alt=\"1:1 aspect ratio crop diagram\" src=\"../images/camera2/metadata/android.control.zoomRatio/zoom-ratio-0.5-crop-11.png\" /></li>\n <li><code>640x480</code> stream source area: <code>(250, 187, 1750, 1312)</code> (letterboxed)</li>\n <li><code>1280x720</code> stream source area: <code>(250, 328, 1750, 1172)</code> (letterboxed)</li>\n </ul>\n </li>\n </ul>\n <p>As seen from the graphs above, the coordinate system of cropRegion now changes to the\n effective after-zoom field-of-view, and is represented by the rectangle of (0, 0,\n activeArrayWith, activeArrayHeight). The same applies to AE/AWB/AF regions, and faces.\n This coordinate system change isn't applicable to RAW capture and its related\n metadata such as intrinsicCalibration and lensShadingMap.</p>\n <p>Using the same hypothetical example above, and assuming output stream #1 (640x480) is\n the viewfinder stream, the application can achieve 2.0x zoom in one of two ways:</p>\n <ul>\n <li>zoomRatio = 2.0, scaler.cropRegion = (0, 0, 2000, 1500)</li>\n <li>zoomRatio = 1.0 (default), scaler.cropRegion = (500, 375, 1500, 1125)</li>\n </ul>\n <p>If the application intends to set aeRegions to be top-left quarter of the viewfinder\n field-of-view, the ACAMERA_CONTROL_AE_REGIONS should be set to (0, 0, 1000, 750) with\n zoomRatio set to 2.0. Alternatively, the application can set aeRegions to the equivalent\n region of (500, 375, 1000, 750) for zoomRatio of 1.0. If the application doesn't\n explicitly set ACAMERA_CONTROL_ZOOM_RATIO, its value defaults to 1.0.</p>\n <p>One limitation of controlling zoom using zoomRatio is that the ACAMERA_SCALER_CROP_REGION\n must only be used for letterboxing or pillarboxing of the sensor active array, and no\n FREEFORM cropping can be used with ACAMERA_CONTROL_ZOOM_RATIO other than 1.0. If\n ACAMERA_CONTROL_ZOOM_RATIO is not 1.0, and ACAMERA_SCALER_CROP_REGION is set to be\n windowboxing, the camera framework will override the ACAMERA_SCALER_CROP_REGION to be\n the active array.</p>\n <p>In the capture request, if the application sets ACAMERA_CONTROL_ZOOM_RATIO to a\n value != 1.0, the ACAMERA_CONTROL_ZOOM_RATIO tag in the capture result reflects the\n effective zoom ratio achieved by the camera device, and the ACAMERA_SCALER_CROP_REGION\n adjusts for additional crops that are not zoom related. Otherwise, if the application\n sets ACAMERA_CONTROL_ZOOM_RATIO to 1.0, or does not set it at all, the\n ACAMERA_CONTROL_ZOOM_RATIO tag in the result metadata will also be 1.0.</p>\n <p>When the application requests a physical stream for a logical multi-camera, the\n ACAMERA_CONTROL_ZOOM_RATIO in the physical camera result metadata will be 1.0, and\n the ACAMERA_SCALER_CROP_REGION tag reflects the amount of zoom and crop done by the\n physical camera device.</p>\n\n @see ACAMERA_CONTROL_AE_REGIONS\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_SCALER_CROP_REGION"]
pub const acamera_metadata_tag_ACAMERA_CONTROL_END: acamera_metadata_tag = 65584;
#[doc = " <p>Operation mode for edge\n enhancement.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_edge_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Edge enhancement improves sharpness and details in the captured image. OFF means\n no enhancement will be applied by the camera device.</p>\n <p>FAST/HIGH_QUALITY both mean camera device determined enhancement\n will be applied. HIGH_QUALITY mode indicates that the\n camera device will use the highest-quality enhancement algorithms,\n even if it slows down capture rate. FAST means the camera device will\n not slow down capture rate when applying edge enhancement. FAST may be the same as OFF if\n edge enhancement will slow down capture rate. Every output stream will have a similar\n amount of enhancement applied.</p>\n <p>ZERO_SHUTTER_LAG is meant to be used by applications that maintain a continuous circular\n buffer of high-resolution images during preview and reprocess image(s) from that buffer\n into a final capture when triggered by the user. In this mode, the camera device applies\n edge enhancement to low-resolution streams (below maximum recording resolution) to\n maximize preview quality, but does not apply edge enhancement to high-resolution streams,\n since those will be reprocessed later if necessary.</p>\n <p>For YUV_REPROCESSING, these FAST/HIGH_QUALITY modes both mean that the camera\n device will apply FAST/HIGH_QUALITY YUV-domain edge enhancement, respectively.\n The camera device may adjust its internal edge enhancement parameters for best\n image quality based on the android.reprocess.effectiveExposureFactor, if it is set.</p>"]
pub const acamera_metadata_tag_ACAMERA_EDGE_MODE: acamera_metadata_tag = 196608;
#[doc = " <p>List of edge enhancement modes for ACAMERA_EDGE_MODE that are supported by this camera\n device.</p>\n\n @see ACAMERA_EDGE_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Full-capability camera devices must always support OFF; camera devices that support\n YUV_REPROCESSING or PRIVATE_REPROCESSING will list ZERO_SHUTTER_LAG; all devices will\n list FAST.</p>"]
pub const acamera_metadata_tag_ACAMERA_EDGE_AVAILABLE_EDGE_MODES: acamera_metadata_tag = 196610;
#[doc = " <p>List of edge enhancement modes for ACAMERA_EDGE_MODE that are supported by this camera\n device.</p>\n\n @see ACAMERA_EDGE_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Full-capability camera devices must always support OFF; camera devices that support\n YUV_REPROCESSING or PRIVATE_REPROCESSING will list ZERO_SHUTTER_LAG; all devices will\n list FAST.</p>"]
pub const acamera_metadata_tag_ACAMERA_EDGE_END: acamera_metadata_tag = 196611;
#[doc = " <p>The desired mode for for the camera device's flash control.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_flash_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This control is only effective when flash unit is available\n (<code>ACAMERA_FLASH_INFO_AVAILABLE == true</code>).</p>\n <p>When this control is used, the ACAMERA_CONTROL_AE_MODE must be set to ON or OFF.\n Otherwise, the camera device auto-exposure related flash control (ON_AUTO_FLASH,\n ON_ALWAYS_FLASH, or ON_AUTO_FLASH_REDEYE) will override this control.</p>\n <p>When set to OFF, the camera device will not fire flash for this capture.</p>\n <p>When set to SINGLE, the camera device will fire flash regardless of the camera\n device's auto-exposure routine's result. When used in still capture case, this\n control should be used along with auto-exposure (AE) precapture metering sequence\n (ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER), otherwise, the image may be incorrectly exposed.</p>\n <p>When set to TORCH, the flash will be on continuously. This mode can be used\n for use cases such as preview, auto-focus assist, still capture, or video recording.</p>\n <p>The flash status will be reported by ACAMERA_FLASH_STATE in the capture result metadata.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER\n @see ACAMERA_FLASH_INFO_AVAILABLE\n @see ACAMERA_FLASH_STATE"]
pub const acamera_metadata_tag_ACAMERA_FLASH_MODE: acamera_metadata_tag = 262146;
#[doc = " <p>Current state of the flash\n unit.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_flash_state_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>When the camera device doesn't have flash unit\n (i.e. <code>ACAMERA_FLASH_INFO_AVAILABLE == false</code>), this state will always be UNAVAILABLE.\n Other states indicate the current flash status.</p>\n <p>In certain conditions, this will be available on LEGACY devices:</p>\n <ul>\n <li>Flash-less cameras always return UNAVAILABLE.</li>\n <li>Using ACAMERA_CONTROL_AE_MODE <code>==</code> ON_ALWAYS_FLASH\n    will always return FIRED.</li>\n <li>Using ACAMERA_FLASH_MODE <code>==</code> TORCH\n    will always return FIRED.</li>\n </ul>\n <p>In all other conditions the state will not be available on\n LEGACY devices (i.e. it will be <code>null</code>).</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_FLASH_INFO_AVAILABLE\n @see ACAMERA_FLASH_MODE"]
pub const acamera_metadata_tag_ACAMERA_FLASH_STATE: acamera_metadata_tag = 262149;
#[doc = " <p>Current state of the flash\n unit.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_flash_state_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>When the camera device doesn't have flash unit\n (i.e. <code>ACAMERA_FLASH_INFO_AVAILABLE == false</code>), this state will always be UNAVAILABLE.\n Other states indicate the current flash status.</p>\n <p>In certain conditions, this will be available on LEGACY devices:</p>\n <ul>\n <li>Flash-less cameras always return UNAVAILABLE.</li>\n <li>Using ACAMERA_CONTROL_AE_MODE <code>==</code> ON_ALWAYS_FLASH\n    will always return FIRED.</li>\n <li>Using ACAMERA_FLASH_MODE <code>==</code> TORCH\n    will always return FIRED.</li>\n </ul>\n <p>In all other conditions the state will not be available on\n LEGACY devices (i.e. it will be <code>null</code>).</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_FLASH_INFO_AVAILABLE\n @see ACAMERA_FLASH_MODE"]
pub const acamera_metadata_tag_ACAMERA_FLASH_END: acamera_metadata_tag = 262150;
#[doc = " <p>Whether this camera device has a\n flash unit.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_flash_info_available_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Will be <code>false</code> if no flash is available.</p>\n <p>If there is no flash unit, none of the flash controls do\n anything.</p>"]
pub const acamera_metadata_tag_ACAMERA_FLASH_INFO_AVAILABLE: acamera_metadata_tag = 327680;
#[doc = " <p>Maximum flashlight brightness level.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If this value is greater than 1, then the device supports controlling the\n flashlight brightness level via\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraManager.html#turnOnTorchWithStrengthLevel\">CameraManager#turnOnTorchWithStrengthLevel</a>.\n If this value is equal to 1, flashlight brightness control is not supported.\n The value for this key will be null for devices with no flash unit.</p>\n <p>The maximum value is guaranteed to be safe to use for an indefinite duration in\n terms of device flashlight lifespan, but may be too bright for comfort for many\n use cases. Use the default torch brightness value to avoid problems with an\n over-bright flashlight.</p>"]
pub const acamera_metadata_tag_ACAMERA_FLASH_INFO_STRENGTH_MAXIMUM_LEVEL: acamera_metadata_tag =
    327682;
#[doc = " <p>Default flashlight brightness level to be set via\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraManager.html#turnOnTorchWithStrengthLevel\">CameraManager#turnOnTorchWithStrengthLevel</a>.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If flash unit is available this will be greater than or equal to 1 and less\n or equal to <code>ACAMERA_FLASH_INFO_STRENGTH_MAXIMUM_LEVEL</code>.</p>\n <p>Setting flashlight brightness above the default level\n (i.e.<code>ACAMERA_FLASH_INFO_STRENGTH_DEFAULT_LEVEL</code>) may make the device more\n likely to reach thermal throttling conditions and slow down, or drain the\n battery quicker than normal. To minimize such issues, it is recommended to\n start the flashlight at this default brightness until a user explicitly requests\n a brighter level.\n Note that the value for this key will be null for devices with no flash unit.\n The default level should always be &gt; 0.</p>\n\n @see ACAMERA_FLASH_INFO_STRENGTH_DEFAULT_LEVEL\n @see ACAMERA_FLASH_INFO_STRENGTH_MAXIMUM_LEVEL"]
pub const acamera_metadata_tag_ACAMERA_FLASH_INFO_STRENGTH_DEFAULT_LEVEL: acamera_metadata_tag =
    327683;
#[doc = " <p>Default flashlight brightness level to be set via\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraManager.html#turnOnTorchWithStrengthLevel\">CameraManager#turnOnTorchWithStrengthLevel</a>.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If flash unit is available this will be greater than or equal to 1 and less\n or equal to <code>ACAMERA_FLASH_INFO_STRENGTH_MAXIMUM_LEVEL</code>.</p>\n <p>Setting flashlight brightness above the default level\n (i.e.<code>ACAMERA_FLASH_INFO_STRENGTH_DEFAULT_LEVEL</code>) may make the device more\n likely to reach thermal throttling conditions and slow down, or drain the\n battery quicker than normal. To minimize such issues, it is recommended to\n start the flashlight at this default brightness until a user explicitly requests\n a brighter level.\n Note that the value for this key will be null for devices with no flash unit.\n The default level should always be &gt; 0.</p>\n\n @see ACAMERA_FLASH_INFO_STRENGTH_DEFAULT_LEVEL\n @see ACAMERA_FLASH_INFO_STRENGTH_MAXIMUM_LEVEL"]
pub const acamera_metadata_tag_ACAMERA_FLASH_INFO_END: acamera_metadata_tag = 327684;
#[doc = " <p>Operational mode for hot pixel correction.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_hot_pixel_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Hotpixel correction interpolates out, or otherwise removes, pixels\n that do not accurately measure the incoming light (i.e. pixels that\n are stuck at an arbitrary value or are oversensitive).</p>"]
pub const acamera_metadata_tag_ACAMERA_HOT_PIXEL_MODE: acamera_metadata_tag = 393216;
#[doc = " <p>List of hot pixel correction modes for ACAMERA_HOT_PIXEL_MODE that are supported by this\n camera device.</p>\n\n @see ACAMERA_HOT_PIXEL_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>FULL mode camera devices will always support FAST.</p>"]
pub const acamera_metadata_tag_ACAMERA_HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES: acamera_metadata_tag =
    393217;
#[doc = " <p>List of hot pixel correction modes for ACAMERA_HOT_PIXEL_MODE that are supported by this\n camera device.</p>\n\n @see ACAMERA_HOT_PIXEL_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>FULL mode camera devices will always support FAST.</p>"]
pub const acamera_metadata_tag_ACAMERA_HOT_PIXEL_END: acamera_metadata_tag = 393218;
#[doc = " <p>GPS coordinates to include in output JPEG\n EXIF.</p>\n\n <p>Type: double[3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This tag is also used for HEIC image capture.</p>"]
pub const acamera_metadata_tag_ACAMERA_JPEG_GPS_COORDINATES: acamera_metadata_tag = 458752;
#[doc = " <p>32 characters describing GPS algorithm to\n include in EXIF.</p>\n\n <p>Type: byte</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This tag is also used for HEIC image capture.</p>"]
pub const acamera_metadata_tag_ACAMERA_JPEG_GPS_PROCESSING_METHOD: acamera_metadata_tag = 458753;
#[doc = " <p>Time GPS fix was made to include in\n EXIF.</p>\n\n <p>Type: int64</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This tag is also used for HEIC image capture.</p>"]
pub const acamera_metadata_tag_ACAMERA_JPEG_GPS_TIMESTAMP: acamera_metadata_tag = 458754;
#[doc = " <p>The orientation for a JPEG image.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>The clockwise rotation angle in degrees, relative to the orientation\n to the camera, that the JPEG picture needs to be rotated by, to be viewed\n upright.</p>\n <p>Camera devices may either encode this value into the JPEG EXIF header, or\n rotate the image data to match this orientation. When the image data is rotated,\n the thumbnail data will also be rotated.</p>\n <p>Note that this orientation is relative to the orientation of the camera sensor, given\n by ACAMERA_SENSOR_ORIENTATION.</p>\n <p>To translate from the device orientation given by the Android sensor APIs for camera\n sensors which are not EXTERNAL, the following sample code may be used:</p>\n <pre><code>private int getJpegOrientation(CameraCharacteristics c, int deviceOrientation) {\n     if (deviceOrientation == android.view.OrientationEventListener.ORIENTATION_UNKNOWN) return 0;\n     int sensorOrientation = c.get(CameraCharacteristics.SENSOR_ORIENTATION);\n\n     // Round device orientation to a multiple of 90\n     deviceOrientation = (deviceOrientation + 45) / 90 * 90;\n\n     // Reverse device orientation for front-facing cameras\n     boolean facingFront = c.get(CameraCharacteristics.LENS_FACING) == CameraCharacteristics.LENS_FACING_FRONT;\n     if (facingFront) deviceOrientation = -deviceOrientation;\n\n     // Calculate desired JPEG orientation relative to camera orientation to make\n     // the image upright relative to the device orientation\n     int jpegOrientation = (sensorOrientation + deviceOrientation + 360) % 360;\n\n     return jpegOrientation;\n }\n </code></pre>\n <p>For EXTERNAL cameras the sensor orientation will always be set to 0 and the facing will\n also be set to EXTERNAL. The above code is not relevant in such case.</p>\n <p>This tag is also used to describe the orientation of the HEIC image capture, in which\n case the rotation is reflected by\n <a href=\"https://developer.android.com/reference/android/media/ExifInterface.html#TAG_ORIENTATION\">EXIF orientation flag</a>, and not by\n rotating the image data itself.</p>\n\n @see ACAMERA_SENSOR_ORIENTATION"]
pub const acamera_metadata_tag_ACAMERA_JPEG_ORIENTATION: acamera_metadata_tag = 458755;
#[doc = " <p>Compression quality of the final JPEG\n image.</p>\n\n <p>Type: byte</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>85-95 is typical usage range. This tag is also used to describe the quality\n of the HEIC image capture.</p>"]
pub const acamera_metadata_tag_ACAMERA_JPEG_QUALITY: acamera_metadata_tag = 458756;
#[doc = " <p>Compression quality of JPEG\n thumbnail.</p>\n\n <p>Type: byte</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This tag is also used to describe the quality of the HEIC image capture.</p>"]
pub const acamera_metadata_tag_ACAMERA_JPEG_THUMBNAIL_QUALITY: acamera_metadata_tag = 458757;
#[doc = " <p>Resolution of embedded JPEG thumbnail.</p>\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>When set to (0, 0) value, the JPEG EXIF will not contain thumbnail,\n but the captured JPEG will still be a valid image.</p>\n <p>For best results, when issuing a request for a JPEG image, the thumbnail size selected\n should have the same aspect ratio as the main JPEG output.</p>\n <p>If the thumbnail image aspect ratio differs from the JPEG primary image aspect\n ratio, the camera device creates the thumbnail by cropping it from the primary image.\n For example, if the primary image has 4:3 aspect ratio, the thumbnail image has\n 16:9 aspect ratio, the primary image will be cropped vertically (letterbox) to\n generate the thumbnail image. The thumbnail image will always have a smaller Field\n Of View (FOV) than the primary image when aspect ratios differ.</p>\n <p>When an ACAMERA_JPEG_ORIENTATION of non-zero degree is requested,\n the camera device will handle thumbnail rotation in one of the following ways:</p>\n <ul>\n <li>Set the <a href=\"https://developer.android.com/reference/android/media/ExifInterface.html#TAG_ORIENTATION\">EXIF orientation flag</a>\n   and keep jpeg and thumbnail image data unrotated.</li>\n <li>Rotate the jpeg and thumbnail image data and not set\n   <a href=\"https://developer.android.com/reference/android/media/ExifInterface.html#TAG_ORIENTATION\">EXIF orientation flag</a>. In this\n   case, LIMITED or FULL hardware level devices will report rotated thumbnail size in\n   capture result, so the width and height will be interchanged if 90 or 270 degree\n   orientation is requested. LEGACY device will always report unrotated thumbnail\n   size.</li>\n </ul>\n <p>The tag is also used as thumbnail size for HEIC image format capture, in which case the\n the thumbnail rotation is reflected by\n <a href=\"https://developer.android.com/reference/android/media/ExifInterface.html#TAG_ORIENTATION\">EXIF orientation flag</a>, and not by\n rotating the thumbnail data itself.</p>\n\n @see ACAMERA_JPEG_ORIENTATION"]
pub const acamera_metadata_tag_ACAMERA_JPEG_THUMBNAIL_SIZE: acamera_metadata_tag = 458758;
#[doc = " <p>List of JPEG thumbnail sizes for ACAMERA_JPEG_THUMBNAIL_SIZE supported by this\n camera device.</p>\n\n @see ACAMERA_JPEG_THUMBNAIL_SIZE\n\n <p>Type: int32[2*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This list will include at least one non-zero resolution, plus <code>(0,0)</code> for indicating no\n thumbnail should be generated.</p>\n <p>Below conditions will be satisfied for this size list:</p>\n <ul>\n <li>The sizes will be sorted by increasing pixel area (width x height).\n If several resolutions have the same area, they will be sorted by increasing width.</li>\n <li>The aspect ratio of the largest thumbnail size will be same as the\n aspect ratio of largest JPEG output size in ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS.\n The largest size is defined as the size that has the largest pixel area\n in a given size list.</li>\n <li>Each output JPEG size in ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS will have at least\n one corresponding size that has the same aspect ratio in availableThumbnailSizes,\n and vice versa.</li>\n <li>All non-<code>(0, 0)</code> sizes will have non-zero widths and heights.</li>\n </ul>\n <p>This list is also used as supported thumbnail sizes for HEIC image format capture.</p>\n\n @see ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS"]
pub const acamera_metadata_tag_ACAMERA_JPEG_AVAILABLE_THUMBNAIL_SIZES: acamera_metadata_tag =
    458759;
#[doc = " <p>List of JPEG thumbnail sizes for ACAMERA_JPEG_THUMBNAIL_SIZE supported by this\n camera device.</p>\n\n @see ACAMERA_JPEG_THUMBNAIL_SIZE\n\n <p>Type: int32[2*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This list will include at least one non-zero resolution, plus <code>(0,0)</code> for indicating no\n thumbnail should be generated.</p>\n <p>Below conditions will be satisfied for this size list:</p>\n <ul>\n <li>The sizes will be sorted by increasing pixel area (width x height).\n If several resolutions have the same area, they will be sorted by increasing width.</li>\n <li>The aspect ratio of the largest thumbnail size will be same as the\n aspect ratio of largest JPEG output size in ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS.\n The largest size is defined as the size that has the largest pixel area\n in a given size list.</li>\n <li>Each output JPEG size in ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS will have at least\n one corresponding size that has the same aspect ratio in availableThumbnailSizes,\n and vice versa.</li>\n <li>All non-<code>(0, 0)</code> sizes will have non-zero widths and heights.</li>\n </ul>\n <p>This list is also used as supported thumbnail sizes for HEIC image format capture.</p>\n\n @see ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS"]
pub const acamera_metadata_tag_ACAMERA_JPEG_END: acamera_metadata_tag = 458760;
#[doc = " <p>The desired lens aperture size, as a ratio of lens focal length to the\n effective aperture diameter.</p>\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Setting this value is only supported on the camera devices that have a variable\n aperture lens.</p>\n <p>When this is supported and ACAMERA_CONTROL_AE_MODE is OFF,\n this can be set along with ACAMERA_SENSOR_EXPOSURE_TIME,\n ACAMERA_SENSOR_SENSITIVITY, and ACAMERA_SENSOR_FRAME_DURATION\n to achieve manual exposure control.</p>\n <p>The requested aperture value may take several frames to reach the\n requested value; the camera device will report the current (intermediate)\n aperture size in capture result metadata while the aperture is changing.\n While the aperture is still changing, ACAMERA_LENS_STATE will be set to MOVING.</p>\n <p>When this is supported and ACAMERA_CONTROL_AE_MODE is one of\n the ON modes, this will be overridden by the camera device\n auto-exposure algorithm, the overridden values are then provided\n back to the user in the corresponding result.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_LENS_STATE\n @see ACAMERA_SENSOR_EXPOSURE_TIME\n @see ACAMERA_SENSOR_FRAME_DURATION\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_tag_ACAMERA_LENS_APERTURE: acamera_metadata_tag = 524288;
#[doc = " <p>The desired setting for the lens neutral density filter(s).</p>\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This control will not be supported on most camera devices.</p>\n <p>Lens filters are typically used to lower the amount of light the\n sensor is exposed to (measured in steps of EV). As used here, an EV\n step is the standard logarithmic representation, which are\n non-negative, and inversely proportional to the amount of light\n hitting the sensor.  For example, setting this to 0 would result\n in no reduction of the incoming light, and setting this to 2 would\n mean that the filter is set to reduce incoming light by two stops\n (allowing 1/4 of the prior amount of light to the sensor).</p>\n <p>It may take several frames before the lens filter density changes\n to the requested value. While the filter density is still changing,\n ACAMERA_LENS_STATE will be set to MOVING.</p>\n\n @see ACAMERA_LENS_STATE"]
pub const acamera_metadata_tag_ACAMERA_LENS_FILTER_DENSITY: acamera_metadata_tag = 524289;
#[doc = " <p>The desired lens focal length; used for optical zoom.</p>\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This setting controls the physical focal length of the camera\n device's lens. Changing the focal length changes the field of\n view of the camera device, and is usually used for optical zoom.</p>\n <p>Like ACAMERA_LENS_FOCUS_DISTANCE and ACAMERA_LENS_APERTURE, this\n setting won't be applied instantaneously, and it may take several\n frames before the lens can change to the requested focal length.\n While the focal length is still changing, ACAMERA_LENS_STATE will\n be set to MOVING.</p>\n <p>Optical zoom via this control will not be supported on most devices. Starting from API\n level 30, the camera device may combine optical and digital zoom through the\n ACAMERA_CONTROL_ZOOM_RATIO control.</p>\n\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_LENS_APERTURE\n @see ACAMERA_LENS_FOCUS_DISTANCE\n @see ACAMERA_LENS_STATE"]
pub const acamera_metadata_tag_ACAMERA_LENS_FOCAL_LENGTH: acamera_metadata_tag = 524290;
#[doc = " <p>Desired distance to plane of sharpest focus,\n measured from frontmost surface of the lens.</p>\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Should be zero for fixed-focus cameras</p>"]
pub const acamera_metadata_tag_ACAMERA_LENS_FOCUS_DISTANCE: acamera_metadata_tag = 524291;
#[doc = " <p>Sets whether the camera device uses optical image stabilization (OIS)\n when capturing images.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_lens_optical_stabilization_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>OIS is used to compensate for motion blur due to small\n movements of the camera during capture. Unlike digital image\n stabilization (ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE), OIS\n makes use of mechanical elements to stabilize the camera\n sensor, and thus allows for longer exposure times before\n camera shake becomes apparent.</p>\n <p>Switching between different optical stabilization modes may take several\n frames to initialize, the camera device will report the current mode in\n capture result metadata. For example, When \"ON\" mode is requested, the\n optical stabilization modes in the first several capture results may still\n be \"OFF\", and it will become \"ON\" when the initialization is done.</p>\n <p>If a camera device supports both OIS and digital image stabilization\n (ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE), turning both modes on may produce undesirable\n interaction, so it is recommended not to enable both at the same time.</p>\n <p>If ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE is set to \"PREVIEW_STABILIZATION\",\n ACAMERA_LENS_OPTICAL_STABILIZATION_MODE is overridden. The camera sub-system may choose\n to turn on hardware based image stabilization in addition to software based stabilization\n if it deems that appropriate. This key's value in the capture result will reflect which\n OIS mode was chosen.</p>\n <p>Not all devices will support OIS; see\n ACAMERA_LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION for\n available controls.</p>\n\n @see ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE\n @see ACAMERA_LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION\n @see ACAMERA_LENS_OPTICAL_STABILIZATION_MODE"]
pub const acamera_metadata_tag_ACAMERA_LENS_OPTICAL_STABILIZATION_MODE: acamera_metadata_tag =
    524292;
#[doc = " <p>Direction the camera faces relative to\n device screen.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_lens_facing_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n"]
pub const acamera_metadata_tag_ACAMERA_LENS_FACING: acamera_metadata_tag = 524293;
#[doc = " <p>The orientation of the camera relative to the sensor\n coordinate system.</p>\n\n <p>Type: float[4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The four coefficients that describe the quaternion\n rotation from the Android sensor coordinate system to a\n camera-aligned coordinate system where the X-axis is\n aligned with the long side of the image sensor, the Y-axis\n is aligned with the short side of the image sensor, and\n the Z-axis is aligned with the optical axis of the sensor.</p>\n <p>To convert from the quaternion coefficients <code>(x,y,z,w)</code>\n to the axis of rotation <code>(a_x, a_y, a_z)</code> and rotation\n amount <code>theta</code>, the following formulas can be used:</p>\n <pre><code> theta = 2 * acos(w)\n a_x = x / sin(theta/2)\n a_y = y / sin(theta/2)\n a_z = z / sin(theta/2)\n </code></pre>\n <p>To create a 3x3 rotation matrix that applies the rotation\n defined by this quaternion, the following matrix can be\n used:</p>\n <pre><code>R = [ 1 - 2y^2 - 2z^2,       2xy - 2zw,       2xz + 2yw,\n            2xy + 2zw, 1 - 2x^2 - 2z^2,       2yz - 2xw,\n            2xz - 2yw,       2yz + 2xw, 1 - 2x^2 - 2y^2 ]\n </code></pre>\n <p>This matrix can then be used to apply the rotation to a\n  column vector point with</p>\n <p><code>p' = Rp</code></p>\n <p>where <code>p</code> is in the device sensor coordinate system, and\n  <code>p'</code> is in the camera-oriented coordinate system.</p>\n <p>If ACAMERA_LENS_POSE_REFERENCE is UNDEFINED, the quaternion rotation cannot\n  be accurately represented by the camera device, and will be represented by\n  default values matching its default facing.</p>\n\n @see ACAMERA_LENS_POSE_REFERENCE"]
pub const acamera_metadata_tag_ACAMERA_LENS_POSE_ROTATION: acamera_metadata_tag = 524294;
#[doc = " <p>Position of the camera optical center.</p>\n\n <p>Type: float[3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The position of the camera device's lens optical center,\n as a three-dimensional vector <code>(x,y,z)</code>.</p>\n <p>Prior to Android P, or when ACAMERA_LENS_POSE_REFERENCE is PRIMARY_CAMERA, this position\n is relative to the optical center of the largest camera device facing in the same\n direction as this camera, in the <a href=\"https://developer.android.com/reference/android/hardware/SensorEvent.html\">Android sensor\n coordinate axes</a>. Note that only the axis definitions are shared with the sensor\n coordinate system, but not the origin.</p>\n <p>If this device is the largest or only camera device with a given facing, then this\n position will be <code>(0, 0, 0)</code>; a camera device with a lens optical center located 3 cm\n from the main sensor along the +X axis (to the right from the user's perspective) will\n report <code>(0.03, 0, 0)</code>.  Note that this means that, for many computer vision\n applications, the position needs to be negated to convert it to a translation from the\n camera to the origin.</p>\n <p>To transform a pixel coordinates between two cameras facing the same direction, first\n the source camera ACAMERA_LENS_DISTORTION must be corrected for.  Then the source\n camera ACAMERA_LENS_INTRINSIC_CALIBRATION needs to be applied, followed by the\n ACAMERA_LENS_POSE_ROTATION of the source camera, the translation of the source camera\n relative to the destination camera, the ACAMERA_LENS_POSE_ROTATION of the destination\n camera, and finally the inverse of ACAMERA_LENS_INTRINSIC_CALIBRATION of the destination\n camera. This obtains a radial-distortion-free coordinate in the destination camera pixel\n coordinates.</p>\n <p>To compare this against a real image from the destination camera, the destination camera\n image then needs to be corrected for radial distortion before comparison or sampling.</p>\n <p>When ACAMERA_LENS_POSE_REFERENCE is GYROSCOPE, then this position is relative to\n the center of the primary gyroscope on the device. The axis definitions are the same as\n with PRIMARY_CAMERA.</p>\n <p>When ACAMERA_LENS_POSE_REFERENCE is UNDEFINED, this position cannot be accurately\n represented by the camera device, and will be represented as <code>(0, 0, 0)</code>.</p>\n <p>When ACAMERA_LENS_POSE_REFERENCE is AUTOMOTIVE, then this position is relative to the\n origin of the automotive sensor coordinate system, which is at the center of the rear\n axle.</p>\n\n @see ACAMERA_LENS_DISTORTION\n @see ACAMERA_LENS_INTRINSIC_CALIBRATION\n @see ACAMERA_LENS_POSE_REFERENCE\n @see ACAMERA_LENS_POSE_ROTATION"]
pub const acamera_metadata_tag_ACAMERA_LENS_POSE_TRANSLATION: acamera_metadata_tag = 524295;
#[doc = " <p>The range of scene distances that are in\n sharp focus (depth of field).</p>\n\n <p>Type: float[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>If variable focus not supported, can still report\n fixed depth of field range</p>"]
pub const acamera_metadata_tag_ACAMERA_LENS_FOCUS_RANGE: acamera_metadata_tag = 524296;
#[doc = " <p>Current lens status.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_lens_state_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>For lens parameters ACAMERA_LENS_FOCAL_LENGTH, ACAMERA_LENS_FOCUS_DISTANCE,\n ACAMERA_LENS_FILTER_DENSITY and ACAMERA_LENS_APERTURE, when changes are requested,\n they may take several frames to reach the requested values. This state indicates\n the current status of the lens parameters.</p>\n <p>When the state is STATIONARY, the lens parameters are not changing. This could be\n either because the parameters are all fixed, or because the lens has had enough\n time to reach the most recently-requested values.\n If all these lens parameters are not changeable for a camera device, as listed below:</p>\n <ul>\n <li>Fixed focus (<code>ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE == 0</code>), which means\n ACAMERA_LENS_FOCUS_DISTANCE parameter will always be 0.</li>\n <li>Fixed focal length (ACAMERA_LENS_INFO_AVAILABLE_FOCAL_LENGTHS contains single value),\n which means the optical zoom is not supported.</li>\n <li>No ND filter (ACAMERA_LENS_INFO_AVAILABLE_FILTER_DENSITIES contains only 0).</li>\n <li>Fixed aperture (ACAMERA_LENS_INFO_AVAILABLE_APERTURES contains single value).</li>\n </ul>\n <p>Then this state will always be STATIONARY.</p>\n <p>When the state is MOVING, it indicates that at least one of the lens parameters\n is changing.</p>\n\n @see ACAMERA_LENS_APERTURE\n @see ACAMERA_LENS_FILTER_DENSITY\n @see ACAMERA_LENS_FOCAL_LENGTH\n @see ACAMERA_LENS_FOCUS_DISTANCE\n @see ACAMERA_LENS_INFO_AVAILABLE_APERTURES\n @see ACAMERA_LENS_INFO_AVAILABLE_FILTER_DENSITIES\n @see ACAMERA_LENS_INFO_AVAILABLE_FOCAL_LENGTHS\n @see ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE"]
pub const acamera_metadata_tag_ACAMERA_LENS_STATE: acamera_metadata_tag = 524297;
#[doc = " <p>The parameters for this camera device's intrinsic\n calibration.</p>\n\n <p>Type: float[5]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The five calibration parameters that describe the\n transform from camera-centric 3D coordinates to sensor\n pixel coordinates:</p>\n <pre><code>[f_x, f_y, c_x, c_y, s]\n </code></pre>\n <p>Where <code>f_x</code> and <code>f_y</code> are the horizontal and vertical\n focal lengths, <code>[c_x, c_y]</code> is the position of the optical\n axis, and <code>s</code> is a skew parameter for the sensor plane not\n being aligned with the lens plane.</p>\n <p>These are typically used within a transformation matrix K:</p>\n <pre><code>K = [ f_x,   s, c_x,\n        0, f_y, c_y,\n        0    0,   1 ]\n </code></pre>\n <p>which can then be combined with the camera pose rotation\n <code>R</code> and translation <code>t</code> (ACAMERA_LENS_POSE_ROTATION and\n ACAMERA_LENS_POSE_TRANSLATION, respectively) to calculate the\n complete transform from world coordinates to pixel\n coordinates:</p>\n <pre><code>P = [ K 0   * [ R -Rt\n      0 1 ]      0 1 ]\n </code></pre>\n <p>(Note the negation of poseTranslation when mapping from camera\n to world coordinates, and multiplication by the rotation).</p>\n <p>With <code>p_w</code> being a point in the world coordinate system\n and <code>p_s</code> being a point in the camera active pixel array\n coordinate system, and with the mapping including the\n homogeneous division by z:</p>\n <pre><code> p_h = (x_h, y_h, z_h) = P p_w\n p_s = p_h / z_h\n </code></pre>\n <p>so <code>[x_s, y_s]</code> is the pixel coordinates of the world\n point, <code>z_s = 1</code>, and <code>w_s</code> is a measurement of disparity\n (depth) in pixel coordinates.</p>\n <p>Note that the coordinate system for this transform is the\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE system,\n where <code>(0,0)</code> is the top-left of the\n preCorrectionActiveArraySize rectangle. Once the pose and\n intrinsic calibration transforms have been applied to a\n world point, then the ACAMERA_LENS_DISTORTION\n transform needs to be applied, and the result adjusted to\n be in the ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE coordinate\n system (where <code>(0, 0)</code> is the top-left of the\n activeArraySize rectangle), to determine the final pixel\n coordinate of the world point for processed (non-RAW)\n output buffers.</p>\n <p>For camera devices, the center of pixel <code>(x,y)</code> is located at\n coordinate <code>(x + 0.5, y + 0.5)</code>.  So on a device with a\n precorrection active array of size <code>(10,10)</code>, the valid pixel\n indices go from <code>(0,0)-(9,9)</code>, and an perfectly-built camera would\n have an optical center at the exact center of the pixel grid, at\n coordinates <code>(5.0, 5.0)</code>, which is the top-left corner of pixel\n <code>(5,5)</code>.</p>\n\n @see ACAMERA_LENS_DISTORTION\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_LENS_INTRINSIC_CALIBRATION: acamera_metadata_tag = 524298;
#[doc = " <p>The parameters for this camera device's intrinsic\n calibration.</p>\n\n <p>Type: float[5]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The five calibration parameters that describe the\n transform from camera-centric 3D coordinates to sensor\n pixel coordinates:</p>\n <pre><code>[f_x, f_y, c_x, c_y, s]\n </code></pre>\n <p>Where <code>f_x</code> and <code>f_y</code> are the horizontal and vertical\n focal lengths, <code>[c_x, c_y]</code> is the position of the optical\n axis, and <code>s</code> is a skew parameter for the sensor plane not\n being aligned with the lens plane.</p>\n <p>These are typically used within a transformation matrix K:</p>\n <pre><code>K = [ f_x,   s, c_x,\n        0, f_y, c_y,\n        0    0,   1 ]\n </code></pre>\n <p>which can then be combined with the camera pose rotation\n <code>R</code> and translation <code>t</code> (ACAMERA_LENS_POSE_ROTATION and\n ACAMERA_LENS_POSE_TRANSLATION, respectively) to calculate the\n complete transform from world coordinates to pixel\n coordinates:</p>\n <pre><code>P = [ K 0   * [ R -Rt\n      0 1 ]      0 1 ]\n </code></pre>\n <p>(Note the negation of poseTranslation when mapping from camera\n to world coordinates, and multiplication by the rotation).</p>\n <p>With <code>p_w</code> being a point in the world coordinate system\n and <code>p_s</code> being a point in the camera active pixel array\n coordinate system, and with the mapping including the\n homogeneous division by z:</p>\n <pre><code> p_h = (x_h, y_h, z_h) = P p_w\n p_s = p_h / z_h\n </code></pre>\n <p>so <code>[x_s, y_s]</code> is the pixel coordinates of the world\n point, <code>z_s = 1</code>, and <code>w_s</code> is a measurement of disparity\n (depth) in pixel coordinates.</p>\n <p>Note that the coordinate system for this transform is the\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE system,\n where <code>(0,0)</code> is the top-left of the\n preCorrectionActiveArraySize rectangle. Once the pose and\n intrinsic calibration transforms have been applied to a\n world point, then the ACAMERA_LENS_DISTORTION\n transform needs to be applied, and the result adjusted to\n be in the ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE coordinate\n system (where <code>(0, 0)</code> is the top-left of the\n activeArraySize rectangle), to determine the final pixel\n coordinate of the world point for processed (non-RAW)\n output buffers.</p>\n <p>For camera devices, the center of pixel <code>(x,y)</code> is located at\n coordinate <code>(x + 0.5, y + 0.5)</code>.  So on a device with a\n precorrection active array of size <code>(10,10)</code>, the valid pixel\n indices go from <code>(0,0)-(9,9)</code>, and an perfectly-built camera would\n have an optical center at the exact center of the pixel grid, at\n coordinates <code>(5.0, 5.0)</code>, which is the top-left corner of pixel\n <code>(5,5)</code>.</p>\n\n @see ACAMERA_LENS_DISTORTION\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_LENS_RADIAL_DISTORTION: acamera_metadata_tag = 524299;
#[doc = " <p>The origin for ACAMERA_LENS_POSE_TRANSLATION, and the accuracy of\n ACAMERA_LENS_POSE_TRANSLATION and ACAMERA_LENS_POSE_ROTATION.</p>\n\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION\n\n <p>Type: byte (acamera_metadata_enum_android_lens_pose_reference_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Different calibration methods and use cases can produce better or worse results\n depending on the selected coordinate origin.</p>"]
pub const acamera_metadata_tag_ACAMERA_LENS_POSE_REFERENCE: acamera_metadata_tag = 524300;
#[doc = " <p>The correction coefficients to correct for this camera device's\n radial and tangential lens distortion.</p>\n <p>Replaces the deprecated ACAMERA_LENS_RADIAL_DISTORTION field, which was\n inconsistently defined.</p>\n\n @see ACAMERA_LENS_RADIAL_DISTORTION\n\n <p>Type: float[5]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>Three radial distortion coefficients <code>[kappa_1, kappa_2,\n kappa_3]</code> and two tangential distortion coefficients\n <code>[kappa_4, kappa_5]</code> that can be used to correct the\n lens's geometric distortion with the mapping equations:</p>\n <pre><code> x_c = x_i * ( 1 + kappa_1 * r^2 + kappa_2 * r^4 + kappa_3 * r^6 ) +\n        kappa_4 * (2 * x_i * y_i) + kappa_5 * ( r^2 + 2 * x_i^2 )\n  y_c = y_i * ( 1 + kappa_1 * r^2 + kappa_2 * r^4 + kappa_3 * r^6 ) +\n        kappa_5 * (2 * x_i * y_i) + kappa_4 * ( r^2 + 2 * y_i^2 )\n </code></pre>\n <p>Here, <code>[x_c, y_c]</code> are the coordinates to sample in the\n input image that correspond to the pixel values in the\n corrected image at the coordinate <code>[x_i, y_i]</code>:</p>\n <pre><code> correctedImage(x_i, y_i) = sample_at(x_c, y_c, inputImage)\n </code></pre>\n <p>The pixel coordinates are defined in a coordinate system\n related to the ACAMERA_LENS_INTRINSIC_CALIBRATION\n calibration fields; see that entry for details of the mapping stages.\n Both <code>[x_i, y_i]</code> and <code>[x_c, y_c]</code>\n have <code>(0,0)</code> at the lens optical center <code>[c_x, c_y]</code>, and\n the range of the coordinates depends on the focal length\n terms of the intrinsic calibration.</p>\n <p>Finally, <code>r</code> represents the radial distance from the\n optical center, <code>r^2 = x_i^2 + y_i^2</code>.</p>\n <p>The distortion model used is the Brown-Conrady model.</p>\n\n @see ACAMERA_LENS_INTRINSIC_CALIBRATION"]
pub const acamera_metadata_tag_ACAMERA_LENS_DISTORTION: acamera_metadata_tag = 524301;
#[doc = " <p>The correction coefficients to correct for this camera device's\n radial and tangential lens distortion for a\n CaptureRequest with ACAMERA_SENSOR_PIXEL_MODE set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: float[5]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_LENS_DISTORTION, when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_LENS_DISTORTION\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_LENS_DISTORTION_MAXIMUM_RESOLUTION: acamera_metadata_tag =
    524302;
#[doc = " <p>The parameters for this camera device's intrinsic\n calibration when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: float[5]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_LENS_INTRINSIC_CALIBRATION, when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_LENS_INTRINSIC_CALIBRATION\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_LENS_INTRINSIC_CALIBRATION_MAXIMUM_RESOLUTION:
    acamera_metadata_tag = 524303;
#[doc = " <p>The parameters for this camera device's intrinsic\n calibration when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: float[5]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_LENS_INTRINSIC_CALIBRATION, when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_LENS_INTRINSIC_CALIBRATION\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_LENS_END: acamera_metadata_tag = 524304;
#[doc = " <p>List of aperture size values for ACAMERA_LENS_APERTURE that are\n supported by this camera device.</p>\n\n @see ACAMERA_LENS_APERTURE\n\n <p>Type: float[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If the camera device doesn't support a variable lens aperture,\n this list will contain only one value, which is the fixed aperture size.</p>\n <p>If the camera device supports a variable aperture, the aperture values\n in this list will be sorted in ascending order.</p>"]
pub const acamera_metadata_tag_ACAMERA_LENS_INFO_AVAILABLE_APERTURES: acamera_metadata_tag = 589824;
#[doc = " <p>List of neutral density filter values for\n ACAMERA_LENS_FILTER_DENSITY that are supported by this camera device.</p>\n\n @see ACAMERA_LENS_FILTER_DENSITY\n\n <p>Type: float[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If a neutral density filter is not supported by this camera device,\n this list will contain only 0. Otherwise, this list will include every\n filter density supported by the camera device, in ascending order.</p>"]
pub const acamera_metadata_tag_ACAMERA_LENS_INFO_AVAILABLE_FILTER_DENSITIES: acamera_metadata_tag =
    589825;
#[doc = " <p>List of focal lengths for ACAMERA_LENS_FOCAL_LENGTH that are supported by this camera\n device.</p>\n\n @see ACAMERA_LENS_FOCAL_LENGTH\n\n <p>Type: float[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If optical zoom is not supported, this list will only contain\n a single value corresponding to the fixed focal length of the\n device. Otherwise, this list will include every focal length supported\n by the camera device, in ascending order.</p>"]
pub const acamera_metadata_tag_ACAMERA_LENS_INFO_AVAILABLE_FOCAL_LENGTHS: acamera_metadata_tag =
    589826;
#[doc = " <p>List of optical image stabilization (OIS) modes for\n ACAMERA_LENS_OPTICAL_STABILIZATION_MODE that are supported by this camera device.</p>\n\n @see ACAMERA_LENS_OPTICAL_STABILIZATION_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If OIS is not supported by a given camera device, this list will\n contain only OFF.</p>"]
pub const acamera_metadata_tag_ACAMERA_LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION:
    acamera_metadata_tag = 589827;
#[doc = " <p>Hyperfocal distance for this lens.</p>\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If the lens is not fixed focus, the camera device will report this\n field when ACAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION is APPROXIMATE or CALIBRATED.</p>\n\n @see ACAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION"]
pub const acamera_metadata_tag_ACAMERA_LENS_INFO_HYPERFOCAL_DISTANCE: acamera_metadata_tag = 589828;
#[doc = " <p>Shortest distance from frontmost surface\n of the lens that can be brought into sharp focus.</p>\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If the lens is fixed-focus, this will be\n 0.</p>"]
pub const acamera_metadata_tag_ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE: acamera_metadata_tag =
    589829;
#[doc = " <p>Dimensions of lens shading map.</p>\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The map should be on the order of 30-40 rows and columns, and\n must be smaller than 64x64.</p>"]
pub const acamera_metadata_tag_ACAMERA_LENS_INFO_SHADING_MAP_SIZE: acamera_metadata_tag = 589830;
#[doc = " <p>The lens focus distance calibration quality.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_lens_info_focus_distance_calibration_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The lens focus distance calibration quality determines the reliability of\n focus related metadata entries, i.e. ACAMERA_LENS_FOCUS_DISTANCE,\n ACAMERA_LENS_FOCUS_RANGE, ACAMERA_LENS_INFO_HYPERFOCAL_DISTANCE, and\n ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE.</p>\n <p>APPROXIMATE and CALIBRATED devices report the focus metadata in\n units of diopters (1/meter), so <code>0.0f</code> represents focusing at infinity,\n and increasing positive numbers represent focusing closer and closer\n to the camera device. The focus distance control also uses diopters\n on these devices.</p>\n <p>UNCALIBRATED devices do not use units that are directly comparable\n to any real physical measurement, but <code>0.0f</code> still represents farthest\n focus, and ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE represents the\n nearest focus the device can achieve.</p>\n\n @see ACAMERA_LENS_FOCUS_DISTANCE\n @see ACAMERA_LENS_FOCUS_RANGE\n @see ACAMERA_LENS_INFO_HYPERFOCAL_DISTANCE\n @see ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE"]
pub const acamera_metadata_tag_ACAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION: acamera_metadata_tag =
    589831;
#[doc = " <p>The lens focus distance calibration quality.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_lens_info_focus_distance_calibration_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The lens focus distance calibration quality determines the reliability of\n focus related metadata entries, i.e. ACAMERA_LENS_FOCUS_DISTANCE,\n ACAMERA_LENS_FOCUS_RANGE, ACAMERA_LENS_INFO_HYPERFOCAL_DISTANCE, and\n ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE.</p>\n <p>APPROXIMATE and CALIBRATED devices report the focus metadata in\n units of diopters (1/meter), so <code>0.0f</code> represents focusing at infinity,\n and increasing positive numbers represent focusing closer and closer\n to the camera device. The focus distance control also uses diopters\n on these devices.</p>\n <p>UNCALIBRATED devices do not use units that are directly comparable\n to any real physical measurement, but <code>0.0f</code> still represents farthest\n focus, and ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE represents the\n nearest focus the device can achieve.</p>\n\n @see ACAMERA_LENS_FOCUS_DISTANCE\n @see ACAMERA_LENS_FOCUS_RANGE\n @see ACAMERA_LENS_INFO_HYPERFOCAL_DISTANCE\n @see ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE"]
pub const acamera_metadata_tag_ACAMERA_LENS_INFO_END: acamera_metadata_tag = 589832;
#[doc = " <p>Mode of operation for the noise reduction algorithm.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_noise_reduction_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>The noise reduction algorithm attempts to improve image quality by removing\n excessive noise added by the capture process, especially in dark conditions.</p>\n <p>OFF means no noise reduction will be applied by the camera device, for both raw and\n YUV domain.</p>\n <p>MINIMAL means that only sensor raw domain basic noise reduction is enabled ,to remove\n demosaicing or other processing artifacts. For YUV_REPROCESSING, MINIMAL is same as OFF.\n This mode is optional, may not be support by all devices. The application should check\n ACAMERA_NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES before using it.</p>\n <p>FAST/HIGH_QUALITY both mean camera device determined noise filtering\n will be applied. HIGH_QUALITY mode indicates that the camera device\n will use the highest-quality noise filtering algorithms,\n even if it slows down capture rate. FAST means the camera device will not\n slow down capture rate when applying noise filtering. FAST may be the same as MINIMAL if\n MINIMAL is listed, or the same as OFF if any noise filtering will slow down capture rate.\n Every output stream will have a similar amount of enhancement applied.</p>\n <p>ZERO_SHUTTER_LAG is meant to be used by applications that maintain a continuous circular\n buffer of high-resolution images during preview and reprocess image(s) from that buffer\n into a final capture when triggered by the user. In this mode, the camera device applies\n noise reduction to low-resolution streams (below maximum recording resolution) to maximize\n preview quality, but does not apply noise reduction to high-resolution streams, since\n those will be reprocessed later if necessary.</p>\n <p>For YUV_REPROCESSING, these FAST/HIGH_QUALITY modes both mean that the camera device\n will apply FAST/HIGH_QUALITY YUV domain noise reduction, respectively. The camera device\n may adjust the noise reduction parameters for best image quality based on the\n android.reprocess.effectiveExposureFactor if it is set.</p>\n\n @see ACAMERA_NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES"]
pub const acamera_metadata_tag_ACAMERA_NOISE_REDUCTION_MODE: acamera_metadata_tag = 655360;
#[doc = " <p>List of noise reduction modes for ACAMERA_NOISE_REDUCTION_MODE that are supported\n by this camera device.</p>\n\n @see ACAMERA_NOISE_REDUCTION_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Full-capability camera devices will always support OFF and FAST.</p>\n <p>Camera devices that support YUV_REPROCESSING or PRIVATE_REPROCESSING will support\n ZERO_SHUTTER_LAG.</p>\n <p>Legacy-capability camera devices will only support FAST mode.</p>"]
pub const acamera_metadata_tag_ACAMERA_NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES:
    acamera_metadata_tag = 655362;
#[doc = " <p>List of noise reduction modes for ACAMERA_NOISE_REDUCTION_MODE that are supported\n by this camera device.</p>\n\n @see ACAMERA_NOISE_REDUCTION_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Full-capability camera devices will always support OFF and FAST.</p>\n <p>Camera devices that support YUV_REPROCESSING or PRIVATE_REPROCESSING will support\n ZERO_SHUTTER_LAG.</p>\n <p>Legacy-capability camera devices will only support FAST mode.</p>"]
pub const acamera_metadata_tag_ACAMERA_NOISE_REDUCTION_END: acamera_metadata_tag = 655363;
#[doc = " <p>The maximum numbers of different types of output streams\n that can be configured and used simultaneously by a camera device.</p>\n\n <p>Type: int32[3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This is a 3 element tuple that contains the max number of output simultaneous\n streams for raw sensor, processed (but not stalling), and processed (and stalling)\n formats respectively. For example, assuming that JPEG is typically a processed and\n stalling stream, if max raw sensor format output stream number is 1, max YUV streams\n number is 3, and max JPEG stream number is 2, then this tuple should be <code>(1, 3, 2)</code>.</p>\n <p>This lists the upper bound of the number of output streams supported by\n the camera device. Using more streams simultaneously may require more hardware and\n CPU resources that will consume more power. The image format for an output stream can\n be any supported format provided by ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS.\n The formats defined in ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS can be categorized\n into the 3 stream types as below:</p>\n <ul>\n <li>Processed (but stalling): any non-RAW format with a stallDurations &gt; 0.\n   Typically {@link AIMAGE_FORMAT_JPEG JPEG format}.</li>\n <li>Raw formats: {@link AIMAGE_FORMAT_RAW16 RAW_SENSOR}, {@link AIMAGE_FORMAT_RAW10 RAW10}, or\n   {@link AIMAGE_FORMAT_RAW12 RAW12}.</li>\n <li>Processed (but not-stalling): any non-RAW format without a stall duration.  Typically\n   {@link AIMAGE_FORMAT_YUV_420_888 YUV_420_888},\n   <a href=\"https://developer.android.com/reference/android/graphics/ImageFormat.html#NV21\">NV21</a>, <a href=\"https://developer.android.com/reference/android/graphics/ImageFormat.html#YV12\">YV12</a>, or {@link AIMAGE_FORMAT_Y8 Y8} .</li>\n </ul>\n\n @see ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_MAX_NUM_OUTPUT_STREAMS: acamera_metadata_tag =
    786438;
#[doc = " <p>Specifies the number of pipeline stages the frame went\n through from when it was exposed to when the final completed result\n was available to the framework.</p>\n\n <p>Type: byte</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>Depending on what settings are used in the request, and\n what streams are configured, the data may undergo less processing,\n and some pipeline stages skipped.</p>\n <p>See ACAMERA_REQUEST_PIPELINE_MAX_DEPTH for more details.</p>\n\n @see ACAMERA_REQUEST_PIPELINE_MAX_DEPTH"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_PIPELINE_DEPTH: acamera_metadata_tag = 786441;
#[doc = " <p>Specifies the number of maximum pipeline stages a frame\n has to go through from when it's exposed to when it's available\n to the framework.</p>\n\n <p>Type: byte</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>A typical minimum value for this is 2 (one stage to expose,\n one stage to readout) from the sensor. The ISP then usually adds\n its own stages to do custom HW processing. Further stages may be\n added by SW processing.</p>\n <p>Depending on what settings are used (e.g. YUV, JPEG) and what\n processing is enabled (e.g. face detection), the actual pipeline\n depth (specified by ACAMERA_REQUEST_PIPELINE_DEPTH) may be less than\n the max pipeline depth.</p>\n <p>A pipeline depth of X stages is equivalent to a pipeline latency of\n X frame intervals.</p>\n <p>This value will normally be 8 or less, however, for high speed capture session,\n the max pipeline depth will be up to 8 x size of high speed capture request list.</p>\n\n @see ACAMERA_REQUEST_PIPELINE_DEPTH"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_PIPELINE_MAX_DEPTH: acamera_metadata_tag = 786442;
#[doc = " <p>Defines how many sub-components\n a result will be composed of.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>In order to combat the pipeline latency, partial results\n may be delivered to the application layer from the camera device as\n soon as they are available.</p>\n <p>Optional; defaults to 1. A value of 1 means that partial\n results are not supported, and only the final TotalCaptureResult will\n be produced by the camera device.</p>\n <p>A typical use case for this might be: after requesting an\n auto-focus (AF) lock the new AF state might be available 50%\n of the way through the pipeline.  The camera device could\n then immediately dispatch this state via a partial result to\n the application, and the rest of the metadata via later\n partial results.</p>"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_PARTIAL_RESULT_COUNT: acamera_metadata_tag = 786443;
#[doc = " <p>List of capabilities that this camera device\n advertises as fully supporting.</p>\n\n <p>Type: byte[n] (acamera_metadata_enum_android_request_available_capabilities_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>A capability is a contract that the camera device makes in order\n to be able to satisfy one or more use cases.</p>\n <p>Listing a capability guarantees that the whole set of features\n required to support a common use will all be available.</p>\n <p>Using a subset of the functionality provided by an unsupported\n capability may be possible on a specific camera device implementation;\n to do this query each of ACAMERA_REQUEST_AVAILABLE_REQUEST_KEYS,\n ACAMERA_REQUEST_AVAILABLE_RESULT_KEYS,\n ACAMERA_REQUEST_AVAILABLE_CHARACTERISTICS_KEYS.</p>\n <p>The following capabilities are guaranteed to be available on\n ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL <code>==</code> FULL devices:</p>\n <ul>\n <li>MANUAL_SENSOR</li>\n <li>MANUAL_POST_PROCESSING</li>\n </ul>\n <p>Other capabilities may be available on either FULL or LIMITED\n devices, but the application should query this key to be sure.</p>\n\n @see ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL\n @see ACAMERA_REQUEST_AVAILABLE_CHARACTERISTICS_KEYS\n @see ACAMERA_REQUEST_AVAILABLE_REQUEST_KEYS\n @see ACAMERA_REQUEST_AVAILABLE_RESULT_KEYS"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES: acamera_metadata_tag =
    786444;
#[doc = " <p>A list of all keys that the camera device has available\n to use with {@link ACaptureRequest }.</p>\n\n <p>Type: int32[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Attempting to set a key into a CaptureRequest that is not\n listed here will result in an invalid request and will be rejected\n by the camera device.</p>\n <p>This field can be used to query the feature set of a camera device\n at a more granular level than capabilities. This is especially\n important for optional keys that are not listed under any capability\n in ACAMERA_REQUEST_AVAILABLE_CAPABILITIES.</p>\n\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_AVAILABLE_REQUEST_KEYS: acamera_metadata_tag =
    786445;
#[doc = " <p>A list of all keys that the camera device has available to use with {@link ACameraCaptureSession_captureCallback_result }.</p>\n\n <p>Type: int32[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Attempting to get a key from a CaptureResult that is not\n listed here will always return a <code>null</code> value. Getting a key from\n a CaptureResult that is listed here will generally never return a <code>null</code>\n value.</p>\n <p>The following keys may return <code>null</code> unless they are enabled:</p>\n <ul>\n <li>ACAMERA_STATISTICS_LENS_SHADING_MAP (non-null iff ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE == ON)</li>\n </ul>\n <p>(Those sometimes-null keys will nevertheless be listed here\n if they are available.)</p>\n <p>This field can be used to query the feature set of a camera device\n at a more granular level than capabilities. This is especially\n important for optional keys that are not listed under any capability\n in ACAMERA_REQUEST_AVAILABLE_CAPABILITIES.</p>\n\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES\n @see ACAMERA_STATISTICS_LENS_SHADING_MAP\n @see ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_AVAILABLE_RESULT_KEYS: acamera_metadata_tag = 786446;
#[doc = " <p>A list of all keys that the camera device has available to use with {@link ACameraManager_getCameraCharacteristics }.</p>\n\n <p>Type: int32[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This entry follows the same rules as\n ACAMERA_REQUEST_AVAILABLE_RESULT_KEYS (except that it applies for\n CameraCharacteristics instead of CaptureResult). See above for more\n details.</p>\n\n @see ACAMERA_REQUEST_AVAILABLE_RESULT_KEYS"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_AVAILABLE_CHARACTERISTICS_KEYS:
    acamera_metadata_tag = 786447;
#[doc = " <p>A subset of the available request keys that the camera device\n can pass as part of the capture session initialization.</p>\n\n <p>Type: int32[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This is a subset of ACAMERA_REQUEST_AVAILABLE_REQUEST_KEYS which\n contains a list of keys that are difficult to apply per-frame and\n can result in unexpected delays when modified during the capture session\n lifetime. Typical examples include parameters that require a\n time-consuming hardware re-configuration or internal camera pipeline\n change. For performance reasons we advise clients to pass their initial\n values as part of\n {@link ACameraDevice_createCaptureSessionWithSessionParameters }.\n Once the camera capture session is enabled it is also recommended to avoid\n changing them from their initial values set in\n {@link ACameraDevice_createCaptureSessionWithSessionParameters }.\n Control over session parameters can still be exerted in capture requests\n but clients should be aware and expect delays during their application.\n An example usage scenario could look like this:</p>\n <ul>\n <li>The camera client starts by querying the session parameter key list via\n   {@link ACameraManager_getCameraCharacteristics }.</li>\n <li>Before triggering the capture session create sequence, a capture request\n   must be built via\n   {@link ACameraDevice_createCaptureRequest }\n   using an appropriate template matching the particular use case.</li>\n <li>The client should go over the list of session parameters and check\n   whether some of the keys listed matches with the parameters that\n   they intend to modify as part of the first capture request.</li>\n <li>If there is no such match, the capture request can be  passed\n   unmodified to\n   {@link ACameraDevice_createCaptureSessionWithSessionParameters }.</li>\n <li>If matches do exist, the client should update the respective values\n   and pass the request to\n   {@link ACameraDevice_createCaptureSessionWithSessionParameters }.</li>\n <li>After the capture session initialization completes the session parameter\n   key list can continue to serve as reference when posting or updating\n   further requests. As mentioned above further changes to session\n   parameters should ideally be avoided, if updates are necessary\n   however clients could expect a delay/glitch during the\n   parameter switch.</li>\n </ul>\n\n @see ACAMERA_REQUEST_AVAILABLE_REQUEST_KEYS"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_AVAILABLE_SESSION_KEYS: acamera_metadata_tag =
    786448;
#[doc = " <p>A subset of the available request keys that can be overridden for\n physical devices backing a logical multi-camera.</p>\n\n <p>Type: int32[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This is a subset of ACAMERA_REQUEST_AVAILABLE_REQUEST_KEYS which contains a list\n of keys that can be overridden using\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CaptureRequest.Builder.html#setPhysicalCameraKey\">Builder#setPhysicalCameraKey</a>.\n The respective value of such request key can be obtained by calling\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CaptureRequest.Builder.html#getPhysicalCameraKey\">Builder#getPhysicalCameraKey</a>.\n Capture requests that contain individual physical device requests must be built via\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraDevice.html#createCaptureRequest(int,\">Set)</a>.</p>\n\n @see ACAMERA_REQUEST_AVAILABLE_REQUEST_KEYS"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS:
    acamera_metadata_tag = 786449;
#[doc = " <p>A map of all available 10-bit dynamic range profiles along with their\n capture request constraints.</p>\n\n <p>Type: int64[n*3] (acamera_metadata_enum_android_request_available_dynamic_range_profiles_map_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Devices supporting the 10-bit output capability\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#REQUEST_AVAILABLE_CAPABILITIES_DYNAMIC_RANGE_TEN_BIT\">CameraCharacteristics#REQUEST_AVAILABLE_CAPABILITIES_DYNAMIC_RANGE_TEN_BIT</a>\n must list their supported dynamic range profiles. In case the camera is not able to\n support every possible profile combination within a single capture request, then the\n constraints must be listed here as well.</p>"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP:
    acamera_metadata_tag = 786451;
#[doc = " <p>A map of all available 10-bit dynamic range profiles along with their\n capture request constraints.</p>\n\n <p>Type: int64[n*3] (acamera_metadata_enum_android_request_available_dynamic_range_profiles_map_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Devices supporting the 10-bit output capability\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#REQUEST_AVAILABLE_CAPABILITIES_DYNAMIC_RANGE_TEN_BIT\">CameraCharacteristics#REQUEST_AVAILABLE_CAPABILITIES_DYNAMIC_RANGE_TEN_BIT</a>\n must list their supported dynamic range profiles. In case the camera is not able to\n support every possible profile combination within a single capture request, then the\n constraints must be listed here as well.</p>"]
pub const acamera_metadata_tag_ACAMERA_REQUEST_END: acamera_metadata_tag = 786452;
#[doc = " <p>The desired region of the sensor to read out for this capture.</p>\n\n <p>Type: int32[4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This control can be used to implement digital zoom.</p>\n <p>For devices not supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system always follows that of ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with <code>(0, 0)</code> being\n the top-left pixel of the active array.</p>\n <p>For devices supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate system\n depends on the mode being set.  When the distortion correction mode is OFF, the\n coordinate system follows ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE, with <code>(0,\n 0)</code> being the top-left pixel of the pre-correction active array.  When the distortion\n correction mode is not OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with <code>(0, 0)</code> being the top-left pixel of the\n active array.</p>\n <p>Output streams use this rectangle to produce their output, cropping to a smaller region\n if necessary to maintain the stream's aspect ratio, then scaling the sensor input to\n match the output's configured resolution.</p>\n <p>The crop region is applied after the RAW to other color space (e.g. YUV)\n conversion. Since raw streams (e.g. RAW16) don't have the conversion stage, they are not\n croppable. The crop region will be ignored by raw streams.</p>\n <p>For non-raw streams, any additional per-stream cropping will be done to maximize the\n final pixel area of the stream.</p>\n <p>For example, if the crop region is set to a 4:3 aspect ratio, then 4:3 streams will use\n the exact crop region. 16:9 streams will further crop vertically (letterbox).</p>\n <p>Conversely, if the crop region is set to a 16:9, then 4:3 outputs will crop horizontally\n (pillarbox), and 16:9 streams will match exactly. These additional crops will be\n centered within the crop region.</p>\n <p>To illustrate, here are several scenarios of different crop regions and output streams,\n for a hypothetical camera device with an active array of size <code>(2000,1500)</code>.  Note that\n several of these examples use non-centered crop regions for ease of illustration; such\n regions are only supported on devices with FREEFORM capability\n (ACAMERA_SCALER_CROPPING_TYPE <code>== FREEFORM</code>), but this does not affect the way the crop\n rules work otherwise.</p>\n <ul>\n <li>Camera Configuration:<ul>\n <li>Active array size: <code>2000x1500</code> (3 MP, 4:3 aspect ratio)</li>\n <li>Output stream #1: <code>640x480</code> (VGA, 4:3 aspect ratio)</li>\n <li>Output stream #2: <code>1280x720</code> (720p, 16:9 aspect ratio)</li>\n </ul>\n </li>\n <li>Case #1: 4:3 crop region with 2x digital zoom<ul>\n <li>Crop region: <code>Rect(500, 375, 1500, 1125) // (left, top, right, bottom)</code></li>\n <li><img alt=\"4:3 aspect ratio crop diagram\" src=\"../images/camera2/metadata/android.scaler.cropRegion/crop-region-43-ratio.png\" /></li>\n <li><code>640x480</code> stream source area: <code>(500, 375, 1500, 1125)</code> (equal to crop region)</li>\n <li><code>1280x720</code> stream source area: <code>(500, 469, 1500, 1031)</code> (letterboxed)</li>\n </ul>\n </li>\n <li>Case #2: 16:9 crop region with ~1.5x digital zoom.<ul>\n <li>Crop region: <code>Rect(500, 375, 1833, 1125)</code></li>\n <li><img alt=\"16:9 aspect ratio crop diagram\" src=\"../images/camera2/metadata/android.scaler.cropRegion/crop-region-169-ratio.png\" /></li>\n <li><code>640x480</code> stream source area: <code>(666, 375, 1666, 1125)</code> (pillarboxed)</li>\n <li><code>1280x720</code> stream source area: <code>(500, 375, 1833, 1125)</code> (equal to crop region)</li>\n </ul>\n </li>\n <li>Case #3: 1:1 crop region with ~2.6x digital zoom.<ul>\n <li>Crop region: <code>Rect(500, 375, 1250, 1125)</code></li>\n <li><img alt=\"1:1 aspect ratio crop diagram\" src=\"../images/camera2/metadata/android.scaler.cropRegion/crop-region-11-ratio.png\" /></li>\n <li><code>640x480</code> stream source area: <code>(500, 469, 1250, 1031)</code> (letterboxed)</li>\n <li><code>1280x720</code> stream source area: <code>(500, 543, 1250, 957)</code> (letterboxed)</li>\n </ul>\n </li>\n <li>Case #4: Replace <code>640x480</code> stream with <code>1024x1024</code> stream, with 4:3 crop region:<ul>\n <li>Crop region: <code>Rect(500, 375, 1500, 1125)</code></li>\n <li><img alt=\"Square output, 4:3 aspect ratio crop diagram\" src=\"../images/camera2/metadata/android.scaler.cropRegion/crop-region-43-square-ratio.png\" /></li>\n <li><code>1024x1024</code> stream source area: <code>(625, 375, 1375, 1125)</code> (pillarboxed)</li>\n <li><code>1280x720</code> stream source area: <code>(500, 469, 1500, 1031)</code> (letterboxed)</li>\n <li>Note that in this case, neither of the two outputs is a subset of the other, with\n   each containing image data the other doesn't have.</li>\n </ul>\n </li>\n </ul>\n <p>If the coordinate system is ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, the width and height\n of the crop region cannot be set to be smaller than\n <code>floor( activeArraySize.width / ACAMERA_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM )</code> and\n <code>floor( activeArraySize.height / ACAMERA_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM )</code>, respectively.</p>\n <p>If the coordinate system is ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE, the width\n and height of the crop region cannot be set to be smaller than\n <code>floor( preCorrectionActiveArraySize.width / ACAMERA_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM )</code>\n and\n <code>floor( preCorrectionActiveArraySize.height / ACAMERA_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM )</code>,\n respectively.</p>\n <p>The camera device may adjust the crop region to account for rounding and other hardware\n requirements; the final crop region used will be included in the output capture result.</p>\n <p>The camera sensor output aspect ratio depends on factors such as output stream\n combination and ACAMERA_CONTROL_AE_TARGET_FPS_RANGE, and shouldn't be adjusted by using\n this control. And the camera device will treat different camera sensor output sizes\n (potentially with in-sensor crop) as the same crop of\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE. As a result, the application shouldn't assume the\n maximum crop region always maps to the same aspect ratio or field of view for the\n sensor output.</p>\n <p>Starting from API level 30, it's strongly recommended to use ACAMERA_CONTROL_ZOOM_RATIO\n to take advantage of better support for zoom with logical multi-camera. The benefits\n include better precision with optical-digital zoom combination, and ability to do\n zoom-out from 1.0x. When using ACAMERA_CONTROL_ZOOM_RATIO for zoom, the crop region in\n the capture request should be left as the default activeArray size. The\n coordinate system is post-zoom, meaning that the activeArraySize or\n preCorrectionActiveArraySize covers the camera device's field of view \"after\" zoom.  See\n ACAMERA_CONTROL_ZOOM_RATIO for details.</p>\n <p>For camera devices with the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability, ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION /\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION must be used as the\n coordinate system for requests where ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n <p>The data representation is int[4], which maps to (left, top, width, height).</p>\n\n @see ACAMERA_CONTROL_AE_TARGET_FPS_RANGE\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_DISTORTION_CORRECTION_MODE\n @see ACAMERA_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM\n @see ACAMERA_SCALER_CROPPING_TYPE\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_SCALER_CROP_REGION: acamera_metadata_tag = 851968;
#[doc = " <p>The maximum ratio between both active area width\n and crop region width, and active area height and\n crop region height, for ACAMERA_SCALER_CROP_REGION.</p>\n\n @see ACAMERA_SCALER_CROP_REGION\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This represents the maximum amount of zooming possible by\n the camera device, or equivalently, the minimum cropping\n window size.</p>\n <p>Crop regions that have a width or height that is smaller\n than this ratio allows will be rounded up to the minimum\n allowed size by the camera device.</p>\n <p>Starting from API level 30, when using ACAMERA_CONTROL_ZOOM_RATIO to zoom in or out,\n the application must use ACAMERA_CONTROL_ZOOM_RATIO_RANGE to query both the minimum and\n maximum zoom ratio.</p>\n\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_CONTROL_ZOOM_RATIO_RANGE"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM: acamera_metadata_tag =
    851972;
#[doc = " <p>The available stream configurations that this\n camera device supports\n (i.e. format, width, height, output/input stream).</p>\n\n <p>Type: int32[n*4] (acamera_metadata_enum_android_scaler_available_stream_configurations_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The configurations are listed as <code>(format, width, height, input?)</code>\n tuples.</p>\n <p>For a given use case, the actual maximum supported resolution\n may be lower than what is listed here, depending on the destination\n Surface for the image data. For example, for recording video,\n the video encoder chosen may have a maximum size limit (e.g. 1080p)\n smaller than what the camera (e.g. maximum resolution is 3264x2448)\n can provide.</p>\n <p>Please reference the documentation for the image data destination to\n check if it limits the maximum size for image data.</p>\n <p>Not all output formats may be supported in a configuration with\n an input stream of a particular format. For more details, see\n android.scaler.availableInputOutputFormatsMap.</p>\n <p>For applications targeting SDK version older than 31, the following table\n describes the minimum required output stream configurations based on the hardware level\n (ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL):</p>\n <p>Format         | Size                                         | Hardware Level | Notes\n :-------------:|:--------------------------------------------:|:--------------:|:--------------:\n JPEG           | ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE          | Any            |\n JPEG           | 1920x1080 (1080p)                            | Any            | if 1080p &lt;= activeArraySize\n JPEG           | 1280x720 (720)                               | Any            | if 720p &lt;= activeArraySize\n JPEG           | 640x480 (480p)                               | Any            | if 480p &lt;= activeArraySize\n JPEG           | 320x240 (240p)                               | Any            | if 240p &lt;= activeArraySize\n YUV_420_888    | all output sizes available for JPEG          | FULL           |\n YUV_420_888    | all output sizes available for JPEG, up to the maximum video size | LIMITED        |\n IMPLEMENTATION_DEFINED | same as YUV_420_888                  | Any            |</p>\n <p>For applications targeting SDK version 31 or newer, if the mobile device declares to be\n media performance class 12 or higher by setting\n <a href=\"https://developer.android.com/reference/android/os/Build.VERSION.html#MEDIA_PERFORMANCE_CLASS\">VERSION#MEDIA_PERFORMANCE_CLASS</a> to be 31 or larger,\n the primary camera devices (first rear/front camera in the camera ID list) will not\n support JPEG sizes smaller than 1080p. If the application configures a JPEG stream\n smaller than 1080p, the camera device will round up the JPEG image size to at least\n 1080p. The requirements for IMPLEMENTATION_DEFINED and YUV_420_888 stay the same.\n This new minimum required output stream configurations are illustrated by the table below:</p>\n <p>Format         | Size                                         | Hardware Level | Notes\n :-------------:|:--------------------------------------------:|:--------------:|:--------------:\n JPEG           | ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE          | Any            |\n JPEG           | 1920x1080 (1080p)                            | Any            | if 1080p &lt;= activeArraySize\n YUV_420_888    | ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE          | FULL           |\n YUV_420_888    | 1920x1080 (1080p)                            | FULL           | if 1080p &lt;= activeArraySize\n YUV_420_888    | 1280x720 (720)                               | FULL           | if 720p &lt;= activeArraySize\n YUV_420_888    | 640x480 (480p)                               | FULL           | if 480p &lt;= activeArraySize\n YUV_420_888    | 320x240 (240p)                               | FULL           | if 240p &lt;= activeArraySize\n YUV_420_888    | all output sizes available for FULL hardware level, up to the maximum video size | LIMITED        |\n IMPLEMENTATION_DEFINED | same as YUV_420_888                  | Any            |</p>\n <p>For applications targeting SDK version 31 or newer, if the mobile device doesn't declare\n to be media performance class 12 or better by setting\n <a href=\"https://developer.android.com/reference/android/os/Build.VERSION.html#MEDIA_PERFORMANCE_CLASS\">VERSION#MEDIA_PERFORMANCE_CLASS</a> to be 31 or larger,\n or if the camera device isn't a primary rear/front camera, the minimum required output\n stream configurations are the same as for applications targeting SDK version older than\n 31.</p>\n <p>Refer to ACAMERA_REQUEST_AVAILABLE_CAPABILITIES for additional\n mandatory stream configurations on a per-capability basis.</p>\n <p>Exception on 176x144 (QCIF) resolution: camera devices usually have a fixed capability for\n downscaling from larger resolution to smaller, and the QCIF resolution sometimes is not\n fully supported due to this limitation on devices with high-resolution image sensors.\n Therefore, trying to configure a QCIF resolution stream together with any other\n stream larger than 1920x1080 resolution (either width or height) might not be supported,\n and capture session creation will fail if it is not.</p>\n\n @see ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS:
    acamera_metadata_tag = 851978;
#[doc = " <p>This lists the minimum frame duration for each\n format/size combination.</p>\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This should correspond to the frame duration when only that\n stream is active, with all processing (typically in android.*.mode)\n set to either OFF or FAST.</p>\n <p>When multiple streams are used in a request, the minimum frame\n duration will be max(individual stream min durations).</p>\n <p>See ACAMERA_SENSOR_FRAME_DURATION and\n ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS for more details about\n calculating the max frame rate.</p>\n\n @see ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS\n @see ACAMERA_SENSOR_FRAME_DURATION"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS: acamera_metadata_tag =
    851979;
#[doc = " <p>This lists the maximum stall duration for each\n output format/size combination.</p>\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>A stall duration is how much extra time would get added\n to the normal minimum frame duration for a repeating request\n that has streams with non-zero stall.</p>\n <p>For example, consider JPEG captures which have the following\n characteristics:</p>\n <ul>\n <li>JPEG streams act like processed YUV streams in requests for which\n they are not included; in requests in which they are directly\n referenced, they act as JPEG streams. This is because supporting a\n JPEG stream requires the underlying YUV data to always be ready for\n use by a JPEG encoder, but the encoder will only be used (and impact\n frame duration) on requests that actually reference a JPEG stream.</li>\n <li>The JPEG processor can run concurrently to the rest of the camera\n pipeline, but cannot process more than 1 capture at a time.</li>\n </ul>\n <p>In other words, using a repeating YUV request would result\n in a steady frame rate (let's say it's 30 FPS). If a single\n JPEG request is submitted periodically, the frame rate will stay\n at 30 FPS (as long as we wait for the previous JPEG to return each\n time). If we try to submit a repeating YUV + JPEG request, then\n the frame rate will drop from 30 FPS.</p>\n <p>In general, submitting a new request with a non-0 stall time\n stream will <em>not</em> cause a frame rate drop unless there are still\n outstanding buffers for that stream from previous requests.</p>\n <p>Submitting a repeating request with streams (call this <code>S</code>)\n is the same as setting the minimum frame duration from\n the normal minimum frame duration corresponding to <code>S</code>, added with\n the maximum stall duration for <code>S</code>.</p>\n <p>If interleaving requests with and without a stall duration,\n a request will stall by the maximum of the remaining times\n for each can-stall stream with outstanding buffers.</p>\n <p>This means that a stalling request will not have an exposure start\n until the stall has completed.</p>\n <p>This should correspond to the stall duration when only that stream is\n active, with all processing (typically in android.*.mode) set to FAST\n or OFF. Setting any of the processing modes to HIGH_QUALITY\n effectively results in an indeterminate stall duration for all\n streams in a request (the regular stall calculation rules are\n ignored).</p>\n <p>The following formats may always have a stall duration:</p>\n <ul>\n <li>{@link AIMAGE_FORMAT_JPEG }</li>\n <li>{@link AIMAGE_FORMAT_RAW16 }</li>\n </ul>\n <p>The following formats will never have a stall duration:</p>\n <ul>\n <li>{@link AIMAGE_FORMAT_YUV_420_888 }</li>\n <li>{@link AIMAGE_FORMAT_RAW10 }</li>\n <li>{@link AIMAGE_FORMAT_RAW12 }</li>\n <li>{@link AIMAGE_FORMAT_Y8 }</li>\n </ul>\n <p>All other formats may or may not have an allowed stall duration on\n a per-capability basis; refer to ACAMERA_REQUEST_AVAILABLE_CAPABILITIES\n for more details.</p>\n <p>See ACAMERA_SENSOR_FRAME_DURATION for more information about\n calculating the max frame rate (absent stalls).</p>\n\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES\n @see ACAMERA_SENSOR_FRAME_DURATION"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS: acamera_metadata_tag =
    851980;
#[doc = " <p>The crop type that this camera device supports.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_scaler_cropping_type_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>When passing a non-centered crop region (ACAMERA_SCALER_CROP_REGION) to a camera\n device that only supports CENTER_ONLY cropping, the camera device will move the\n crop region to the center of the sensor active array (ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE)\n and keep the crop region width and height unchanged. The camera device will return the\n final used crop region in metadata result ACAMERA_SCALER_CROP_REGION.</p>\n <p>Camera devices that support FREEFORM cropping will support any crop region that\n is inside of the active array. The camera device will apply the same crop region and\n return the final used crop region in capture result metadata ACAMERA_SCALER_CROP_REGION.</p>\n <p>Starting from API level 30,</p>\n <ul>\n <li>If the camera device supports FREEFORM cropping, in order to do FREEFORM cropping, the\n application must set ACAMERA_CONTROL_ZOOM_RATIO to 1.0, and use ACAMERA_SCALER_CROP_REGION\n for zoom.</li>\n <li>To do CENTER_ONLY zoom, the application has below 2 options:<ol>\n <li>Set ACAMERA_CONTROL_ZOOM_RATIO to 1.0; adjust zoom by ACAMERA_SCALER_CROP_REGION.</li>\n <li>Adjust zoom by ACAMERA_CONTROL_ZOOM_RATIO; use ACAMERA_SCALER_CROP_REGION to crop\n the field of view vertically (letterboxing) or horizontally (pillarboxing), but not\n windowboxing.</li>\n </ol>\n </li>\n <li>Setting ACAMERA_CONTROL_ZOOM_RATIO to values different than 1.0 and\n ACAMERA_SCALER_CROP_REGION to be windowboxing at the same time are not supported. In this\n case, the camera framework will override the ACAMERA_SCALER_CROP_REGION to be the active\n array.</li>\n </ul>\n <p>LEGACY capability devices will only support CENTER_ONLY cropping.</p>\n\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_SCALER_CROP_REGION\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_SCALER_CROPPING_TYPE: acamera_metadata_tag = 851981;
#[doc = " <p>Recommended stream configurations for common client use cases.</p>\n\n <p>Type: int32[n*5] (acamera_metadata_enum_android_scaler_available_recommended_stream_configurations_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Optional subset of the ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS that contains\n similar tuples listed as\n (i.e. width, height, format, output/input stream, usecase bit field).\n Camera devices will be able to suggest particular stream configurations which are\n power and performance efficient for specific use cases. For more information about\n retrieving the suggestions see\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#getRecommendedStreamConfigurationMap\">CameraCharacteristics#getRecommendedStreamConfigurationMap</a>.</p>\n <p>The data representation is int[5], which maps to\n (width, height, format, output/input stream, usecase bit field). The array can be\n parsed using the following pseudo code:</p>\n <p>struct StreamConfiguration {\n int32_t format;\n int32_t width;\n int32_t height;\n int32_t isInput; };</p>\n <p>void getPreferredStreamConfigurations(\n     int32_t *array, size_t count, int32_t usecaseId,\n     Vector &lt; StreamConfiguration &gt; * scs) {\n     const size_t STREAM_CONFIGURATION_SIZE = 5;\n     const size_t STREAM_WIDTH_OFFSET = 0;\n     const size_t STREAM_HEIGHT_OFFSET = 1;\n     const size_t STREAM_FORMAT_OFFSET = 2;\n     const size_t STREAM_IS_INPUT_OFFSET = 3;\n     const size_t STREAM_USECASE_BITMAP_OFFSET = 4;</p>\n <pre><code>for (size_t i = 0; i &lt; count; i+= STREAM_CONFIGURATION_SIZE) {\n     int32_t width = array[i + STREAM_WIDTH_OFFSET];\n     int32_t height = array[i + STREAM_HEIGHT_OFFSET];\n     int32_t format = array[i + STREAM_FORMAT_OFFSET];\n     int32_t isInput = array[i + STREAM_IS_INPUT_OFFSET];\n     int32_t supportedUsecases = array[i + STREAM_USECASE_BITMAP_OFFSET];\n     if (supportedUsecases &amp; (1 &lt;&lt; usecaseId)) {\n         StreamConfiguration sc = {format, width, height, isInput};\n         scs-&gt;add(sc);\n     }\n }\n </code></pre>\n <p>}</p>\n\n @see ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS:
    acamera_metadata_tag = 851982;
#[doc = " <p>Recommended mappings of image formats that are supported by this\n camera device for input streams, to their corresponding output formats.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This is a recommended subset of the complete list of mappings found in\n android.scaler.availableInputOutputFormatsMap. The same requirements apply here as well.\n The list however doesn't need to contain all available and supported mappings. Instead of\n this developers must list only recommended and efficient entries.\n If set, the information will be available in the ZERO_SHUTTER_LAG recommended stream\n configuration see\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#getRecommendedStreamConfigurationMap\">CameraCharacteristics#getRecommendedStreamConfigurationMap</a>.</p>"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_INPUT_OUTPUT_FORMATS_MAP:
    acamera_metadata_tag = 851983;
#[doc = " <p>List of rotate-and-crop modes for ACAMERA_SCALER_ROTATE_AND_CROP that are supported by this camera device.</p>\n\n @see ACAMERA_SCALER_ROTATE_AND_CROP\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This entry lists the valid modes for ACAMERA_SCALER_ROTATE_AND_CROP for this camera device.</p>\n <p>Starting with API level 30, all devices will list at least <code>ROTATE_AND_CROP_NONE</code>.\n Devices with support for rotate-and-crop will additionally list at least\n <code>ROTATE_AND_CROP_AUTO</code> and <code>ROTATE_AND_CROP_90</code>.</p>\n\n @see ACAMERA_SCALER_ROTATE_AND_CROP"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_ROTATE_AND_CROP_MODES:
    acamera_metadata_tag = 851984;
#[doc = " <p>Whether a rotation-and-crop operation is applied to processed\n outputs from the camera.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_scaler_rotate_and_crop_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This control is primarily intended to help camera applications with no support for\n multi-window modes to work correctly on devices where multi-window scenarios are\n unavoidable, such as foldables or other devices with variable display geometry or more\n free-form window placement (such as laptops, which often place portrait-orientation apps\n in landscape with pillarboxing).</p>\n <p>If supported, the default value is <code>ROTATE_AND_CROP_AUTO</code>, which allows the camera API\n to enable backwards-compatibility support for applications that do not support resizing\n / multi-window modes, when the device is in fact in a multi-window mode (such as inset\n portrait on laptops, or on a foldable device in some fold states).  In addition,\n <code>ROTATE_AND_CROP_NONE</code> and <code>ROTATE_AND_CROP_90</code> will always be available if this control\n is supported by the device.  If not supported, devices API level 30 or higher will always\n list only <code>ROTATE_AND_CROP_NONE</code>.</p>\n <p>When <code>CROP_AUTO</code> is in use, and the camera API activates backward-compatibility mode,\n several metadata fields will also be parsed differently to ensure that coordinates are\n correctly handled for features like drawing face detection boxes or passing in\n tap-to-focus coordinates.  The camera API will convert positions in the active array\n coordinate system to/from the cropped-and-rotated coordinate system to make the\n operation transparent for applications.  The following controls are affected:</p>\n <ul>\n <li>ACAMERA_CONTROL_AE_REGIONS</li>\n <li>ACAMERA_CONTROL_AF_REGIONS</li>\n <li>ACAMERA_CONTROL_AWB_REGIONS</li>\n <li>android.statistics.faces</li>\n </ul>\n <p>Capture results will contain the actual value selected by the API;\n <code>ROTATE_AND_CROP_AUTO</code> will never be seen in a capture result.</p>\n <p>Applications can also select their preferred cropping mode, either to opt out of the\n backwards-compatibility treatment, or to use the cropping feature themselves as needed.\n In this case, no coordinate translation will be done automatically, and all controls\n will continue to use the normal active array coordinates.</p>\n <p>Cropping and rotating is done after the application of digital zoom (via either\n ACAMERA_SCALER_CROP_REGION or ACAMERA_CONTROL_ZOOM_RATIO), but before each individual\n output is further cropped and scaled. It only affects processed outputs such as\n YUV, PRIVATE, and JPEG.  It has no effect on RAW outputs.</p>\n <p>When <code>CROP_90</code> or <code>CROP_270</code> are selected, there is a significant loss to the field of\n view. For example, with a 4:3 aspect ratio output of 1600x1200, <code>CROP_90</code> will still\n produce 1600x1200 output, but these buffers are cropped from a vertical 3:4 slice at the\n center of the 4:3 area, then rotated to be 4:3, and then upscaled to 1600x1200.  Only\n 56.25% of the original FOV is still visible.  In general, for an aspect ratio of <code>w:h</code>,\n the crop and rotate operation leaves <code>(h/w)^2</code> of the field of view visible. For 16:9,\n this is ~31.6%.</p>\n <p>As a visual example, the figure below shows the effect of <code>ROTATE_AND_CROP_90</code> on the\n outputs for the following parameters:</p>\n <ul>\n <li>Sensor active array: <code>2000x1500</code></li>\n <li>Crop region: top-left: <code>(500, 375)</code>, size: <code>(1000, 750)</code> (4:3 aspect ratio)</li>\n <li>Output streams: YUV <code>640x480</code> and YUV <code>1280x720</code></li>\n <li><code>ROTATE_AND_CROP_90</code></li>\n </ul>\n <p><img alt=\"Effect of ROTATE_AND_CROP_90\" src=\"../images/camera2/metadata/android.scaler.rotateAndCrop/crop-region-rotate-90-43-ratio.png\" /></p>\n <p>With these settings, the regions of the active array covered by the output streams are:</p>\n <ul>\n <li>640x480 stream crop: top-left: <code>(219, 375)</code>, size: <code>(562, 750)</code></li>\n <li>1280x720 stream crop: top-left: <code>(289, 375)</code>, size: <code>(422, 750)</code></li>\n </ul>\n <p>Since the buffers are rotated, the buffers as seen by the application are:</p>\n <ul>\n <li>640x480 stream: top-left: <code>(781, 375)</code> on active array, size: <code>(640, 480)</code>, downscaled 1.17x from sensor pixels</li>\n <li>1280x720 stream: top-left: <code>(711, 375)</code> on active array, size: <code>(1280, 720)</code>, upscaled 1.71x from sensor pixels</li>\n </ul>\n\n @see ACAMERA_CONTROL_AE_REGIONS\n @see ACAMERA_CONTROL_AF_REGIONS\n @see ACAMERA_CONTROL_AWB_REGIONS\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_SCALER_CROP_REGION"]
pub const acamera_metadata_tag_ACAMERA_SCALER_ROTATE_AND_CROP: acamera_metadata_tag = 851985;
#[doc = " <p>Default YUV/PRIVATE size to use for requesting secure image buffers.</p>\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This entry lists the default size supported in the secure camera mode. This entry is\n optional on devices support the SECURE_IMAGE_DATA capability. This entry will be null\n if the camera device does not list SECURE_IMAGE_DATA capability.</p>\n <p>When the key is present, only a PRIVATE/YUV output of the specified size is guaranteed\n to be supported by the camera HAL in the secure camera mode. Any other format or\n resolutions might not be supported. Use\n {@link ACameraDevice_isSessionConfigurationSupported }\n API to query if a secure session configuration is supported if the device supports this\n API.</p>\n <p>If this key returns null on a device with SECURE_IMAGE_DATA capability, the application\n can assume all output sizes listed in the\n {@link ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS }\n are supported.</p>"]
pub const acamera_metadata_tag_ACAMERA_SCALER_DEFAULT_SECURE_IMAGE_SIZE: acamera_metadata_tag =
    851986;
#[doc = " <p>The available multi-resolution stream configurations that this\n physical camera device supports\n (i.e. format, width, height, output/input stream).</p>\n\n <p>Type: int32[n*4] (acamera_metadata_enum_android_scaler_physical_camera_multi_resolution_stream_configurations_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This list contains a subset of the parent logical camera's multi-resolution stream\n configurations which belong to this physical camera, and it will advertise and will only\n advertise the maximum supported resolutions for a particular format.</p>\n <p>If this camera device isn't a physical camera device constituting a logical camera,\n but a standalone <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n camera, this field represents the multi-resolution input/output stream configurations of\n default mode and max resolution modes. The sizes will be the maximum resolution of a\n particular format for default mode and max resolution mode.</p>\n <p>This field will only be advertised if the device is a physical camera of a\n logical multi-camera device or an ultra high resolution sensor camera. For a logical\n multi-camera, the camera API will derive the logical cameras multi-resolution stream\n configurations from all physical cameras. For an ultra high resolution sensor camera, this\n is used directly as the cameras multi-resolution stream configurations.</p>"]
pub const acamera_metadata_tag_ACAMERA_SCALER_PHYSICAL_CAMERA_MULTI_RESOLUTION_STREAM_CONFIGURATIONS : acamera_metadata_tag = 851987 ;
#[doc = " <p>The available stream configurations that this\n camera device supports (i.e. format, width, height, output/input stream) for a\n CaptureRequest with ACAMERA_SENSOR_PIXEL_MODE set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int32[n*4] (acamera_metadata_enum_android_scaler_available_stream_configurations_maximum_resolution_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS, for configurations\n which are applicable when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n <p>Not all output formats may be supported in a configuration with\n an input stream of a particular format. For more details, see\n android.scaler.availableInputOutputFormatsMapMaximumResolution.</p>\n\n @see ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION:
    acamera_metadata_tag = 851988;
#[doc = " <p>This lists the minimum frame duration for each\n format/size combination when the camera device is sent a CaptureRequest with\n ACAMERA_SENSOR_PIXEL_MODE set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS, for configurations\n which are applicable when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n <p>When multiple streams are used in a request (if supported, when ACAMERA_SENSOR_PIXEL_MODE\n is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>), the\n minimum frame duration will be max(individual stream min durations).</p>\n <p>See ACAMERA_SENSOR_FRAME_DURATION and\n ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS_MAXIMUM_RESOLUTION for more details about\n calculating the max frame rate.</p>\n\n @see ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS\n @see ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS_MAXIMUM_RESOLUTION\n @see ACAMERA_SENSOR_FRAME_DURATION\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS_MAXIMUM_RESOLUTION:
    acamera_metadata_tag = 851989;
#[doc = " <p>This lists the maximum stall duration for each\n output format/size combination when CaptureRequests are submitted with\n ACAMERA_SENSOR_PIXEL_MODE set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a></p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS, for configurations\n which are applicable when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS_MAXIMUM_RESOLUTION:
    acamera_metadata_tag = 851990;
#[doc = " <p>Whether the camera device supports multi-resolution input or output streams</p>\n\n <p>Type: byte (acamera_metadata_enum_android_scaler_multi_resolution_stream_supported_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>A logical multi-camera or an ultra high resolution camera may support multi-resolution\n input or output streams. With multi-resolution output streams, the camera device is able\n to output different resolution images depending on the current active physical camera or\n pixel mode. With multi-resolution input streams, the camera device can reprocess images\n of different resolutions from different physical cameras or sensor pixel modes.</p>\n <p>When set to TRUE:</p>\n <ul>\n <li>For a logical multi-camera, the camera framework derives\n android.scaler.multiResolutionStreamConfigurationMap by combining the\n ACAMERA_SCALER_PHYSICAL_CAMERA_MULTI_RESOLUTION_STREAM_CONFIGURATIONS from its physical\n cameras.</li>\n <li>For an ultra-high resolution sensor camera, the camera framework directly copies\n the value of ACAMERA_SCALER_PHYSICAL_CAMERA_MULTI_RESOLUTION_STREAM_CONFIGURATIONS to\n android.scaler.multiResolutionStreamConfigurationMap.</li>\n </ul>\n\n @see ACAMERA_SCALER_PHYSICAL_CAMERA_MULTI_RESOLUTION_STREAM_CONFIGURATIONS"]
pub const acamera_metadata_tag_ACAMERA_SCALER_MULTI_RESOLUTION_STREAM_SUPPORTED:
    acamera_metadata_tag = 851992;
#[doc = " <p>The stream use cases supported by this camera device.</p>\n\n <p>Type: int64[n] (acamera_metadata_enum_android_scaler_available_stream_use_cases_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The stream use case indicates the purpose of a particular camera stream from\n the end-user perspective. Some examples of camera use cases are: preview stream for\n live viewfinder shown to the user, still capture for generating high quality photo\n capture, video record for encoding the camera output for the purpose of future playback,\n and video call for live realtime video conferencing.</p>\n <p>With this flag, the camera device can optimize the image processing pipeline\n parameters, such as tuning, sensor mode, and ISP settings, independent of\n the properties of the immediate camera output surface. For example, if the output\n surface is a SurfaceTexture, the stream use case flag can be used to indicate whether\n the camera frames eventually go to display, video encoder,\n still image capture, or all of them combined.</p>\n <p>The application sets the use case of a camera stream by calling\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/params/OutputConfiguration.html#setStreamUseCase\">OutputConfiguration#setStreamUseCase</a>.</p>\n <p>A camera device with\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#REQUEST_AVAILABLE_CAPABILITIES_STREAM_USE_CASE\">CameraCharacteristics#REQUEST_AVAILABLE_CAPABILITIES_STREAM_USE_CASE</a>\n capability must support the following stream use cases:</p>\n <ul>\n <li>DEFAULT</li>\n <li>PREVIEW</li>\n <li>STILL_CAPTURE</li>\n <li>VIDEO_RECORD</li>\n <li>PREVIEW_VIDEO_STILL</li>\n <li>VIDEO_CALL</li>\n </ul>\n <p>The guaranteed stream combinations related to stream use case for a camera device with\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#REQUEST_AVAILABLE_CAPABILITIES_STREAM_USE_CASE\">CameraCharacteristics#REQUEST_AVAILABLE_CAPABILITIES_STREAM_USE_CASE</a>\n capability is documented in the camera device\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraDevice.html#createCaptureSession\">guideline</a>. The\n application is strongly recommended to use one of the guaranteed stream combinations.\n If the application creates a session with a stream combination not in the guaranteed\n list, or with mixed DEFAULT and non-DEFAULT use cases within the same session,\n the camera device may ignore some stream use cases due to hardware constraints\n and implementation details.</p>\n <p>For stream combinations not covered by the stream use case mandatory lists, such as\n reprocessable session, constrained high speed session, or RAW stream combinations, the\n application should leave stream use cases within the session as DEFAULT.</p>"]
pub const acamera_metadata_tag_ACAMERA_SCALER_AVAILABLE_STREAM_USE_CASES: acamera_metadata_tag =
    851993;
#[doc = " <p>The stream use cases supported by this camera device.</p>\n\n <p>Type: int64[n] (acamera_metadata_enum_android_scaler_available_stream_use_cases_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The stream use case indicates the purpose of a particular camera stream from\n the end-user perspective. Some examples of camera use cases are: preview stream for\n live viewfinder shown to the user, still capture for generating high quality photo\n capture, video record for encoding the camera output for the purpose of future playback,\n and video call for live realtime video conferencing.</p>\n <p>With this flag, the camera device can optimize the image processing pipeline\n parameters, such as tuning, sensor mode, and ISP settings, independent of\n the properties of the immediate camera output surface. For example, if the output\n surface is a SurfaceTexture, the stream use case flag can be used to indicate whether\n the camera frames eventually go to display, video encoder,\n still image capture, or all of them combined.</p>\n <p>The application sets the use case of a camera stream by calling\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/params/OutputConfiguration.html#setStreamUseCase\">OutputConfiguration#setStreamUseCase</a>.</p>\n <p>A camera device with\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#REQUEST_AVAILABLE_CAPABILITIES_STREAM_USE_CASE\">CameraCharacteristics#REQUEST_AVAILABLE_CAPABILITIES_STREAM_USE_CASE</a>\n capability must support the following stream use cases:</p>\n <ul>\n <li>DEFAULT</li>\n <li>PREVIEW</li>\n <li>STILL_CAPTURE</li>\n <li>VIDEO_RECORD</li>\n <li>PREVIEW_VIDEO_STILL</li>\n <li>VIDEO_CALL</li>\n </ul>\n <p>The guaranteed stream combinations related to stream use case for a camera device with\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#REQUEST_AVAILABLE_CAPABILITIES_STREAM_USE_CASE\">CameraCharacteristics#REQUEST_AVAILABLE_CAPABILITIES_STREAM_USE_CASE</a>\n capability is documented in the camera device\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraDevice.html#createCaptureSession\">guideline</a>. The\n application is strongly recommended to use one of the guaranteed stream combinations.\n If the application creates a session with a stream combination not in the guaranteed\n list, or with mixed DEFAULT and non-DEFAULT use cases within the same session,\n the camera device may ignore some stream use cases due to hardware constraints\n and implementation details.</p>\n <p>For stream combinations not covered by the stream use case mandatory lists, such as\n reprocessable session, constrained high speed session, or RAW stream combinations, the\n application should leave stream use cases within the session as DEFAULT.</p>"]
pub const acamera_metadata_tag_ACAMERA_SCALER_END: acamera_metadata_tag = 851994;
#[doc = " <p>Duration each pixel is exposed to\n light.</p>\n\n <p>Type: int64</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>If the sensor can't expose this exact duration, it will shorten the\n duration exposed to the nearest possible value (rather than expose longer).\n The final exposure time used will be available in the output capture result.</p>\n <p>This control is only effective if ACAMERA_CONTROL_AE_MODE or ACAMERA_CONTROL_MODE is set to\n OFF; otherwise the auto-exposure algorithm will override this value.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_MODE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_EXPOSURE_TIME: acamera_metadata_tag = 917504;
#[doc = " <p>Duration from start of frame exposure to\n start of next frame exposure.</p>\n\n <p>Type: int64</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>The maximum frame rate that can be supported by a camera subsystem is\n a function of many factors:</p>\n <ul>\n <li>Requested resolutions of output image streams</li>\n <li>Availability of binning / skipping modes on the imager</li>\n <li>The bandwidth of the imager interface</li>\n <li>The bandwidth of the various ISP processing blocks</li>\n </ul>\n <p>Since these factors can vary greatly between different ISPs and\n sensors, the camera abstraction tries to represent the bandwidth\n restrictions with as simple a model as possible.</p>\n <p>The model presented has the following characteristics:</p>\n <ul>\n <li>The image sensor is always configured to output the smallest\n resolution possible given the application's requested output stream\n sizes.  The smallest resolution is defined as being at least as large\n as the largest requested output stream size; the camera pipeline must\n never digitally upsample sensor data when the crop region covers the\n whole sensor. In general, this means that if only small output stream\n resolutions are configured, the sensor can provide a higher frame\n rate.</li>\n <li>Since any request may use any or all the currently configured\n output streams, the sensor and ISP must be configured to support\n scaling a single capture to all the streams at the same time.  This\n means the camera pipeline must be ready to produce the largest\n requested output size without any delay.  Therefore, the overall\n frame rate of a given configured stream set is governed only by the\n largest requested stream resolution.</li>\n <li>Using more than one output stream in a request does not affect the\n frame duration.</li>\n <li>Certain format-streams may need to do additional background processing\n before data is consumed/produced by that stream. These processors\n can run concurrently to the rest of the camera pipeline, but\n cannot process more than 1 capture at a time.</li>\n </ul>\n <p>The necessary information for the application, given the model above, is provided via\n {@link ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS }.\n These are used to determine the maximum frame rate / minimum frame duration that is\n possible for a given stream configuration.</p>\n <p>Specifically, the application can use the following rules to\n determine the minimum frame duration it can request from the camera\n device:</p>\n <ol>\n <li>Let the set of currently configured input/output streams be called <code>S</code>.</li>\n <li>Find the minimum frame durations for each stream in <code>S</code>, by looking it up in {@link ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS }\n (with its respective size/format). Let this set of frame durations be called <code>F</code>.</li>\n <li>For any given request <code>R</code>, the minimum frame duration allowed for <code>R</code> is the maximum\n out of all values in <code>F</code>. Let the streams used in <code>R</code> be called <code>S_r</code>.</li>\n </ol>\n <p>If none of the streams in <code>S_r</code> have a stall time (listed in {@link ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS }\n using its respective size/format), then the frame duration in <code>F</code> determines the steady\n state frame rate that the application will get if it uses <code>R</code> as a repeating request. Let\n this special kind of request be called <code>Rsimple</code>.</p>\n <p>A repeating request <code>Rsimple</code> can be <em>occasionally</em> interleaved by a single capture of a\n new request <code>Rstall</code> (which has at least one in-use stream with a non-0 stall time) and if\n <code>Rstall</code> has the same minimum frame duration this will not cause a frame rate loss if all\n buffers from the previous <code>Rstall</code> have already been delivered.</p>\n <p>For more details about stalling, see {@link ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS }.</p>\n <p>This control is only effective if ACAMERA_CONTROL_AE_MODE or ACAMERA_CONTROL_MODE is set to\n OFF; otherwise the auto-exposure algorithm will override this value.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_MODE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_FRAME_DURATION: acamera_metadata_tag = 917505;
#[doc = " <p>The amount of gain applied to sensor data\n before processing.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>The sensitivity is the standard ISO sensitivity value,\n as defined in ISO 12232:2006.</p>\n <p>The sensitivity must be within ACAMERA_SENSOR_INFO_SENSITIVITY_RANGE, and\n if if it less than ACAMERA_SENSOR_MAX_ANALOG_SENSITIVITY, the camera device\n is guaranteed to use only analog amplification for applying the gain.</p>\n <p>If the camera device cannot apply the exact sensitivity\n requested, it will reduce the gain to the nearest supported\n value. The final sensitivity used will be available in the\n output capture result.</p>\n <p>This control is only effective if ACAMERA_CONTROL_AE_MODE or ACAMERA_CONTROL_MODE is set to\n OFF; otherwise the auto-exposure algorithm will override this value.</p>\n <p>Note that for devices supporting postRawSensitivityBoost, the total sensitivity applied\n to the final processed image is the combination of ACAMERA_SENSOR_SENSITIVITY and\n ACAMERA_CONTROL_POST_RAW_SENSITIVITY_BOOST. In case the application uses the sensor\n sensitivity from last capture result of an auto request for a manual request, in order\n to achieve the same brightness in the output image, the application should also\n set postRawSensitivityBoost.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_MODE\n @see ACAMERA_CONTROL_POST_RAW_SENSITIVITY_BOOST\n @see ACAMERA_SENSOR_INFO_SENSITIVITY_RANGE\n @see ACAMERA_SENSOR_MAX_ANALOG_SENSITIVITY\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_SENSITIVITY: acamera_metadata_tag = 917506;
#[doc = " <p>The standard reference illuminant used as the scene light source when\n calculating the ACAMERA_SENSOR_COLOR_TRANSFORM1,\n ACAMERA_SENSOR_CALIBRATION_TRANSFORM1, and\n ACAMERA_SENSOR_FORWARD_MATRIX1 matrices.</p>\n\n @see ACAMERA_SENSOR_CALIBRATION_TRANSFORM1\n @see ACAMERA_SENSOR_COLOR_TRANSFORM1\n @see ACAMERA_SENSOR_FORWARD_MATRIX1\n\n <p>Type: byte (acamera_metadata_enum_android_sensor_reference_illuminant1_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The values in this key correspond to the values defined for the\n EXIF LightSource tag. These illuminants are standard light sources\n that are often used calibrating camera devices.</p>\n <p>If this key is present, then ACAMERA_SENSOR_COLOR_TRANSFORM1,\n ACAMERA_SENSOR_CALIBRATION_TRANSFORM1, and\n ACAMERA_SENSOR_FORWARD_MATRIX1 will also be present.</p>\n <p>Some devices may choose to provide a second set of calibration\n information for improved quality, including\n ACAMERA_SENSOR_REFERENCE_ILLUMINANT2 and its corresponding matrices.</p>\n <p>Starting from Android Q, this key will not be present for a MONOCHROME camera, even if\n the camera device has RAW capability.</p>\n\n @see ACAMERA_SENSOR_CALIBRATION_TRANSFORM1\n @see ACAMERA_SENSOR_COLOR_TRANSFORM1\n @see ACAMERA_SENSOR_FORWARD_MATRIX1\n @see ACAMERA_SENSOR_REFERENCE_ILLUMINANT2"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1: acamera_metadata_tag = 917507;
#[doc = " <p>The standard reference illuminant used as the scene light source when\n calculating the ACAMERA_SENSOR_COLOR_TRANSFORM2,\n ACAMERA_SENSOR_CALIBRATION_TRANSFORM2, and\n ACAMERA_SENSOR_FORWARD_MATRIX2 matrices.</p>\n\n @see ACAMERA_SENSOR_CALIBRATION_TRANSFORM2\n @see ACAMERA_SENSOR_COLOR_TRANSFORM2\n @see ACAMERA_SENSOR_FORWARD_MATRIX2\n\n <p>Type: byte</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>See ACAMERA_SENSOR_REFERENCE_ILLUMINANT1 for more details.</p>\n <p>If this key is present, then ACAMERA_SENSOR_COLOR_TRANSFORM2,\n ACAMERA_SENSOR_CALIBRATION_TRANSFORM2, and\n ACAMERA_SENSOR_FORWARD_MATRIX2 will also be present.</p>\n <p>Starting from Android Q, this key will not be present for a MONOCHROME camera, even if\n the camera device has RAW capability.</p>\n\n @see ACAMERA_SENSOR_CALIBRATION_TRANSFORM2\n @see ACAMERA_SENSOR_COLOR_TRANSFORM2\n @see ACAMERA_SENSOR_FORWARD_MATRIX2\n @see ACAMERA_SENSOR_REFERENCE_ILLUMINANT1"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_REFERENCE_ILLUMINANT2: acamera_metadata_tag = 917508;
#[doc = " <p>A per-device calibration transform matrix that maps from the\n reference sensor colorspace to the actual device sensor colorspace.</p>\n\n <p>Type: rational[3*3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This matrix is used to correct for per-device variations in the\n sensor colorspace, and is used for processing raw buffer data.</p>\n <p>The matrix is expressed as a 3x3 matrix in row-major-order, and\n contains a per-device calibration transform that maps colors\n from reference sensor color space (i.e. the \"golden module\"\n colorspace) into this camera device's native sensor color\n space under the first reference illuminant\n (ACAMERA_SENSOR_REFERENCE_ILLUMINANT1).</p>\n <p>Starting from Android Q, this key will not be present for a MONOCHROME camera, even if\n the camera device has RAW capability.</p>\n\n @see ACAMERA_SENSOR_REFERENCE_ILLUMINANT1"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_CALIBRATION_TRANSFORM1: acamera_metadata_tag = 917509;
#[doc = " <p>A per-device calibration transform matrix that maps from the\n reference sensor colorspace to the actual device sensor colorspace\n (this is the colorspace of the raw buffer data).</p>\n\n <p>Type: rational[3*3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This matrix is used to correct for per-device variations in the\n sensor colorspace, and is used for processing raw buffer data.</p>\n <p>The matrix is expressed as a 3x3 matrix in row-major-order, and\n contains a per-device calibration transform that maps colors\n from reference sensor color space (i.e. the \"golden module\"\n colorspace) into this camera device's native sensor color\n space under the second reference illuminant\n (ACAMERA_SENSOR_REFERENCE_ILLUMINANT2).</p>\n <p>This matrix will only be present if the second reference\n illuminant is present.</p>\n <p>Starting from Android Q, this key will not be present for a MONOCHROME camera, even if\n the camera device has RAW capability.</p>\n\n @see ACAMERA_SENSOR_REFERENCE_ILLUMINANT2"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_CALIBRATION_TRANSFORM2: acamera_metadata_tag = 917510;
#[doc = " <p>A matrix that transforms color values from CIE XYZ color space to\n reference sensor color space.</p>\n\n <p>Type: rational[3*3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This matrix is used to convert from the standard CIE XYZ color\n space to the reference sensor colorspace, and is used when processing\n raw buffer data.</p>\n <p>The matrix is expressed as a 3x3 matrix in row-major-order, and\n contains a color transform matrix that maps colors from the CIE\n XYZ color space to the reference sensor color space (i.e. the\n \"golden module\" colorspace) under the first reference illuminant\n (ACAMERA_SENSOR_REFERENCE_ILLUMINANT1).</p>\n <p>The white points chosen in both the reference sensor color space\n and the CIE XYZ colorspace when calculating this transform will\n match the standard white point for the first reference illuminant\n (i.e. no chromatic adaptation will be applied by this transform).</p>\n <p>Starting from Android Q, this key will not be present for a MONOCHROME camera, even if\n the camera device has RAW capability.</p>\n\n @see ACAMERA_SENSOR_REFERENCE_ILLUMINANT1"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_COLOR_TRANSFORM1: acamera_metadata_tag = 917511;
#[doc = " <p>A matrix that transforms color values from CIE XYZ color space to\n reference sensor color space.</p>\n\n <p>Type: rational[3*3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This matrix is used to convert from the standard CIE XYZ color\n space to the reference sensor colorspace, and is used when processing\n raw buffer data.</p>\n <p>The matrix is expressed as a 3x3 matrix in row-major-order, and\n contains a color transform matrix that maps colors from the CIE\n XYZ color space to the reference sensor color space (i.e. the\n \"golden module\" colorspace) under the second reference illuminant\n (ACAMERA_SENSOR_REFERENCE_ILLUMINANT2).</p>\n <p>The white points chosen in both the reference sensor color space\n and the CIE XYZ colorspace when calculating this transform will\n match the standard white point for the second reference illuminant\n (i.e. no chromatic adaptation will be applied by this transform).</p>\n <p>This matrix will only be present if the second reference\n illuminant is present.</p>\n <p>Starting from Android Q, this key will not be present for a MONOCHROME camera, even if\n the camera device has RAW capability.</p>\n\n @see ACAMERA_SENSOR_REFERENCE_ILLUMINANT2"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_COLOR_TRANSFORM2: acamera_metadata_tag = 917512;
#[doc = " <p>A matrix that transforms white balanced camera colors from the reference\n sensor colorspace to the CIE XYZ colorspace with a D50 whitepoint.</p>\n\n <p>Type: rational[3*3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This matrix is used to convert to the standard CIE XYZ colorspace, and\n is used when processing raw buffer data.</p>\n <p>This matrix is expressed as a 3x3 matrix in row-major-order, and contains\n a color transform matrix that maps white balanced colors from the\n reference sensor color space to the CIE XYZ color space with a D50 white\n point.</p>\n <p>Under the first reference illuminant (ACAMERA_SENSOR_REFERENCE_ILLUMINANT1)\n this matrix is chosen so that the standard white point for this reference\n illuminant in the reference sensor colorspace is mapped to D50 in the\n CIE XYZ colorspace.</p>\n <p>Starting from Android Q, this key will not be present for a MONOCHROME camera, even if\n the camera device has RAW capability.</p>\n\n @see ACAMERA_SENSOR_REFERENCE_ILLUMINANT1"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_FORWARD_MATRIX1: acamera_metadata_tag = 917513;
#[doc = " <p>A matrix that transforms white balanced camera colors from the reference\n sensor colorspace to the CIE XYZ colorspace with a D50 whitepoint.</p>\n\n <p>Type: rational[3*3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This matrix is used to convert to the standard CIE XYZ colorspace, and\n is used when processing raw buffer data.</p>\n <p>This matrix is expressed as a 3x3 matrix in row-major-order, and contains\n a color transform matrix that maps white balanced colors from the\n reference sensor color space to the CIE XYZ color space with a D50 white\n point.</p>\n <p>Under the second reference illuminant (ACAMERA_SENSOR_REFERENCE_ILLUMINANT2)\n this matrix is chosen so that the standard white point for this reference\n illuminant in the reference sensor colorspace is mapped to D50 in the\n CIE XYZ colorspace.</p>\n <p>This matrix will only be present if the second reference\n illuminant is present.</p>\n <p>Starting from Android Q, this key will not be present for a MONOCHROME camera, even if\n the camera device has RAW capability.</p>\n\n @see ACAMERA_SENSOR_REFERENCE_ILLUMINANT2"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_FORWARD_MATRIX2: acamera_metadata_tag = 917514;
#[doc = " <p>A fixed black level offset for each of the color filter arrangement\n (CFA) mosaic channels.</p>\n\n <p>Type: int32[4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This key specifies the zero light value for each of the CFA mosaic\n channels in the camera sensor.  The maximal value output by the\n sensor is represented by the value in ACAMERA_SENSOR_INFO_WHITE_LEVEL.</p>\n <p>The values are given in the same order as channels listed for the CFA\n layout key (see ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT), i.e. the\n nth value given corresponds to the black level offset for the nth\n color channel listed in the CFA.</p>\n <p>The black level values of captured images may vary for different\n capture settings (e.g., ACAMERA_SENSOR_SENSITIVITY). This key\n represents a coarse approximation for such case. It is recommended to\n use ACAMERA_SENSOR_DYNAMIC_BLACK_LEVEL or use pixels from\n ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS directly for captures when\n supported by the camera device, which provides more accurate black\n level values. For raw capture in particular, it is recommended to use\n pixels from ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS to calculate black\n level values for each frame.</p>\n <p>For a MONOCHROME camera device, all of the 2x2 channels must have the same values.</p>\n\n @see ACAMERA_SENSOR_DYNAMIC_BLACK_LEVEL\n @see ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT\n @see ACAMERA_SENSOR_INFO_WHITE_LEVEL\n @see ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_BLACK_LEVEL_PATTERN: acamera_metadata_tag = 917516;
#[doc = " <p>Maximum sensitivity that is implemented\n purely through analog gain.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>For ACAMERA_SENSOR_SENSITIVITY values less than or\n equal to this, all applied gain must be analog. For\n values above this, the gain applied can be a mix of analog and\n digital.</p>\n\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_MAX_ANALOG_SENSITIVITY: acamera_metadata_tag = 917517;
#[doc = " <p>Clockwise angle through which the output image needs to be rotated to be\n upright on the device screen in its native orientation.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Also defines the direction of rolling shutter readout, which is from top to bottom in\n the sensor's coordinate system.</p>\n <p>Starting with Android API level 32, camera clients that query the orientation via\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#get\">CameraCharacteristics#get</a> on foldable devices which\n include logical cameras can receive a value that can dynamically change depending on the\n device/fold state.\n Clients are advised to not cache or store the orientation value of such logical sensors.\n In case repeated queries to CameraCharacteristics are not preferred, then clients can\n also access the entire mapping from device state to sensor orientation in\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/params/DeviceStateSensorOrientationMap.html\">DeviceStateSensorOrientationMap</a>.\n Do note that a dynamically changing sensor orientation value in camera characteristics\n will not be the best way to establish the orientation per frame. Clients that want to\n know the sensor orientation of a particular captured frame should query the\n ACAMERA_LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID from the corresponding capture result and\n check the respective physical camera orientation.</p>\n <p>Native camera clients must query ACAMERA_INFO_DEVICE_STATE_ORIENTATIONS for the mapping\n between device state and camera sensor orientation. Dynamic updates to the sensor\n orientation are not supported in this code path.</p>\n\n @see ACAMERA_INFO_DEVICE_STATE_ORIENTATIONS\n @see ACAMERA_LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_ORIENTATION: acamera_metadata_tag = 917518;
#[doc = " <p>Time at start of exposure of first\n row of the image sensor active array, in nanoseconds.</p>\n\n <p>Type: int64</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The timestamps are also included in all image\n buffers produced for the same capture, and will be identical\n on all the outputs.</p>\n <p>When ACAMERA_SENSOR_INFO_TIMESTAMP_SOURCE <code>==</code> UNKNOWN,\n the timestamps measure time since an unspecified starting point,\n and are monotonically increasing. They can be compared with the\n timestamps for other captures from the same camera device, but are\n not guaranteed to be comparable to any other time source.</p>\n <p>When ACAMERA_SENSOR_INFO_TIMESTAMP_SOURCE <code>==</code> REALTIME, the\n timestamps measure time in the same timebase as <a href=\"https://developer.android.com/reference/android/os/SystemClock.html#elapsedRealtimeNanos\">SystemClock#elapsedRealtimeNanos</a>, and they can\n be compared to other timestamps from other subsystems that\n are using that base.</p>\n <p>For reprocessing, the timestamp will match the start of exposure of\n the input image, i.e. <a href=\"https://developer.android.com/reference/CaptureResult.html#SENSOR_TIMESTAMP\">the\n timestamp</a> in the TotalCaptureResult that was used to create the\n reprocess capture request.</p>\n\n @see ACAMERA_SENSOR_INFO_TIMESTAMP_SOURCE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_TIMESTAMP: acamera_metadata_tag = 917520;
#[doc = " <p>The estimated camera neutral color in the native sensor colorspace at\n the time of capture.</p>\n\n <p>Type: rational[3]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>This value gives the neutral color point encoded as an RGB value in the\n native sensor color space.  The neutral color point indicates the\n currently estimated white point of the scene illumination.  It can be\n used to interpolate between the provided color transforms when\n processing raw sensor data.</p>\n <p>The order of the values is R, G, B; where R is in the lowest index.</p>\n <p>Starting from Android Q, this key will not be present for a MONOCHROME camera, even if\n the camera device has RAW capability.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_NEUTRAL_COLOR_POINT: acamera_metadata_tag = 917522;
#[doc = " <p>Noise model coefficients for each CFA mosaic channel.</p>\n\n <p>Type: double[2*CFA Channels]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>This key contains two noise model coefficients for each CFA channel\n corresponding to the sensor amplification (S) and sensor readout\n noise (O).  These are given as pairs of coefficients for each channel\n in the same order as channels listed for the CFA layout key\n (see ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT).  This is\n represented as an array of Pair&lt;Double, Double&gt;, where\n the first member of the Pair at index n is the S coefficient and the\n second member is the O coefficient for the nth color channel in the CFA.</p>\n <p>These coefficients are used in a two parameter noise model to describe\n the amount of noise present in the image for each CFA channel.  The\n noise model used here is:</p>\n <p>N(x) = sqrt(Sx + O)</p>\n <p>Where x represents the recorded signal of a CFA channel normalized to\n the range [0, 1], and S and O are the noise model coefficients for\n that channel.</p>\n <p>A more detailed description of the noise model can be found in the\n Adobe DNG specification for the NoiseProfile tag.</p>\n <p>For a MONOCHROME camera, there is only one color channel. So the noise model coefficients\n will only contain one S and one O.</p>\n\n @see ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_NOISE_PROFILE: acamera_metadata_tag = 917523;
#[doc = " <p>The worst-case divergence between Bayer green channels.</p>\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>This value is an estimate of the worst case split between the\n Bayer green channels in the red and blue rows in the sensor color\n filter array.</p>\n <p>The green split is calculated as follows:</p>\n <ol>\n <li>A 5x5 pixel (or larger) window W within the active sensor array is\n chosen. The term 'pixel' here is taken to mean a group of 4 Bayer\n mosaic channels (R, Gr, Gb, B).  The location and size of the window\n chosen is implementation defined, and should be chosen to provide a\n green split estimate that is both representative of the entire image\n for this camera sensor, and can be calculated quickly.</li>\n <li>The arithmetic mean of the green channels from the red\n rows (mean_Gr) within W is computed.</li>\n <li>The arithmetic mean of the green channels from the blue\n rows (mean_Gb) within W is computed.</li>\n <li>The maximum ratio R of the two means is computed as follows:\n <code>R = max((mean_Gr + 1)/(mean_Gb + 1), (mean_Gb + 1)/(mean_Gr + 1))</code></li>\n </ol>\n <p>The ratio R is the green split divergence reported for this property,\n which represents how much the green channels differ in the mosaic\n pattern.  This value is typically used to determine the treatment of\n the green mosaic channels when demosaicing.</p>\n <p>The green split value can be roughly interpreted as follows:</p>\n <ul>\n <li>R &lt; 1.03 is a negligible split (&lt;3% divergence).</li>\n <li>1.20 &lt;= R &gt;= 1.03 will require some software\n correction to avoid demosaic errors (3-20% divergence).</li>\n <li>R &gt; 1.20 will require strong software correction to produce\n a usable image (&gt;20% divergence).</li>\n </ul>\n <p>Starting from Android Q, this key will not be present for a MONOCHROME camera, even if\n the camera device has RAW capability.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_GREEN_SPLIT: acamera_metadata_tag = 917526;
#[doc = " <p>A pixel <code>[R, G_even, G_odd, B]</code> that supplies the test pattern\n when ACAMERA_SENSOR_TEST_PATTERN_MODE is SOLID_COLOR.</p>\n\n @see ACAMERA_SENSOR_TEST_PATTERN_MODE\n\n <p>Type: int32[4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Each color channel is treated as an unsigned 32-bit integer.\n The camera device then uses the most significant X bits\n that correspond to how many bits are in its Bayer raw sensor\n output.</p>\n <p>For example, a sensor with RAW10 Bayer output would use the\n 10 most significant bits from each color channel.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_TEST_PATTERN_DATA: acamera_metadata_tag = 917527;
#[doc = " <p>When enabled, the sensor sends a test pattern instead of\n doing a real exposure from the camera.</p>\n\n <p>Type: int32 (acamera_metadata_enum_android_sensor_test_pattern_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>When a test pattern is enabled, all manual sensor controls specified\n by ACAMERA_SENSOR_* will be ignored. All other controls should\n work as normal.</p>\n <p>For example, if manual flash is enabled, flash firing should still\n occur (and that the test pattern remain unmodified, since the flash\n would not actually affect it).</p>\n <p>Defaults to OFF.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_TEST_PATTERN_MODE: acamera_metadata_tag = 917528;
#[doc = " <p>List of sensor test pattern modes for ACAMERA_SENSOR_TEST_PATTERN_MODE\n supported by this camera device.</p>\n\n @see ACAMERA_SENSOR_TEST_PATTERN_MODE\n\n <p>Type: int32[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Defaults to OFF, and always includes OFF if defined.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_AVAILABLE_TEST_PATTERN_MODES: acamera_metadata_tag =
    917529;
#[doc = " <p>Duration between the start of exposure for the first row of the image sensor,\n and the start of exposure for one past the last row of the image sensor.</p>\n\n <p>Type: int64</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>This is the exposure time skew between the first and <code>(last+1)</code> row exposure start times. The\n first row and the last row are the first and last rows inside of the\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.</p>\n <p>For typical camera sensors that use rolling shutters, this is also equivalent to the frame\n readout time.</p>\n <p>If the image sensor is operating in a binned or cropped mode due to the current output\n target resolutions, it's possible this skew is reported to be larger than the exposure\n time, for example, since it is based on the full array even if a partial array is read\n out. Be sure to scale the number to cover the section of the sensor actually being used\n for the outputs you care about. So if your output covers N rows of the active array of\n height H, scale this value by N/H to get the total skew for that viewport.</p>\n <p><em>Note:</em> Prior to Android 11, this field was described as measuring duration from\n first to last row of the image sensor, which is not equal to the frame readout time for a\n rolling shutter sensor. Implementations generally reported the latter value, so to resolve\n the inconsistency, the description has been updated to range from (first, last+1) row\n exposure start, instead.</p>\n\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_ROLLING_SHUTTER_SKEW: acamera_metadata_tag = 917530;
#[doc = " <p>List of disjoint rectangles indicating the sensor\n optically shielded black pixel regions.</p>\n\n <p>Type: int32[4*num_regions]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>In most camera sensors, the active array is surrounded by some\n optically shielded pixel areas. By blocking light, these pixels\n provides a reliable black reference for black level compensation\n in active array region.</p>\n <p>This key provides a list of disjoint rectangles specifying the\n regions of optically shielded (with metal shield) black pixel\n regions if the camera device is capable of reading out these black\n pixels in the output raw images. In comparison to the fixed black\n level values reported by ACAMERA_SENSOR_BLACK_LEVEL_PATTERN, this key\n may provide a more accurate way for the application to calculate\n black level of each captured raw images.</p>\n <p>When this key is reported, the ACAMERA_SENSOR_DYNAMIC_BLACK_LEVEL and\n ACAMERA_SENSOR_DYNAMIC_WHITE_LEVEL will also be reported.</p>\n <p>The data representation is <code>int[4]</code>, which maps to <code>(left, top, width, height)</code>.</p>\n\n @see ACAMERA_SENSOR_BLACK_LEVEL_PATTERN\n @see ACAMERA_SENSOR_DYNAMIC_BLACK_LEVEL\n @see ACAMERA_SENSOR_DYNAMIC_WHITE_LEVEL"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS: acamera_metadata_tag = 917531;
#[doc = " <p>A per-frame dynamic black level offset for each of the color filter\n arrangement (CFA) mosaic channels.</p>\n\n <p>Type: float[4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>Camera sensor black levels may vary dramatically for different\n capture settings (e.g. ACAMERA_SENSOR_SENSITIVITY). The fixed black\n level reported by ACAMERA_SENSOR_BLACK_LEVEL_PATTERN may be too\n inaccurate to represent the actual value on a per-frame basis. The\n camera device internal pipeline relies on reliable black level values\n to process the raw images appropriately. To get the best image\n quality, the camera device may choose to estimate the per frame black\n level values either based on optically shielded black regions\n (ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS) or its internal model.</p>\n <p>This key reports the camera device estimated per-frame zero light\n value for each of the CFA mosaic channels in the camera sensor. The\n ACAMERA_SENSOR_BLACK_LEVEL_PATTERN may only represent a coarse\n approximation of the actual black level values. This value is the\n black level used in camera device internal image processing pipeline\n and generally more accurate than the fixed black level values.\n However, since they are estimated values by the camera device, they\n may not be as accurate as the black level values calculated from the\n optical black pixels reported by ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS.</p>\n <p>The values are given in the same order as channels listed for the CFA\n layout key (see ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT), i.e. the\n nth value given corresponds to the black level offset for the nth\n color channel listed in the CFA.</p>\n <p>For a MONOCHROME camera, all of the 2x2 channels must have the same values.</p>\n <p>This key will be available if ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS is available or the\n camera device advertises this key via {@link ACAMERA_REQUEST_AVAILABLE_RESULT_KEYS }.</p>\n\n @see ACAMERA_SENSOR_BLACK_LEVEL_PATTERN\n @see ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT\n @see ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_DYNAMIC_BLACK_LEVEL: acamera_metadata_tag = 917532;
#[doc = " <p>Maximum raw value output by sensor for this frame.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>Since the ACAMERA_SENSOR_BLACK_LEVEL_PATTERN may change for different\n capture settings (e.g., ACAMERA_SENSOR_SENSITIVITY), the white\n level will change accordingly. This key is similar to\n ACAMERA_SENSOR_INFO_WHITE_LEVEL, but specifies the camera device\n estimated white level for each frame.</p>\n <p>This key will be available if ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS is\n available or the camera device advertises this key via\n {@link ACAMERA_REQUEST_AVAILABLE_RESULT_KEYS }.</p>\n\n @see ACAMERA_SENSOR_BLACK_LEVEL_PATTERN\n @see ACAMERA_SENSOR_INFO_WHITE_LEVEL\n @see ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_DYNAMIC_WHITE_LEVEL: acamera_metadata_tag = 917533;
#[doc = " <p>Switches sensor pixel mode between maximum resolution mode and default mode.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_sensor_pixel_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>This key controls whether the camera sensor operates in\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>\n mode or not. By default, all camera devices operate in\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_DEFAULT\">CameraMetadata#SENSOR_PIXEL_MODE_DEFAULT</a> mode.\n When operating in\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_DEFAULT\">CameraMetadata#SENSOR_PIXEL_MODE_DEFAULT</a> mode, sensors\n with <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability would typically perform pixel binning in order to improve low light\n performance, noise reduction etc. However, in\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>\n mode (supported only\n by <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n sensors), sensors typically operate in unbinned mode allowing for a larger image size.\n The stream configurations supported in\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>\n mode are also different from those of\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_DEFAULT\">CameraMetadata#SENSOR_PIXEL_MODE_DEFAULT</a> mode.\n They can be queried through\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#get\">CameraCharacteristics#get</a> with\n <a href=\"https://developer.android.com/reference/CameraCharacteristics.html#SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION)\">CameraCharacteristics#SCALER_STREAM_CONFIGURATION_MAP_MAXIMUM_RESOLUTION)</a>.\n Unless reported by both\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/params/StreamConfigurationMap.html\">StreamConfigurationMap</a>s, the outputs from\n <code>android.scaler.streamConfigurationMapMaximumResolution</code> and\n <code>android.scaler.streamConfigurationMap</code>\n must not be mixed in the same CaptureRequest. In other words, these outputs are\n exclusive to each other.\n This key does not need to be set for reprocess requests.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_PIXEL_MODE: acamera_metadata_tag = 917536;
#[doc = " <p>Whether <code>RAW</code> images requested have their bayer pattern as described by\n ACAMERA_SENSOR_INFO_BINNING_FACTOR.</p>\n\n @see ACAMERA_SENSOR_INFO_BINNING_FACTOR\n\n <p>Type: byte (acamera_metadata_enum_android_sensor_raw_binning_factor_used_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>This key will only be present in devices advertising the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability which also advertise <code>REMOSAIC_REPROCESSING</code> capability. On all other devices\n RAW targets will have a regular bayer pattern.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_RAW_BINNING_FACTOR_USED: acamera_metadata_tag =
    917537;
#[doc = " <p>Whether <code>RAW</code> images requested have their bayer pattern as described by\n ACAMERA_SENSOR_INFO_BINNING_FACTOR.</p>\n\n @see ACAMERA_SENSOR_INFO_BINNING_FACTOR\n\n <p>Type: byte (acamera_metadata_enum_android_sensor_raw_binning_factor_used_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>This key will only be present in devices advertising the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability which also advertise <code>REMOSAIC_REPROCESSING</code> capability. On all other devices\n RAW targets will have a regular bayer pattern.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_END: acamera_metadata_tag = 917538;
#[doc = " <p>The area of the image sensor which corresponds to active pixels after any geometric\n distortion correction has been applied.</p>\n\n <p>Type: int32[4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This is the rectangle representing the size of the active region of the sensor (i.e.\n the region that actually receives light from the scene) after any geometric correction\n has been applied, and should be treated as the maximum size in pixels of any of the\n image output formats aside from the raw formats.</p>\n <p>This rectangle is defined relative to the full pixel array; (0,0) is the top-left of\n the full pixel array, and the size of the full pixel array is given by\n ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE.</p>\n <p>The coordinate system for most other keys that list pixel coordinates, including\n ACAMERA_SCALER_CROP_REGION, is defined relative to the active array rectangle given in\n this field, with <code>(0, 0)</code> being the top-left of this rectangle.</p>\n <p>The active array may be smaller than the full pixel array, since the full array may\n include black calibration pixels or other inactive regions.</p>\n <p>For devices that do not support ACAMERA_DISTORTION_CORRECTION_MODE control, the active\n array must be the same as ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE.</p>\n <p>For devices that support ACAMERA_DISTORTION_CORRECTION_MODE control, the active array must\n be enclosed by ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE. The difference between\n pre-correction active array and active array accounts for scaling or cropping caused\n by lens geometric distortion correction.</p>\n <p>In general, application should always refer to active array size for controls like\n metering regions or crop region. Two exceptions are when the application is dealing with\n RAW image buffers (RAW_SENSOR, RAW10, RAW12 etc), or when application explicitly set\n ACAMERA_DISTORTION_CORRECTION_MODE to OFF. In these cases, application should refer\n to ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE.</p>\n <p>The data representation is <code>int[4]</code>, which maps to <code>(left, top, width, height)</code>.</p>\n\n @see ACAMERA_DISTORTION_CORRECTION_MODE\n @see ACAMERA_SCALER_CROP_REGION\n @see ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE: acamera_metadata_tag = 983040;
#[doc = " <p>Range of sensitivities for ACAMERA_SENSOR_SENSITIVITY supported by this\n camera device.</p>\n\n @see ACAMERA_SENSOR_SENSITIVITY\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The values are the standard ISO sensitivity values,\n as defined in ISO 12232:2006.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_SENSITIVITY_RANGE: acamera_metadata_tag = 983041;
#[doc = " <p>The arrangement of color filters on sensor;\n represents the colors in the top-left 2x2 section of\n the sensor, in reading order, for a Bayer camera, or the\n light spectrum it captures for MONOCHROME camera.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_sensor_info_color_filter_arrangement_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT: acamera_metadata_tag =
    983042;
#[doc = " <p>The range of image exposure times for ACAMERA_SENSOR_EXPOSURE_TIME supported\n by this camera device.</p>\n\n @see ACAMERA_SENSOR_EXPOSURE_TIME\n\n <p>Type: int64[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE: acamera_metadata_tag =
    983043;
#[doc = " <p>The maximum possible frame duration (minimum frame rate) for\n ACAMERA_SENSOR_FRAME_DURATION that is supported this camera device.</p>\n\n @see ACAMERA_SENSOR_FRAME_DURATION\n\n <p>Type: int64</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Attempting to use frame durations beyond the maximum will result in the frame\n duration being clipped to the maximum. See that control for a full definition of frame\n durations.</p>\n <p>Refer to {@link ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS }\n for the minimum frame duration values.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_MAX_FRAME_DURATION: acamera_metadata_tag =
    983044;
#[doc = " <p>The physical dimensions of the full pixel\n array.</p>\n\n <p>Type: float[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This is the physical size of the sensor pixel\n array defined by ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE.</p>\n\n @see ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_PHYSICAL_SIZE: acamera_metadata_tag = 983045;
#[doc = " <p>Dimensions of the full pixel array, possibly\n including black calibration pixels.</p>\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The pixel count of the full pixel array of the image sensor, which covers\n ACAMERA_SENSOR_INFO_PHYSICAL_SIZE area.  This represents the full pixel dimensions of\n the raw buffers produced by this sensor.</p>\n <p>If a camera device supports raw sensor formats, either this or\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE is the maximum dimensions for the raw\n output formats listed in {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS }\n (this depends on whether or not the image sensor returns buffers containing pixels that\n are not part of the active array region for blacklevel calibration or other purposes).</p>\n <p>Some parts of the full pixel array may not receive light from the scene,\n or be otherwise inactive.  The ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE key\n defines the rectangle of active pixels that will be included in processed image\n formats.</p>\n\n @see ACAMERA_SENSOR_INFO_PHYSICAL_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE: acamera_metadata_tag = 983046;
#[doc = " <p>Maximum raw value output by sensor.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This specifies the fully-saturated encoding level for the raw\n sample values from the sensor.  This is typically caused by the\n sensor becoming highly non-linear or clipping. The minimum for\n each channel is specified by the offset in the\n ACAMERA_SENSOR_BLACK_LEVEL_PATTERN key.</p>\n <p>The white level is typically determined either by sensor bit depth\n (8-14 bits is expected), or by the point where the sensor response\n becomes too non-linear to be useful.  The default value for this is\n maximum representable value for a 16-bit raw sample (2^16 - 1).</p>\n <p>The white level values of captured images may vary for different\n capture settings (e.g., ACAMERA_SENSOR_SENSITIVITY). This key\n represents a coarse approximation for such case. It is recommended\n to use ACAMERA_SENSOR_DYNAMIC_WHITE_LEVEL for captures when supported\n by the camera device, which provides more accurate white level values.</p>\n\n @see ACAMERA_SENSOR_BLACK_LEVEL_PATTERN\n @see ACAMERA_SENSOR_DYNAMIC_WHITE_LEVEL\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_WHITE_LEVEL: acamera_metadata_tag = 983047;
#[doc = " <p>The time base source for sensor capture start timestamps.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_sensor_info_timestamp_source_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The timestamps provided for captures are always in nanoseconds and monotonic, but\n may not based on a time source that can be compared to other system time sources.</p>\n <p>This characteristic defines the source for the timestamps, and therefore whether they\n can be compared against other system time sources/timestamps.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_TIMESTAMP_SOURCE: acamera_metadata_tag = 983048;
#[doc = " <p>Whether the RAW images output from this camera device are subject to\n lens shading correction.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_sensor_info_lens_shading_applied_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If TRUE, all images produced by the camera device in the RAW image formats will\n have lens shading correction already applied to it. If FALSE, the images will\n not be adjusted for lens shading correction.\n See android.request.maxNumOutputRaw for a list of RAW image formats.</p>\n <p>This key will be <code>null</code> for all devices do not report this information.\n Devices with RAW capability will always report this information in this key.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_LENS_SHADING_APPLIED: acamera_metadata_tag =
    983049;
#[doc = " <p>The area of the image sensor which corresponds to active pixels prior to the\n application of any geometric distortion correction.</p>\n\n <p>Type: int32[4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This is the rectangle representing the size of the active region of the sensor (i.e.\n the region that actually receives light from the scene) before any geometric correction\n has been applied, and should be treated as the active region rectangle for any of the\n raw formats.  All metadata associated with raw processing (e.g. the lens shading\n correction map, and radial distortion fields) treats the top, left of this rectangle as\n the origin, (0,0).</p>\n <p>The size of this region determines the maximum field of view and the maximum number of\n pixels that an image from this sensor can contain, prior to the application of\n geometric distortion correction. The effective maximum pixel dimensions of a\n post-distortion-corrected image is given by the ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n field, and the effective maximum field of view for a post-distortion-corrected image\n can be calculated by applying the geometric distortion correction fields to this\n rectangle, and cropping to the rectangle given in ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.</p>\n <p>E.g. to calculate position of a pixel, (x,y), in a processed YUV output image with the\n dimensions in ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE given the position of a pixel,\n (x', y'), in the raw pixel array with dimensions given in\n ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE:</p>\n <ol>\n <li>Choose a pixel (x', y') within the active array region of the raw buffer given in\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE, otherwise this pixel is considered\n to be outside of the FOV, and will not be shown in the processed output image.</li>\n <li>Apply geometric distortion correction to get the post-distortion pixel coordinate,\n (x_i, y_i). When applying geometric correction metadata, note that metadata for raw\n buffers is defined relative to the top, left of the\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE rectangle.</li>\n <li>If the resulting corrected pixel coordinate is within the region given in\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, then the position of this pixel in the\n processed output image buffer is <code>(x_i - activeArray.left, y_i - activeArray.top)</code>,\n when the top, left coordinate of that buffer is treated as (0, 0).</li>\n </ol>\n <p>Thus, for pixel x',y' = (25, 25) on a sensor where ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE\n is (100,100), ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE is (10, 10, 100, 100),\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE is (20, 20, 80, 80), and the geometric distortion\n correction doesn't change the pixel coordinate, the resulting pixel selected in\n pixel coordinates would be x,y = (25, 25) relative to the top,left of the raw buffer\n with dimensions given in ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE, and would be (5, 5)\n relative to the top,left of post-processed YUV output buffer with dimensions given in\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.</p>\n <p>The currently supported fields that correct for geometric distortion are:</p>\n <ol>\n <li>ACAMERA_LENS_DISTORTION.</li>\n </ol>\n <p>If the camera device doesn't support geometric distortion correction, or all of the\n geometric distortion fields are no-ops, this rectangle will be the same as the\n post-distortion-corrected rectangle given in ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.</p>\n <p>This rectangle is defined relative to the full pixel array; (0,0) is the top-left of\n the full pixel array, and the size of the full pixel array is given by\n ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE.</p>\n <p>The pre-correction active array may be smaller than the full pixel array, since the\n full array may include black calibration pixels or other inactive regions.</p>\n <p>The data representation is <code>int[4]</code>, which maps to <code>(left, top, width, height)</code>.</p>\n\n @see ACAMERA_LENS_DISTORTION\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE:
    acamera_metadata_tag = 983050;
#[doc = " <p>The area of the image sensor which corresponds to active pixels after any geometric\n distortion correction has been applied, when the sensor runs in maximum resolution mode.</p>\n\n <p>Type: int32[4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, when ACAMERA_SENSOR_PIXEL_MODE\n is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.\n Refer to ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE for details, with sensor array related keys\n replaced with their\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>\n counterparts.\n This key will only be present for devices which advertise the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability.</p>\n <p>The data representation is <code>int[4]</code>, which maps to <code>(left, top, width, height)</code>.</p>\n\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION:
    acamera_metadata_tag = 983051;
#[doc = " <p>Dimensions of the full pixel array, possibly\n including black calibration pixels, when the sensor runs in maximum resolution mode.\n Analogous to ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE, when ACAMERA_SENSOR_PIXEL_MODE is\n set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The pixel count of the full pixel array of the image sensor, which covers\n ACAMERA_SENSOR_INFO_PHYSICAL_SIZE area. This represents the full pixel dimensions of\n the raw buffers produced by this sensor, when it runs in maximum resolution mode. That\n is, when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.\n This key will only be present for devices which advertise the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability.</p>\n\n @see ACAMERA_SENSOR_INFO_PHYSICAL_SIZE\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE_MAXIMUM_RESOLUTION:
    acamera_metadata_tag = 983052;
#[doc = " <p>The area of the image sensor which corresponds to active pixels prior to the\n application of any geometric distortion correction, when the sensor runs in maximum\n resolution mode. This key must be used for crop / metering regions, only when\n ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int32[4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE,\n when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.\n This key will only be present for devices which advertise the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability.</p>\n <p>The data representation is <code>int[4]</code>, which maps to <code>(left, top, width, height)</code>.</p>\n\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE_MAXIMUM_RESOLUTION : acamera_metadata_tag = 983053 ;
#[doc = " <p>Dimensions of the group of pixels which are under the same color filter.\n This specifies the width and height (pair of integers) of the group of pixels which fall\n under the same color filter for ULTRA_HIGH_RESOLUTION sensors.</p>\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Sensors can have pixels grouped together under the same color filter in order\n to improve various aspects of imaging such as noise reduction, low light\n performance etc. These groups can be of various sizes such as 2X2 (quad bayer),\n 3X3 (nona-bayer). This key specifies the length and width of the pixels grouped under\n the same color filter.</p>\n <p>This key will not be present if REMOSAIC_REPROCESSING is not supported, since RAW images\n will have a regular bayer pattern.</p>\n <p>This key will not be present for sensors which don't have the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_BINNING_FACTOR: acamera_metadata_tag = 983054;
#[doc = " <p>Dimensions of the group of pixels which are under the same color filter.\n This specifies the width and height (pair of integers) of the group of pixels which fall\n under the same color filter for ULTRA_HIGH_RESOLUTION sensors.</p>\n\n <p>Type: int32[2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Sensors can have pixels grouped together under the same color filter in order\n to improve various aspects of imaging such as noise reduction, low light\n performance etc. These groups can be of various sizes such as 2X2 (quad bayer),\n 3X3 (nona-bayer). This key specifies the length and width of the pixels grouped under\n the same color filter.</p>\n <p>This key will not be present if REMOSAIC_REPROCESSING is not supported, since RAW images\n will have a regular bayer pattern.</p>\n <p>This key will not be present for sensors which don't have the\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>\n capability.</p>"]
pub const acamera_metadata_tag_ACAMERA_SENSOR_INFO_END: acamera_metadata_tag = 983055;
#[doc = " <p>Quality of lens shading correction applied\n to the image data.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_shading_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>When set to OFF mode, no lens shading correction will be applied by the\n camera device, and an identity lens shading map data will be provided\n if <code>ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE == ON</code>. For example, for lens\n shading map with size of <code>[ 4, 3 ]</code>,\n the output android.statistics.lensShadingCorrectionMap for this case will be an identity\n map shown below:</p>\n <pre><code>[ 1.0, 1.0, 1.0, 1.0,  1.0, 1.0, 1.0, 1.0,\n  1.0, 1.0, 1.0, 1.0,  1.0, 1.0, 1.0, 1.0,\n  1.0, 1.0, 1.0, 1.0,  1.0, 1.0, 1.0, 1.0,\n  1.0, 1.0, 1.0, 1.0,  1.0, 1.0, 1.0, 1.0,\n  1.0, 1.0, 1.0, 1.0,  1.0, 1.0, 1.0, 1.0,\n  1.0, 1.0, 1.0, 1.0,  1.0, 1.0, 1.0, 1.0 ]\n </code></pre>\n <p>When set to other modes, lens shading correction will be applied by the camera\n device. Applications can request lens shading map data by setting\n ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE to ON, and then the camera device will provide lens\n shading map data in android.statistics.lensShadingCorrectionMap; the returned shading map\n data will be the one applied by the camera device for this capture request.</p>\n <p>The shading map data may depend on the auto-exposure (AE) and AWB statistics, therefore\n the reliability of the map data may be affected by the AE and AWB algorithms. When AE and\n AWB are in AUTO modes(ACAMERA_CONTROL_AE_MODE <code>!=</code> OFF and ACAMERA_CONTROL_AWB_MODE <code>!=</code>\n OFF), to get best results, it is recommended that the applications wait for the AE and AWB\n to be converged before using the returned shading map data.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_AWB_MODE\n @see ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE"]
pub const acamera_metadata_tag_ACAMERA_SHADING_MODE: acamera_metadata_tag = 1048576;
#[doc = " <p>List of lens shading modes for ACAMERA_SHADING_MODE that are supported by this camera device.</p>\n\n @see ACAMERA_SHADING_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This list contains lens shading modes that can be set for the camera device.\n Camera devices that support the MANUAL_POST_PROCESSING capability will always\n list OFF and FAST mode. This includes all FULL level devices.\n LEGACY devices will always only support FAST mode.</p>"]
pub const acamera_metadata_tag_ACAMERA_SHADING_AVAILABLE_MODES: acamera_metadata_tag = 1048578;
#[doc = " <p>List of lens shading modes for ACAMERA_SHADING_MODE that are supported by this camera device.</p>\n\n @see ACAMERA_SHADING_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This list contains lens shading modes that can be set for the camera device.\n Camera devices that support the MANUAL_POST_PROCESSING capability will always\n list OFF and FAST mode. This includes all FULL level devices.\n LEGACY devices will always only support FAST mode.</p>"]
pub const acamera_metadata_tag_ACAMERA_SHADING_END: acamera_metadata_tag = 1048579;
#[doc = " <p>Operating mode for the face detector\n unit.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_statistics_face_detect_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Whether face detection is enabled, and whether it\n should output just the basic fields or the full set of\n fields.</p>"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_FACE_DETECT_MODE: acamera_metadata_tag = 1114112;
#[doc = " <p>Operating mode for hot pixel map generation.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_statistics_hot_pixel_map_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>If set to <code>true</code>, a hot pixel map is returned in ACAMERA_STATISTICS_HOT_PIXEL_MAP.\n If set to <code>false</code>, no hot pixel map will be returned.</p>\n\n @see ACAMERA_STATISTICS_HOT_PIXEL_MAP"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_HOT_PIXEL_MAP_MODE: acamera_metadata_tag =
    1114115;
#[doc = " <p>List of unique IDs for detected faces.</p>\n\n <p>Type: int32[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>Each detected face is given a unique ID that is valid for as long as the face is visible\n to the camera device.  A face that leaves the field of view and later returns may be\n assigned a new ID.</p>\n <p>Only available if ACAMERA_STATISTICS_FACE_DETECT_MODE == FULL</p>\n\n @see ACAMERA_STATISTICS_FACE_DETECT_MODE"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_FACE_IDS: acamera_metadata_tag = 1114116;
#[doc = " <p>List of landmarks for detected\n faces.</p>\n\n <p>Type: int32[n*6]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>For devices not supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system always follows that of ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with <code>(0, 0)</code> being\n the top-left pixel of the active array.</p>\n <p>For devices supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system depends on the mode being set.\n When the distortion correction mode is OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE, with\n <code>(0, 0)</code> being the top-left pixel of the pre-correction active array.\n When the distortion correction mode is not OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with\n <code>(0, 0)</code> being the top-left pixel of the active array.</p>\n <p>Only available if ACAMERA_STATISTICS_FACE_DETECT_MODE == FULL.</p>\n <p>Starting from API level 30, the coordinate system of activeArraySize or\n preCorrectionActiveArraySize is used to represent post-zoomRatio field of view, not\n pre-zoomRatio field of view. This means that if the relative position of faces and\n the camera device doesn't change, when zooming in by increasing\n ACAMERA_CONTROL_ZOOM_RATIO, the face landmarks move farther away from the center of the\n activeArray or preCorrectionActiveArray. If ACAMERA_CONTROL_ZOOM_RATIO is set to 1.0\n (default), the face landmarks coordinates won't change as ACAMERA_SCALER_CROP_REGION\n changes. See ACAMERA_CONTROL_ZOOM_RATIO for details. Whether to use activeArraySize or\n preCorrectionActiveArraySize still depends on distortion correction mode.</p>\n\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_DISTORTION_CORRECTION_MODE\n @see ACAMERA_SCALER_CROP_REGION\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE\n @see ACAMERA_STATISTICS_FACE_DETECT_MODE"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_FACE_LANDMARKS: acamera_metadata_tag = 1114117;
#[doc = " <p>List of the bounding rectangles for detected\n faces.</p>\n\n <p>Type: int32[n*4]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>For devices not supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system always follows that of ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with <code>(0, 0)</code> being\n the top-left pixel of the active array.</p>\n <p>For devices supporting ACAMERA_DISTORTION_CORRECTION_MODE control, the coordinate\n system depends on the mode being set.\n When the distortion correction mode is OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE, with\n <code>(0, 0)</code> being the top-left pixel of the pre-correction active array.\n When the distortion correction mode is not OFF, the coordinate system follows\n ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE, with\n <code>(0, 0)</code> being the top-left pixel of the active array.</p>\n <p>Only available if ACAMERA_STATISTICS_FACE_DETECT_MODE != OFF.</p>\n <p>Starting from API level 30, the coordinate system of activeArraySize or\n preCorrectionActiveArraySize is used to represent post-zoomRatio field of view, not\n pre-zoomRatio field of view. This means that if the relative position of faces and\n the camera device doesn't change, when zooming in by increasing\n ACAMERA_CONTROL_ZOOM_RATIO, the face rectangles grow larger and move farther away from\n the center of the activeArray or preCorrectionActiveArray. If ACAMERA_CONTROL_ZOOM_RATIO\n is set to 1.0 (default), the face rectangles won't change as ACAMERA_SCALER_CROP_REGION\n changes. See ACAMERA_CONTROL_ZOOM_RATIO for details. Whether to use activeArraySize or\n preCorrectionActiveArraySize still depends on distortion correction mode.</p>\n <p>The data representation is <code>int[4]</code>, which maps to <code>(left, top, right, bottom)</code>.</p>\n\n @see ACAMERA_CONTROL_ZOOM_RATIO\n @see ACAMERA_DISTORTION_CORRECTION_MODE\n @see ACAMERA_SCALER_CROP_REGION\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE\n @see ACAMERA_STATISTICS_FACE_DETECT_MODE"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_FACE_RECTANGLES: acamera_metadata_tag = 1114118;
#[doc = " <p>List of the face confidence scores for\n detected faces</p>\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>Only available if ACAMERA_STATISTICS_FACE_DETECT_MODE != OFF.</p>\n\n @see ACAMERA_STATISTICS_FACE_DETECT_MODE"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_FACE_SCORES: acamera_metadata_tag = 1114119;
#[doc = " <p>The shading map is a low-resolution floating-point map\n that lists the coefficients used to correct for vignetting and color shading,\n for each Bayer color channel of RAW image data.</p>\n\n <p>Type: float[4*n*m]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The map provided here is the same map that is used by the camera device to\n correct both color shading and vignetting for output non-RAW images.</p>\n <p>When there is no lens shading correction applied to RAW\n output images (ACAMERA_SENSOR_INFO_LENS_SHADING_APPLIED <code>==</code>\n false), this map is the complete lens shading correction\n map; when there is some lens shading correction applied to\n the RAW output image (ACAMERA_SENSOR_INFO_LENS_SHADING_APPLIED<code>==</code> true), this map reports the remaining lens shading\n correction map that needs to be applied to get shading\n corrected images that match the camera device's output for\n non-RAW formats.</p>\n <p>For a complete shading correction map, the least shaded\n section of the image will have a gain factor of 1; all\n other sections will have gains above 1.</p>\n <p>When ACAMERA_COLOR_CORRECTION_MODE = TRANSFORM_MATRIX, the map\n will take into account the colorCorrection settings.</p>\n <p>The shading map is for the entire active pixel array, and is not\n affected by the crop region specified in the request. Each shading map\n entry is the value of the shading compensation map over a specific\n pixel on the sensor.  Specifically, with a (N x M) resolution shading\n map, and an active pixel array size (W x H), shading map entry\n (x,y)  (0 ... N-1, 0 ... M-1) is the value of the shading map at\n pixel ( ((W-1)/(N-1)) * x, ((H-1)/(M-1)) * y) for the four color channels.\n The map is assumed to be bilinearly interpolated between the sample points.</p>\n <p>For a Bayer camera, the channel order is [R, Geven, Godd, B], where Geven is\n the green channel for the even rows of a Bayer pattern, and Godd is the odd rows.\n The shading map is stored in a fully interleaved format, and its size\n is provided in the camera static metadata by ACAMERA_LENS_INFO_SHADING_MAP_SIZE.</p>\n <p>The shading map will generally have on the order of 30-40 rows and columns,\n and will be smaller than 64x64.</p>\n <p>As an example, given a very small map for a Bayer camera defined as:</p>\n <pre><code>ACAMERA_LENS_INFO_SHADING_MAP_SIZE = [ 4, 3 ]\n ACAMERA_STATISTICS_LENS_SHADING_MAP =\n [ 1.3, 1.2, 1.15, 1.2,  1.2, 1.2, 1.15, 1.2,\n     1.1, 1.2, 1.2, 1.2,  1.3, 1.2, 1.3, 1.3,\n   1.2, 1.2, 1.25, 1.1,  1.1, 1.1, 1.1, 1.0,\n     1.0, 1.0, 1.0, 1.0,  1.2, 1.3, 1.25, 1.2,\n   1.3, 1.2, 1.2, 1.3,   1.2, 1.15, 1.1, 1.2,\n     1.2, 1.1, 1.0, 1.2,  1.3, 1.15, 1.2, 1.3 ]\n </code></pre>\n <p>The low-resolution scaling map images for each channel are\n (displayed using nearest-neighbor interpolation):</p>\n <p><img alt=\"Red lens shading map\" src=\"../images/camera2/metadata/android.statistics.lensShadingMap/red_shading.png\" />\n <img alt=\"Green (even rows) lens shading map\" src=\"../images/camera2/metadata/android.statistics.lensShadingMap/green_e_shading.png\" />\n <img alt=\"Green (odd rows) lens shading map\" src=\"../images/camera2/metadata/android.statistics.lensShadingMap/green_o_shading.png\" />\n <img alt=\"Blue lens shading map\" src=\"../images/camera2/metadata/android.statistics.lensShadingMap/blue_shading.png\" /></p>\n <p>As a visualization only, inverting the full-color map to recover an\n image of a gray wall (using bicubic interpolation for visual quality)\n as captured by the sensor gives:</p>\n <p><img alt=\"Image of a uniform white wall (inverse shading map)\" src=\"../images/camera2/metadata/android.statistics.lensShadingMap/inv_shading.png\" /></p>\n <p>For a MONOCHROME camera, all of the 2x2 channels must have the same values. An example\n shading map for such a camera is defined as:</p>\n <pre><code>ACAMERA_LENS_INFO_SHADING_MAP_SIZE = [ 4, 3 ]\n ACAMERA_STATISTICS_LENS_SHADING_MAP =\n [ 1.3, 1.3, 1.3, 1.3,  1.2, 1.2, 1.2, 1.2,\n     1.1, 1.1, 1.1, 1.1,  1.3, 1.3, 1.3, 1.3,\n   1.2, 1.2, 1.2, 1.2,  1.1, 1.1, 1.1, 1.1,\n     1.0, 1.0, 1.0, 1.0,  1.2, 1.2, 1.2, 1.2,\n   1.3, 1.3, 1.3, 1.3,   1.2, 1.2, 1.2, 1.2,\n     1.2, 1.2, 1.2, 1.2,  1.3, 1.3, 1.3, 1.3 ]\n </code></pre>\n <p>Note that the RAW image data might be subject to lens shading\n correction not reported on this map. Query\n ACAMERA_SENSOR_INFO_LENS_SHADING_APPLIED to see if RAW image data has subject\n to lens shading correction. If ACAMERA_SENSOR_INFO_LENS_SHADING_APPLIED\n is TRUE, the RAW image data is subject to partial or full lens shading\n correction. In the case full lens shading correction is applied to RAW\n images, the gain factor map reported in this key will contain all 1.0 gains.\n In other words, the map reported in this key is the remaining lens shading\n that needs to be applied on the RAW image to get images without lens shading\n artifacts. See android.request.maxNumOutputRaw for a list of RAW image\n formats.</p>\n\n @see ACAMERA_COLOR_CORRECTION_MODE\n @see ACAMERA_LENS_INFO_SHADING_MAP_SIZE\n @see ACAMERA_SENSOR_INFO_LENS_SHADING_APPLIED\n @see ACAMERA_STATISTICS_LENS_SHADING_MAP"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_LENS_SHADING_MAP: acamera_metadata_tag = 1114123;
#[doc = " <p>The camera device estimated scene illumination lighting\n frequency.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_statistics_scene_flicker_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>Many light sources, such as most fluorescent lights, flicker at a rate\n that depends on the local utility power standards. This flicker must be\n accounted for by auto-exposure routines to avoid artifacts in captured images.\n The camera device uses this entry to tell the application what the scene\n illuminant frequency is.</p>\n <p>When manual exposure control is enabled\n (<code>ACAMERA_CONTROL_AE_MODE == OFF</code> or <code>ACAMERA_CONTROL_MODE ==\n OFF</code>), the ACAMERA_CONTROL_AE_ANTIBANDING_MODE doesn't perform\n antibanding, and the application can ensure it selects\n exposure times that do not cause banding issues by looking\n into this metadata field. See\n ACAMERA_CONTROL_AE_ANTIBANDING_MODE for more details.</p>\n <p>Reports NONE if there doesn't appear to be flickering illumination.</p>\n\n @see ACAMERA_CONTROL_AE_ANTIBANDING_MODE\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_MODE"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_SCENE_FLICKER: acamera_metadata_tag = 1114126;
#[doc = " <p>List of <code>(x, y)</code> coordinates of hot/defective pixels on the sensor.</p>\n\n <p>Type: int32[2*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>A coordinate <code>(x, y)</code> must lie between <code>(0, 0)</code>, and\n <code>(width - 1, height - 1)</code> (inclusive), which are the top-left and\n bottom-right of the pixel array, respectively. The width and\n height dimensions are given in ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE.\n This may include hot pixels that lie outside of the active array\n bounds given by ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.</p>\n\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_HOT_PIXEL_MAP: acamera_metadata_tag = 1114127;
#[doc = " <p>Whether the camera device will output the lens\n shading map in output result metadata.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_statistics_lens_shading_map_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>When set to ON,\n ACAMERA_STATISTICS_LENS_SHADING_MAP will be provided in\n the output result metadata.</p>\n <p>ON is always supported on devices with the RAW capability.</p>\n\n @see ACAMERA_STATISTICS_LENS_SHADING_MAP"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE: acamera_metadata_tag =
    1114128;
#[doc = " <p>A control for selecting whether optical stabilization (OIS) position\n information is included in output result metadata.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_statistics_ois_data_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Since optical image stabilization generally involves motion much faster than the duration\n of individual image exposure, multiple OIS samples can be included for a single capture\n result. For example, if the OIS reporting operates at 200 Hz, a typical camera operating\n at 30fps may have 6-7 OIS samples per capture result. This information can be combined\n with the rolling shutter skew to account for lens motion during image exposure in\n post-processing algorithms.</p>"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_OIS_DATA_MODE: acamera_metadata_tag = 1114129;
#[doc = " <p>An array of timestamps of OIS samples, in nanoseconds.</p>\n\n <p>Type: int64[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The array contains the timestamps of OIS samples. The timestamps are in the same\n timebase as and comparable to ACAMERA_SENSOR_TIMESTAMP.</p>\n\n @see ACAMERA_SENSOR_TIMESTAMP"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_OIS_TIMESTAMPS: acamera_metadata_tag = 1114130;
#[doc = " <p>An array of shifts of OIS samples, in x direction.</p>\n\n <p>Type: float[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The array contains the amount of shifts in x direction, in pixels, based on OIS samples.\n A positive value is a shift from left to right in the pre-correction active array\n coordinate system. For example, if the optical center is (1000, 500) in pre-correction\n active array coordinates, a shift of (3, 0) puts the new optical center at (1003, 500).</p>\n <p>The number of shifts must match the number of timestamps in\n ACAMERA_STATISTICS_OIS_TIMESTAMPS.</p>\n <p>The OIS samples are not affected by whether lens distortion correction is enabled (on\n supporting devices). They are always reported in pre-correction active array coordinates,\n since the scaling of OIS shifts would depend on the specific spot on the sensor the shift\n is needed.</p>\n\n @see ACAMERA_STATISTICS_OIS_TIMESTAMPS"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_OIS_X_SHIFTS: acamera_metadata_tag = 1114131;
#[doc = " <p>An array of shifts of OIS samples, in y direction.</p>\n\n <p>Type: float[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The array contains the amount of shifts in y direction, in pixels, based on OIS samples.\n A positive value is a shift from top to bottom in pre-correction active array coordinate\n system. For example, if the optical center is (1000, 500) in active array coordinates, a\n shift of (0, 5) puts the new optical center at (1000, 505).</p>\n <p>The number of shifts must match the number of timestamps in\n ACAMERA_STATISTICS_OIS_TIMESTAMPS.</p>\n <p>The OIS samples are not affected by whether lens distortion correction is enabled (on\n supporting devices). They are always reported in pre-correction active array coordinates,\n since the scaling of OIS shifts would depend on the specific spot on the sensor the shift\n is needed.</p>\n\n @see ACAMERA_STATISTICS_OIS_TIMESTAMPS"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_OIS_Y_SHIFTS: acamera_metadata_tag = 1114132;
#[doc = " <p>An array of shifts of OIS samples, in y direction.</p>\n\n <p>Type: float[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The array contains the amount of shifts in y direction, in pixels, based on OIS samples.\n A positive value is a shift from top to bottom in pre-correction active array coordinate\n system. For example, if the optical center is (1000, 500) in active array coordinates, a\n shift of (0, 5) puts the new optical center at (1000, 505).</p>\n <p>The number of shifts must match the number of timestamps in\n ACAMERA_STATISTICS_OIS_TIMESTAMPS.</p>\n <p>The OIS samples are not affected by whether lens distortion correction is enabled (on\n supporting devices). They are always reported in pre-correction active array coordinates,\n since the scaling of OIS shifts would depend on the specific spot on the sensor the shift\n is needed.</p>\n\n @see ACAMERA_STATISTICS_OIS_TIMESTAMPS"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_END: acamera_metadata_tag = 1114133;
#[doc = " <p>List of face detection modes for ACAMERA_STATISTICS_FACE_DETECT_MODE that are\n supported by this camera device.</p>\n\n @see ACAMERA_STATISTICS_FACE_DETECT_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>OFF is always supported.</p>"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES:
    acamera_metadata_tag = 1179648;
#[doc = " <p>The maximum number of simultaneously detectable\n faces.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_INFO_MAX_FACE_COUNT: acamera_metadata_tag =
    1179650;
#[doc = " <p>List of hot pixel map output modes for ACAMERA_STATISTICS_HOT_PIXEL_MAP_MODE that are\n supported by this camera device.</p>\n\n @see ACAMERA_STATISTICS_HOT_PIXEL_MAP_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If no hotpixel map output is available for this camera device, this will contain only\n <code>false</code>.</p>\n <p>ON is always supported on devices with the RAW capability.</p>"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES:
    acamera_metadata_tag = 1179654;
#[doc = " <p>List of lens shading map output modes for ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE that\n are supported by this camera device.</p>\n\n @see ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If no lens shading map output is available for this camera device, this key will\n contain only OFF.</p>\n <p>ON is always supported on devices with the RAW capability.\n LEGACY mode devices will always only support OFF.</p>"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_INFO_AVAILABLE_LENS_SHADING_MAP_MODES:
    acamera_metadata_tag = 1179655;
#[doc = " <p>List of OIS data output modes for ACAMERA_STATISTICS_OIS_DATA_MODE that\n are supported by this camera device.</p>\n\n @see ACAMERA_STATISTICS_OIS_DATA_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If no OIS data output is available for this camera device, this key will\n contain only OFF.</p>"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_INFO_AVAILABLE_OIS_DATA_MODES:
    acamera_metadata_tag = 1179656;
#[doc = " <p>List of OIS data output modes for ACAMERA_STATISTICS_OIS_DATA_MODE that\n are supported by this camera device.</p>\n\n @see ACAMERA_STATISTICS_OIS_DATA_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If no OIS data output is available for this camera device, this key will\n contain only OFF.</p>"]
pub const acamera_metadata_tag_ACAMERA_STATISTICS_INFO_END: acamera_metadata_tag = 1179657;
#[doc = " <p>Tonemapping / contrast / gamma curve for the blue\n channel, to use when ACAMERA_TONEMAP_MODE is\n CONTRAST_CURVE.</p>\n\n @see ACAMERA_TONEMAP_MODE\n\n <p>Type: float[n*2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>See ACAMERA_TONEMAP_CURVE_RED for more details.</p>\n\n @see ACAMERA_TONEMAP_CURVE_RED"]
pub const acamera_metadata_tag_ACAMERA_TONEMAP_CURVE_BLUE: acamera_metadata_tag = 1245184;
#[doc = " <p>Tonemapping / contrast / gamma curve for the green\n channel, to use when ACAMERA_TONEMAP_MODE is\n CONTRAST_CURVE.</p>\n\n @see ACAMERA_TONEMAP_MODE\n\n <p>Type: float[n*2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>See ACAMERA_TONEMAP_CURVE_RED for more details.</p>\n\n @see ACAMERA_TONEMAP_CURVE_RED"]
pub const acamera_metadata_tag_ACAMERA_TONEMAP_CURVE_GREEN: acamera_metadata_tag = 1245185;
#[doc = " <p>Tonemapping / contrast / gamma curve for the red\n channel, to use when ACAMERA_TONEMAP_MODE is\n CONTRAST_CURVE.</p>\n\n @see ACAMERA_TONEMAP_MODE\n\n <p>Type: float[n*2]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Each channel's curve is defined by an array of control points:</p>\n <pre><code>ACAMERA_TONEMAP_CURVE_RED =\n   [ P0in, P0out, P1in, P1out, P2in, P2out, P3in, P3out, ..., PNin, PNout ]\n 2 &lt;= N &lt;= ACAMERA_TONEMAP_MAX_CURVE_POINTS</code></pre>\n <p>These are sorted in order of increasing <code>Pin</code>; it is\n required that input values 0.0 and 1.0 are included in the list to\n define a complete mapping. For input values between control points,\n the camera device must linearly interpolate between the control\n points.</p>\n <p>Each curve can have an independent number of points, and the number\n of points can be less than max (that is, the request doesn't have to\n always provide a curve with number of points equivalent to\n ACAMERA_TONEMAP_MAX_CURVE_POINTS).</p>\n <p>For devices with MONOCHROME capability, all three channels must have the same set of\n control points.</p>\n <p>A few examples, and their corresponding graphical mappings; these\n only specify the red channel and the precision is limited to 4\n digits, for conciseness.</p>\n <p>Linear mapping:</p>\n <pre><code>ACAMERA_TONEMAP_CURVE_RED = [ 0, 0, 1.0, 1.0 ]\n </code></pre>\n <p><img alt=\"Linear mapping curve\" src=\"../images/camera2/metadata/android.tonemap.curveRed/linear_tonemap.png\" /></p>\n <p>Invert mapping:</p>\n <pre><code>ACAMERA_TONEMAP_CURVE_RED = [ 0, 1.0, 1.0, 0 ]\n </code></pre>\n <p><img alt=\"Inverting mapping curve\" src=\"../images/camera2/metadata/android.tonemap.curveRed/inverse_tonemap.png\" /></p>\n <p>Gamma 1/2.2 mapping, with 16 control points:</p>\n <pre><code>ACAMERA_TONEMAP_CURVE_RED = [\n   0.0000, 0.0000, 0.0667, 0.2920, 0.1333, 0.4002, 0.2000, 0.4812,\n   0.2667, 0.5484, 0.3333, 0.6069, 0.4000, 0.6594, 0.4667, 0.7072,\n   0.5333, 0.7515, 0.6000, 0.7928, 0.6667, 0.8317, 0.7333, 0.8685,\n   0.8000, 0.9035, 0.8667, 0.9370, 0.9333, 0.9691, 1.0000, 1.0000 ]\n </code></pre>\n <p><img alt=\"Gamma = 1/2.2 tonemapping curve\" src=\"../images/camera2/metadata/android.tonemap.curveRed/gamma_tonemap.png\" /></p>\n <p>Standard sRGB gamma mapping, per IEC 61966-2-1:1999, with 16 control points:</p>\n <pre><code>ACAMERA_TONEMAP_CURVE_RED = [\n   0.0000, 0.0000, 0.0667, 0.2864, 0.1333, 0.4007, 0.2000, 0.4845,\n   0.2667, 0.5532, 0.3333, 0.6125, 0.4000, 0.6652, 0.4667, 0.7130,\n   0.5333, 0.7569, 0.6000, 0.7977, 0.6667, 0.8360, 0.7333, 0.8721,\n   0.8000, 0.9063, 0.8667, 0.9389, 0.9333, 0.9701, 1.0000, 1.0000 ]\n </code></pre>\n <p><img alt=\"sRGB tonemapping curve\" src=\"../images/camera2/metadata/android.tonemap.curveRed/srgb_tonemap.png\" /></p>\n\n @see ACAMERA_TONEMAP_CURVE_RED\n @see ACAMERA_TONEMAP_MAX_CURVE_POINTS"]
pub const acamera_metadata_tag_ACAMERA_TONEMAP_CURVE_RED: acamera_metadata_tag = 1245186;
#[doc = " <p>High-level global contrast/gamma/tonemapping control.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_tonemap_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>When switching to an application-defined contrast curve by setting\n ACAMERA_TONEMAP_MODE to CONTRAST_CURVE, the curve is defined\n per-channel with a set of <code>(in, out)</code> points that specify the\n mapping from input high-bit-depth pixel value to the output\n low-bit-depth value.  Since the actual pixel ranges of both input\n and output may change depending on the camera pipeline, the values\n are specified by normalized floating-point numbers.</p>\n <p>More-complex color mapping operations such as 3D color look-up\n tables, selective chroma enhancement, or other non-linear color\n transforms will be disabled when ACAMERA_TONEMAP_MODE is\n CONTRAST_CURVE.</p>\n <p>When using either FAST or HIGH_QUALITY, the camera device will\n emit its own tonemap curve in android.tonemap.curve.\n These values are always available, and as close as possible to the\n actually used nonlinear/nonglobal transforms.</p>\n <p>If a request is sent with CONTRAST_CURVE with the camera device's\n provided curve in FAST or HIGH_QUALITY, the image's tonemap will be\n roughly the same.</p>\n\n @see ACAMERA_TONEMAP_MODE"]
pub const acamera_metadata_tag_ACAMERA_TONEMAP_MODE: acamera_metadata_tag = 1245187;
#[doc = " <p>Maximum number of supported points in the\n tonemap curve that can be used for android.tonemap.curve.</p>\n\n <p>Type: int32</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If the actual number of points provided by the application (in ACAMERA_TONEMAPCURVE_*) is\n less than this maximum, the camera device will resample the curve to its internal\n representation, using linear interpolation.</p>\n <p>The output curves in the result metadata may have a different number\n of points than the input curves, and will represent the actual\n hardware curves used as closely as possible when linearly interpolated.</p>"]
pub const acamera_metadata_tag_ACAMERA_TONEMAP_MAX_CURVE_POINTS: acamera_metadata_tag = 1245188;
#[doc = " <p>List of tonemapping modes for ACAMERA_TONEMAP_MODE that are supported by this camera\n device.</p>\n\n @see ACAMERA_TONEMAP_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Camera devices that support the MANUAL_POST_PROCESSING capability will always contain\n at least one of below mode combinations:</p>\n <ul>\n <li>CONTRAST_CURVE, FAST and HIGH_QUALITY</li>\n <li>GAMMA_VALUE, PRESET_CURVE, FAST and HIGH_QUALITY</li>\n </ul>\n <p>This includes all FULL level devices.</p>"]
pub const acamera_metadata_tag_ACAMERA_TONEMAP_AVAILABLE_TONE_MAP_MODES: acamera_metadata_tag =
    1245189;
#[doc = " <p>Tonemapping curve to use when ACAMERA_TONEMAP_MODE is\n GAMMA_VALUE</p>\n\n @see ACAMERA_TONEMAP_MODE\n\n <p>Type: float</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>The tonemap curve will be defined the following formula:</p>\n <ul>\n <li>OUT = pow(IN, 1.0 / gamma)</li>\n </ul>\n <p>where IN and OUT is the input pixel value scaled to range [0.0, 1.0],\n pow is the power function and gamma is the gamma value specified by this\n key.</p>\n <p>The same curve will be applied to all color channels. The camera device\n may clip the input gamma value to its supported range. The actual applied\n value will be returned in capture result.</p>\n <p>The valid range of gamma value varies on different devices, but values\n within [1.0, 5.0] are guaranteed not to be clipped.</p>"]
pub const acamera_metadata_tag_ACAMERA_TONEMAP_GAMMA: acamera_metadata_tag = 1245190;
#[doc = " <p>Tonemapping curve to use when ACAMERA_TONEMAP_MODE is\n PRESET_CURVE</p>\n\n @see ACAMERA_TONEMAP_MODE\n\n <p>Type: byte (acamera_metadata_enum_android_tonemap_preset_curve_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>The tonemap curve will be defined by specified standard.</p>\n <p>sRGB (approximated by 16 control points):</p>\n <p><img alt=\"sRGB tonemapping curve\" src=\"../images/camera2/metadata/android.tonemap.curveRed/srgb_tonemap.png\" /></p>\n <p>Rec. 709 (approximated by 16 control points):</p>\n <p><img alt=\"Rec. 709 tonemapping curve\" src=\"../images/camera2/metadata/android.tonemap.curveRed/rec709_tonemap.png\" /></p>\n <p>Note that above figures show a 16 control points approximation of preset\n curves. Camera devices may apply a different approximation to the curve.</p>"]
pub const acamera_metadata_tag_ACAMERA_TONEMAP_PRESET_CURVE: acamera_metadata_tag = 1245191;
#[doc = " <p>Tonemapping curve to use when ACAMERA_TONEMAP_MODE is\n PRESET_CURVE</p>\n\n @see ACAMERA_TONEMAP_MODE\n\n <p>Type: byte (acamera_metadata_enum_android_tonemap_preset_curve_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>The tonemap curve will be defined by specified standard.</p>\n <p>sRGB (approximated by 16 control points):</p>\n <p><img alt=\"sRGB tonemapping curve\" src=\"../images/camera2/metadata/android.tonemap.curveRed/srgb_tonemap.png\" /></p>\n <p>Rec. 709 (approximated by 16 control points):</p>\n <p><img alt=\"Rec. 709 tonemapping curve\" src=\"../images/camera2/metadata/android.tonemap.curveRed/rec709_tonemap.png\" /></p>\n <p>Note that above figures show a 16 control points approximation of preset\n curves. Camera devices may apply a different approximation to the curve.</p>"]
pub const acamera_metadata_tag_ACAMERA_TONEMAP_END: acamera_metadata_tag = 1245192;
#[doc = " <p>Generally classifies the overall set of the camera device functionality.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_info_supported_hardware_level_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The supported hardware level is a high-level description of the camera device's\n capabilities, summarizing several capabilities into one field.  Each level adds additional\n features to the previous one, and is always a strict superset of the previous level.\n The ordering is <code>LEGACY &lt; LIMITED &lt; FULL &lt; LEVEL_3</code>.</p>\n <p>Starting from <code>LEVEL_3</code>, the level enumerations are guaranteed to be in increasing\n numerical value as well. To check if a given device is at least at a given hardware level,\n the following code snippet can be used:</p>\n <pre><code>// Returns true if the device supports the required hardware level, or better.\n boolean isHardwareLevelSupported(CameraCharacteristics c, int requiredLevel) {\n     final int[] sortedHwLevels = {\n         CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY,\n         CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL,\n         CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED,\n         CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_FULL,\n         CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3\n     };\n     int deviceLevel = c.get(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL);\n     if (requiredLevel == deviceLevel) {\n         return true;\n     }\n\n     for (int sortedlevel : sortedHwLevels) {\n         if (sortedlevel == requiredLevel) {\n             return true;\n         } else if (sortedlevel == deviceLevel) {\n             return false;\n         }\n     }\n     return false; // Should never reach here\n }\n </code></pre>\n <p>At a high level, the levels are:</p>\n <ul>\n <li><code>LEGACY</code> devices operate in a backwards-compatibility mode for older\n   Android devices, and have very limited capabilities.</li>\n <li><code>LIMITED</code> devices represent the\n   baseline feature set, and may also include additional capabilities that are\n   subsets of <code>FULL</code>.</li>\n <li><code>FULL</code> devices additionally support per-frame manual control of sensor, flash, lens and\n   post-processing settings, and image capture at a high rate.</li>\n <li><code>LEVEL_3</code> devices additionally support YUV reprocessing and RAW image capture, along\n   with additional output stream configurations.</li>\n <li><code>EXTERNAL</code> devices are similar to <code>LIMITED</code> devices with exceptions like some sensor or\n   lens information not reported or less stable framerates.</li>\n </ul>\n <p>See the individual level enums for full descriptions of the supported capabilities.  The\n ACAMERA_REQUEST_AVAILABLE_CAPABILITIES entry describes the device's capabilities at a\n finer-grain level, if needed. In addition, many controls have their available settings or\n ranges defined in individual entries from {@link ACameraManager_getCameraCharacteristics }.</p>\n <p>Some features are not part of any particular hardware level or capability and must be\n queried separately. These include:</p>\n <ul>\n <li>Calibrated timestamps (ACAMERA_SENSOR_INFO_TIMESTAMP_SOURCE <code>==</code> REALTIME)</li>\n <li>Precision lens control (ACAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION <code>==</code> CALIBRATED)</li>\n <li>Face detection (ACAMERA_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES)</li>\n <li>Optical or electrical image stabilization\n   (ACAMERA_LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION,\n    ACAMERA_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES)</li>\n </ul>\n\n @see ACAMERA_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES\n @see ACAMERA_LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION\n @see ACAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES\n @see ACAMERA_SENSOR_INFO_TIMESTAMP_SOURCE\n @see ACAMERA_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES"]
pub const acamera_metadata_tag_ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL: acamera_metadata_tag =
    1376256;
#[doc = " <p>A short string for manufacturer version information about the camera device, such as\n ISP hardware, sensors, etc.</p>\n\n <p>Type: byte</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This can be used in <a href=\"https://developer.android.com/reference/android/media/ExifInterface.html#TAG_IMAGE_DESCRIPTION\">TAG_IMAGE_DESCRIPTION</a>\n in jpeg EXIF. This key may be absent if no version information is available on the\n device.</p>"]
pub const acamera_metadata_tag_ACAMERA_INFO_VERSION: acamera_metadata_tag = 1376257;
#[doc = " <p>Type: int64[2*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>HAL must populate the array with\n (hardware::camera::provider::V2_5::DeviceState, sensorOrientation) pairs for each\n supported device state bitwise combination.</p>"]
pub const acamera_metadata_tag_ACAMERA_INFO_DEVICE_STATE_ORIENTATIONS: acamera_metadata_tag =
    1376259;
#[doc = " <p>Type: int64[2*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>HAL must populate the array with\n (hardware::camera::provider::V2_5::DeviceState, sensorOrientation) pairs for each\n supported device state bitwise combination.</p>"]
pub const acamera_metadata_tag_ACAMERA_INFO_END: acamera_metadata_tag = 1376260;
#[doc = " <p>Whether black-level compensation is locked\n to its current values, or is free to vary.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_black_level_lock_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Whether the black level offset was locked for this frame.  Should be\n ON if ACAMERA_BLACK_LEVEL_LOCK was ON in the capture request, unless\n a change in other capture settings forced the camera device to\n perform a black level reset.</p>\n\n @see ACAMERA_BLACK_LEVEL_LOCK"]
pub const acamera_metadata_tag_ACAMERA_BLACK_LEVEL_LOCK: acamera_metadata_tag = 1441792;
#[doc = " <p>Whether black-level compensation is locked\n to its current values, or is free to vary.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_black_level_lock_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>Whether the black level offset was locked for this frame.  Should be\n ON if ACAMERA_BLACK_LEVEL_LOCK was ON in the capture request, unless\n a change in other capture settings forced the camera device to\n perform a black level reset.</p>\n\n @see ACAMERA_BLACK_LEVEL_LOCK"]
pub const acamera_metadata_tag_ACAMERA_BLACK_LEVEL_END: acamera_metadata_tag = 1441793;
#[doc = " <p>The frame number corresponding to the last request\n with which the output result (metadata + buffers) has been fully\n synchronized.</p>\n\n <p>Type: int64 (acamera_metadata_enum_android_sync_frame_number_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>When a request is submitted to the camera device, there is usually a\n delay of several frames before the controls get applied. A camera\n device may either choose to account for this delay by implementing a\n pipeline and carefully submit well-timed atomic control updates, or\n it may start streaming control changes that span over several frame\n boundaries.</p>\n <p>In the latter case, whenever a request's settings change relative to\n the previous submitted request, the full set of changes may take\n multiple frame durations to fully take effect. Some settings may\n take effect sooner (in less frame durations) than others.</p>\n <p>While a set of control changes are being propagated, this value\n will be CONVERGING.</p>\n <p>Once it is fully known that a set of control changes have been\n finished propagating, and the resulting updated control settings\n have been read back by the camera device, this value will be set\n to a non-negative frame number (corresponding to the request to\n which the results have synchronized to).</p>\n <p>Older camera device implementations may not have a way to detect\n when all camera controls have been applied, and will always set this\n value to UNKNOWN.</p>\n <p>FULL capability devices will always have this value set to the\n frame number of the request corresponding to this result.</p>\n <p><em>Further details</em>:</p>\n <ul>\n <li>Whenever a request differs from the last request, any future\n results not yet returned may have this value set to CONVERGING (this\n could include any in-progress captures not yet returned by the camera\n device, for more details see pipeline considerations below).</li>\n <li>Submitting a series of multiple requests that differ from the\n previous request (e.g. r1, r2, r3 s.t. r1 != r2 != r3)\n moves the new synchronization frame to the last non-repeating\n request (using the smallest frame number from the contiguous list of\n repeating requests).</li>\n <li>Submitting the same request repeatedly will not change this value\n to CONVERGING, if it was already a non-negative value.</li>\n <li>When this value changes to non-negative, that means that all of the\n metadata controls from the request have been applied, all of the\n metadata controls from the camera device have been read to the\n updated values (into the result), and all of the graphics buffers\n corresponding to this result are also synchronized to the request.</li>\n </ul>\n <p><em>Pipeline considerations</em>:</p>\n <p>Submitting a request with updated controls relative to the previously\n submitted requests may also invalidate the synchronization state\n of all the results corresponding to currently in-flight requests.</p>\n <p>In other words, results for this current request and up to\n ACAMERA_REQUEST_PIPELINE_MAX_DEPTH prior requests may have their\n ACAMERA_SYNC_FRAME_NUMBER change to CONVERGING.</p>\n\n @see ACAMERA_REQUEST_PIPELINE_MAX_DEPTH\n @see ACAMERA_SYNC_FRAME_NUMBER"]
pub const acamera_metadata_tag_ACAMERA_SYNC_FRAME_NUMBER: acamera_metadata_tag = 1507328;
#[doc = " <p>The maximum number of frames that can occur after a request\n (different than the previous) has been submitted, and before the\n result's state becomes synchronized.</p>\n\n <p>Type: int32 (acamera_metadata_enum_android_sync_max_latency_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This defines the maximum distance (in number of metadata results),\n between the frame number of the request that has new controls to apply\n and the frame number of the result that has all the controls applied.</p>\n <p>In other words this acts as an upper boundary for how many frames\n must occur before the camera device knows for a fact that the new\n submitted camera settings have been applied in outgoing frames.</p>"]
pub const acamera_metadata_tag_ACAMERA_SYNC_MAX_LATENCY: acamera_metadata_tag = 1507329;
#[doc = " <p>The maximum number of frames that can occur after a request\n (different than the previous) has been submitted, and before the\n result's state becomes synchronized.</p>\n\n <p>Type: int32 (acamera_metadata_enum_android_sync_max_latency_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This defines the maximum distance (in number of metadata results),\n between the frame number of the request that has new controls to apply\n and the frame number of the result that has all the controls applied.</p>\n <p>In other words this acts as an upper boundary for how many frames\n must occur before the camera device knows for a fact that the new\n submitted camera settings have been applied in outgoing frames.</p>"]
pub const acamera_metadata_tag_ACAMERA_SYNC_END: acamera_metadata_tag = 1507330;
#[doc = " <p>The available depth dataspace stream\n configurations that this camera device supports\n (i.e. format, width, height, output/input stream).</p>\n\n <p>Type: int32[n*4] (acamera_metadata_enum_android_depth_available_depth_stream_configurations_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>These are output stream configurations for use with\n dataSpace HAL_DATASPACE_DEPTH. The configurations are\n listed as <code>(format, width, height, input?)</code> tuples.</p>\n <p>Only devices that support depth output for at least\n the HAL_PIXEL_FORMAT_Y16 dense depth map may include\n this entry.</p>\n <p>A device that also supports the HAL_PIXEL_FORMAT_BLOB\n sparse depth point cloud must report a single entry for\n the format in this list as <code>(HAL_PIXEL_FORMAT_BLOB,\n android.depth.maxDepthSamples, 1, OUTPUT)</code> in addition to\n the entries for HAL_PIXEL_FORMAT_Y16.</p>"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DEPTH_STREAM_CONFIGURATIONS:
    acamera_metadata_tag = 1638401;
#[doc = " <p>This lists the minimum frame duration for each\n format/size combination for depth output formats.</p>\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This should correspond to the frame duration when only that\n stream is active, with all processing (typically in android.*.mode)\n set to either OFF or FAST.</p>\n <p>When multiple streams are used in a request, the minimum frame\n duration will be max(individual stream min durations).</p>\n <p>The minimum frame duration of a stream (of a particular format, size)\n is the same regardless of whether the stream is input or output.</p>\n <p>See ACAMERA_SENSOR_FRAME_DURATION and\n ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS for more details about\n calculating the max frame rate.</p>\n\n @see ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS\n @see ACAMERA_SENSOR_FRAME_DURATION"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DEPTH_MIN_FRAME_DURATIONS:
    acamera_metadata_tag = 1638402;
#[doc = " <p>This lists the maximum stall duration for each\n output format/size combination for depth streams.</p>\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>A stall duration is how much extra time would get added\n to the normal minimum frame duration for a repeating request\n that has streams with non-zero stall.</p>\n <p>This functions similarly to\n ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS for depth\n streams.</p>\n <p>All depth output stream formats may have a nonzero stall\n duration.</p>\n\n @see ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DEPTH_STALL_DURATIONS: acamera_metadata_tag =
    1638403;
#[doc = " <p>Indicates whether a capture request may target both a\n DEPTH16 / DEPTH_POINT_CLOUD output, and normal color outputs (such as\n YUV_420_888, JPEG, or RAW) simultaneously.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_depth_depth_is_exclusive_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>If TRUE, including both depth and color outputs in a single\n capture request is not supported. An application must interleave color\n and depth requests.  If FALSE, a single request can target both types\n of output.</p>\n <p>Typically, this restriction exists on camera devices that\n need to emit a specific pattern or wavelength of light to\n measure depth values, which causes the color image to be\n corrupted during depth measurement.</p>"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_DEPTH_IS_EXCLUSIVE: acamera_metadata_tag = 1638404;
#[doc = " <p>Recommended depth stream configurations for common client use cases.</p>\n\n <p>Type: int32[n*5]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Optional subset of the ACAMERA_DEPTH_AVAILABLE_DEPTH_STREAM_CONFIGURATIONS that\n contains similar tuples listed as\n (i.e. width, height, format, output/input stream, usecase bit field).\n Camera devices will be able to suggest particular depth stream configurations which are\n power and performance efficient for specific use cases. For more information about\n retrieving the suggestions see\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#getRecommendedStreamConfigurationMap\">CameraCharacteristics#getRecommendedStreamConfigurationMap</a>.</p>\n <p>For data representation please refer to\n ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS</p>\n\n @see ACAMERA_DEPTH_AVAILABLE_DEPTH_STREAM_CONFIGURATIONS\n @see ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_RECOMMENDED_DEPTH_STREAM_CONFIGURATIONS:
    acamera_metadata_tag = 1638405;
#[doc = " <p>The available dynamic depth dataspace stream\n configurations that this camera device supports\n (i.e. format, width, height, output/input stream).</p>\n\n <p>Type: int32[n*4] (acamera_metadata_enum_android_depth_available_dynamic_depth_stream_configurations_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>These are output stream configurations for use with\n dataSpace DYNAMIC_DEPTH. The configurations are\n listed as <code>(format, width, height, input?)</code> tuples.</p>\n <p>Only devices that support depth output for at least\n the HAL_PIXEL_FORMAT_Y16 dense depth map along with\n HAL_PIXEL_FORMAT_BLOB with the same size or size with\n the same aspect ratio can have dynamic depth dataspace\n stream configuration. ACAMERA_DEPTH_DEPTH_IS_EXCLUSIVE also\n needs to be set to FALSE.</p>\n\n @see ACAMERA_DEPTH_DEPTH_IS_EXCLUSIVE"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STREAM_CONFIGURATIONS:
    acamera_metadata_tag = 1638406;
#[doc = " <p>This lists the minimum frame duration for each\n format/size combination for dynamic depth output streams.</p>\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This should correspond to the frame duration when only that\n stream is active, with all processing (typically in android.*.mode)\n set to either OFF or FAST.</p>\n <p>When multiple streams are used in a request, the minimum frame\n duration will be max(individual stream min durations).</p>\n <p>The minimum frame duration of a stream (of a particular format, size)\n is the same regardless of whether the stream is input or output.</p>"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_MIN_FRAME_DURATIONS:
    acamera_metadata_tag = 1638407;
#[doc = " <p>This lists the maximum stall duration for each\n output format/size combination for dynamic depth streams.</p>\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>A stall duration is how much extra time would get added\n to the normal minimum frame duration for a repeating request\n that has streams with non-zero stall.</p>\n <p>All dynamic depth output streams may have a nonzero stall\n duration.</p>"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STALL_DURATIONS:
    acamera_metadata_tag = 1638408;
#[doc = " <p>The available depth dataspace stream\n configurations that this camera device supports\n (i.e. format, width, height, output/input stream) when a CaptureRequest is submitted with\n ACAMERA_SENSOR_PIXEL_MODE set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int32[n*4] (acamera_metadata_enum_android_depth_available_depth_stream_configurations_maximum_resolution_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_DEPTH_AVAILABLE_DEPTH_STREAM_CONFIGURATIONS, for configurations which\n are applicable when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_DEPTH_AVAILABLE_DEPTH_STREAM_CONFIGURATIONS\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DEPTH_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION : acamera_metadata_tag = 1638409 ;
#[doc = " <p>This lists the minimum frame duration for each\n format/size combination for depth output formats when a CaptureRequest is submitted with\n ACAMERA_SENSOR_PIXEL_MODE set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_DEPTH_AVAILABLE_DEPTH_MIN_FRAME_DURATIONS, for configurations which\n are applicable when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n <p>See ACAMERA_SENSOR_FRAME_DURATION and\n ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS_MAXIMUM_RESOLUTION for more details about\n calculating the max frame rate.</p>\n\n @see ACAMERA_DEPTH_AVAILABLE_DEPTH_MIN_FRAME_DURATIONS\n @see ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS_MAXIMUM_RESOLUTION\n @see ACAMERA_SENSOR_FRAME_DURATION\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DEPTH_MIN_FRAME_DURATIONS_MAXIMUM_RESOLUTION : acamera_metadata_tag = 1638410 ;
#[doc = " <p>This lists the maximum stall duration for each\n output format/size combination for depth streams for CaptureRequests where\n ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_DEPTH_AVAILABLE_DEPTH_STALL_DURATIONS, for configurations which\n are applicable when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_DEPTH_AVAILABLE_DEPTH_STALL_DURATIONS\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DEPTH_STALL_DURATIONS_MAXIMUM_RESOLUTION:
    acamera_metadata_tag = 1638411;
#[doc = " <p>The available dynamic depth dataspace stream\n configurations that this camera device supports (i.e. format, width, height,\n output/input stream) for CaptureRequests where ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int32[n*4] (acamera_metadata_enum_android_depth_available_dynamic_depth_stream_configurations_maximum_resolution_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STREAM_CONFIGURATIONS, for configurations\n which are applicable when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STREAM_CONFIGURATIONS\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION : acamera_metadata_tag = 1638412 ;
#[doc = " <p>This lists the minimum frame duration for each\n format/size combination for dynamic depth output streams  for CaptureRequests where\n ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_MIN_FRAME_DURATIONS, for configurations\n which are applicable when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_MIN_FRAME_DURATIONS\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_MIN_FRAME_DURATIONS_MAXIMUM_RESOLUTION : acamera_metadata_tag = 1638413 ;
#[doc = " <p>This lists the maximum stall duration for each\n output format/size combination for dynamic depth streams for CaptureRequests where\n ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STALL_DURATIONS, for configurations\n which are applicable when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STALL_DURATIONS\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STALL_DURATIONS_MAXIMUM_RESOLUTION : acamera_metadata_tag = 1638414 ;
#[doc = " <p>This lists the maximum stall duration for each\n output format/size combination for dynamic depth streams for CaptureRequests where\n ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Analogous to ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STALL_DURATIONS, for configurations\n which are applicable when ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STALL_DURATIONS\n @see ACAMERA_SENSOR_PIXEL_MODE"]
pub const acamera_metadata_tag_ACAMERA_DEPTH_END: acamera_metadata_tag = 1638415;
#[doc = " <p>String containing the ids of the underlying physical cameras.</p>\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>For a logical camera, this is concatenation of all underlying physical camera IDs.\n The null terminator for physical camera ID must be preserved so that the whole string\n can be tokenized using '\\0' to generate list of physical camera IDs.</p>\n <p>For example, if the physical camera IDs of the logical camera are \"2\" and \"3\", the\n value of this tag will be ['2', '\\0', '3', '\\0'].</p>\n <p>The number of physical camera IDs must be no less than 2.</p>"]
pub const acamera_metadata_tag_ACAMERA_LOGICAL_MULTI_CAMERA_PHYSICAL_IDS: acamera_metadata_tag =
    1703936;
#[doc = " <p>The accuracy of frame timestamp synchronization between physical cameras</p>\n\n <p>Type: byte (acamera_metadata_enum_android_logical_multi_camera_sensor_sync_type_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The accuracy of the frame timestamp synchronization determines the physical cameras'\n ability to start exposure at the same time. If the sensorSyncType is CALIBRATED, the\n physical camera sensors usually run in leader/follower mode where one sensor generates a\n timing signal for the other, so that their shutter time is synchronized. For APPROXIMATE\n sensorSyncType, the camera sensors usually run in leader/leader mode, where both sensors\n use their own timing generator, and there could be offset between their start of exposure.</p>\n <p>In both cases, all images generated for a particular capture request still carry the same\n timestamps, so that they can be used to look up the matching frame number and\n onCaptureStarted callback.</p>\n <p>This tag is only applicable if the logical camera device supports concurrent physical\n streams from different physical cameras.</p>"]
pub const acamera_metadata_tag_ACAMERA_LOGICAL_MULTI_CAMERA_SENSOR_SYNC_TYPE: acamera_metadata_tag =
    1703937;
#[doc = " <p>String containing the ID of the underlying active physical camera.</p>\n\n <p>Type: byte</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The ID of the active physical camera that's backing the logical camera. All camera\n streams and metadata that are not physical camera specific will be originating from this\n physical camera.</p>\n <p>For a logical camera made up of physical cameras where each camera's lenses have\n different characteristics, the camera device may choose to switch between the physical\n cameras when application changes FOCAL_LENGTH or SCALER_CROP_REGION.\n At the time of lens switch, this result metadata reflects the new active physical camera\n ID.</p>\n <p>This key will be available if the camera device advertises this key via {@link ACAMERA_REQUEST_AVAILABLE_RESULT_KEYS }.\n When available, this must be one of valid physical IDs backing this logical multi-camera.\n If this key is not available for a logical multi-camera, the camera device implementation\n may still switch between different active physical cameras based on use case, but the\n current active physical camera information won't be available to the application.</p>"]
pub const acamera_metadata_tag_ACAMERA_LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID:
    acamera_metadata_tag = 1703938;
#[doc = " <p>String containing the ID of the underlying active physical camera.</p>\n\n <p>Type: byte</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n </ul></p>\n\n <p>The ID of the active physical camera that's backing the logical camera. All camera\n streams and metadata that are not physical camera specific will be originating from this\n physical camera.</p>\n <p>For a logical camera made up of physical cameras where each camera's lenses have\n different characteristics, the camera device may choose to switch between the physical\n cameras when application changes FOCAL_LENGTH or SCALER_CROP_REGION.\n At the time of lens switch, this result metadata reflects the new active physical camera\n ID.</p>\n <p>This key will be available if the camera device advertises this key via {@link ACAMERA_REQUEST_AVAILABLE_RESULT_KEYS }.\n When available, this must be one of valid physical IDs backing this logical multi-camera.\n If this key is not available for a logical multi-camera, the camera device implementation\n may still switch between different active physical cameras based on use case, but the\n current active physical camera information won't be available to the application.</p>"]
pub const acamera_metadata_tag_ACAMERA_LOGICAL_MULTI_CAMERA_END: acamera_metadata_tag = 1703939;
#[doc = " <p>Mode of operation for the lens distortion correction block.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_distortion_correction_mode_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraCaptureSession_captureCallback_result callbacks</li>\n   <li>ACaptureRequest</li>\n </ul></p>\n\n <p>The lens distortion correction block attempts to improve image quality by fixing\n radial, tangential, or other geometric aberrations in the camera device's optics.  If\n available, the ACAMERA_LENS_DISTORTION field documents the lens's distortion parameters.</p>\n <p>OFF means no distortion correction is done.</p>\n <p>FAST/HIGH_QUALITY both mean camera device determined distortion correction will be\n applied. HIGH_QUALITY mode indicates that the camera device will use the highest-quality\n correction algorithms, even if it slows down capture rate. FAST means the camera device\n will not slow down capture rate when applying correction. FAST may be the same as OFF if\n any correction at all would slow down capture rate.  Every output stream will have a\n similar amount of enhancement applied.</p>\n <p>The correction only applies to processed outputs such as YUV, Y8, JPEG, or DEPTH16; it is\n not applied to any RAW output.</p>\n <p>This control will be on by default on devices that support this control. Applications\n disabling distortion correction need to pay extra attention with the coordinate system of\n metering regions, crop region, and face rectangles. When distortion correction is OFF,\n metadata coordinates follow the coordinate system of\n ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE. When distortion is not OFF, metadata\n coordinates follow the coordinate system of ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE.  The\n camera device will map these metadata fields to match the corrected image produced by the\n camera device, for both capture requests and results.  However, this mapping is not very\n precise, since rectangles do not generally map to rectangles when corrected.  Only linear\n scaling between the active array and precorrection active array coordinates is\n performed. Applications that require precise correction of metadata need to undo that\n linear scaling, and apply a more complete correction that takes into the account the app's\n own requirements.</p>\n <p>The full list of metadata that is affected in this way by distortion correction is:</p>\n <ul>\n <li>ACAMERA_CONTROL_AF_REGIONS</li>\n <li>ACAMERA_CONTROL_AE_REGIONS</li>\n <li>ACAMERA_CONTROL_AWB_REGIONS</li>\n <li>ACAMERA_SCALER_CROP_REGION</li>\n <li>android.statistics.faces</li>\n </ul>\n\n @see ACAMERA_CONTROL_AE_REGIONS\n @see ACAMERA_CONTROL_AF_REGIONS\n @see ACAMERA_CONTROL_AWB_REGIONS\n @see ACAMERA_LENS_DISTORTION\n @see ACAMERA_SCALER_CROP_REGION\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_tag_ACAMERA_DISTORTION_CORRECTION_MODE: acamera_metadata_tag = 1769472;
#[doc = " <p>List of distortion correction modes for ACAMERA_DISTORTION_CORRECTION_MODE that are\n supported by this camera device.</p>\n\n @see ACAMERA_DISTORTION_CORRECTION_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>No device is required to support this API; such devices will always list only 'OFF'.\n All devices that support this API will list both FAST and HIGH_QUALITY.</p>"]
pub const acamera_metadata_tag_ACAMERA_DISTORTION_CORRECTION_AVAILABLE_MODES: acamera_metadata_tag =
    1769473;
#[doc = " <p>List of distortion correction modes for ACAMERA_DISTORTION_CORRECTION_MODE that are\n supported by this camera device.</p>\n\n @see ACAMERA_DISTORTION_CORRECTION_MODE\n\n <p>Type: byte[n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>No device is required to support this API; such devices will always list only 'OFF'.\n All devices that support this API will list both FAST and HIGH_QUALITY.</p>"]
pub const acamera_metadata_tag_ACAMERA_DISTORTION_CORRECTION_END: acamera_metadata_tag = 1769474;
#[doc = " <p>The available HEIC (ISO/IEC 23008-12) stream\n configurations that this camera device supports\n (i.e. format, width, height, output/input stream).</p>\n\n <p>Type: int32[n*4] (acamera_metadata_enum_android_heic_available_heic_stream_configurations_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>The configurations are listed as <code>(format, width, height, input?)</code> tuples.</p>\n <p>If the camera device supports HEIC image format, it will support identical set of stream\n combinations involving HEIC image format, compared to the combinations involving JPEG\n image format as required by the device's hardware level and capabilities.</p>\n <p>All the static, control, and dynamic metadata tags related to JPEG apply to HEIC formats.\n Configuring JPEG and HEIC streams at the same time is not supported.</p>\n <p>All the configuration tuples <code>(format, width, height, input?)</code> will contain\n AIMAGE_FORMAT_HEIC format as OUTPUT only.</p>"]
pub const acamera_metadata_tag_ACAMERA_HEIC_AVAILABLE_HEIC_STREAM_CONFIGURATIONS:
    acamera_metadata_tag = 1835008;
#[doc = " <p>This lists the minimum frame duration for each\n format/size combination for HEIC output formats.</p>\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This should correspond to the frame duration when only that\n stream is active, with all processing (typically in android.*.mode)\n set to either OFF or FAST.</p>\n <p>When multiple streams are used in a request, the minimum frame\n duration will be max(individual stream min durations).</p>\n <p>See ACAMERA_SENSOR_FRAME_DURATION and\n ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS for more details about\n calculating the max frame rate.</p>\n\n @see ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS\n @see ACAMERA_SENSOR_FRAME_DURATION"]
pub const acamera_metadata_tag_ACAMERA_HEIC_AVAILABLE_HEIC_MIN_FRAME_DURATIONS:
    acamera_metadata_tag = 1835009;
#[doc = " <p>This lists the maximum stall duration for each\n output format/size combination for HEIC streams.</p>\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>A stall duration is how much extra time would get added\n to the normal minimum frame duration for a repeating request\n that has streams with non-zero stall.</p>\n <p>This functions similarly to\n ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS for HEIC\n streams.</p>\n <p>All HEIC output stream formats may have a nonzero stall\n duration.</p>\n\n @see ACAMERA_SCALER_AVAILABLE_STALL_DURATIONS"]
pub const acamera_metadata_tag_ACAMERA_HEIC_AVAILABLE_HEIC_STALL_DURATIONS: acamera_metadata_tag =
    1835010;
#[doc = " <p>The available HEIC (ISO/IEC 23008-12) stream\n configurations that this camera device supports\n (i.e. format, width, height, output/input stream).</p>\n\n <p>Type: int32[n*4] (acamera_metadata_enum_android_heic_available_heic_stream_configurations_maximum_resolution_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Refer to ACAMERA_HEIC_AVAILABLE_HEIC_STREAM_CONFIGURATIONS for details.</p>\n <p>All the configuration tuples <code>(format, width, height, input?)</code> will contain\n AIMAGE_FORMAT_HEIC format as OUTPUT only.</p>\n\n @see ACAMERA_HEIC_AVAILABLE_HEIC_STREAM_CONFIGURATIONS"]
pub const acamera_metadata_tag_ACAMERA_HEIC_AVAILABLE_HEIC_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION : acamera_metadata_tag = 1835011 ;
#[doc = " <p>This lists the minimum frame duration for each\n format/size combination for HEIC output formats for CaptureRequests where\n ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Refer to ACAMERA_HEIC_AVAILABLE_HEIC_MIN_FRAME_DURATIONS for details.</p>\n\n @see ACAMERA_HEIC_AVAILABLE_HEIC_MIN_FRAME_DURATIONS"]
pub const acamera_metadata_tag_ACAMERA_HEIC_AVAILABLE_HEIC_MIN_FRAME_DURATIONS_MAXIMUM_RESOLUTION : acamera_metadata_tag = 1835012 ;
#[doc = " <p>This lists the maximum stall duration for each\n output format/size combination for HEIC streams for CaptureRequests where\n ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Refer to ACAMERA_HEIC_AVAILABLE_HEIC_STALL_DURATIONS for details.</p>\n\n @see ACAMERA_HEIC_AVAILABLE_HEIC_STALL_DURATIONS"]
pub const acamera_metadata_tag_ACAMERA_HEIC_AVAILABLE_HEIC_STALL_DURATIONS_MAXIMUM_RESOLUTION:
    acamera_metadata_tag = 1835013;
#[doc = " <p>This lists the maximum stall duration for each\n output format/size combination for HEIC streams for CaptureRequests where\n ACAMERA_SENSOR_PIXEL_MODE is set to\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION\">CameraMetadata#SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION</a>.</p>\n\n @see ACAMERA_SENSOR_PIXEL_MODE\n\n <p>Type: int64[4*n]</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>Refer to ACAMERA_HEIC_AVAILABLE_HEIC_STALL_DURATIONS for details.</p>\n\n @see ACAMERA_HEIC_AVAILABLE_HEIC_STALL_DURATIONS"]
pub const acamera_metadata_tag_ACAMERA_HEIC_END: acamera_metadata_tag = 1835014;
#[doc = " <p>Location of the cameras on the automotive devices.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_automotive_location_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This enum defines the locations of the cameras relative to the vehicle body frame on\n <a href=\"https://source.android.com/devices/sensors/sensor-types#auto_axes\">the automotive sensor coordinate system</a>.\n If the system has FEATURE_AUTOMOTIVE, the camera will have this entry in its static\n metadata.</p>\n <ul>\n <li>INTERIOR is the inside of the vehicle body frame (or the passenger cabin).</li>\n <li>EXTERIOR is the outside of the vehicle body frame.</li>\n <li>EXTRA is the extra vehicle such as a trailer.</li>\n </ul>\n <p>Each side of the vehicle body frame on this coordinate system is defined as below:</p>\n <ul>\n <li>FRONT is where the Y-axis increases toward.</li>\n <li>REAR is where the Y-axis decreases toward.</li>\n <li>LEFT is where the X-axis decreases toward.</li>\n <li>RIGHT is where the X-axis increases toward.</li>\n </ul>\n <p>If the camera has either EXTERIOR_OTHER or EXTRA_OTHER, its static metadata will list\n the following entries, so that applications can determine the camera's exact location:</p>\n <ul>\n <li>ACAMERA_LENS_POSE_REFERENCE</li>\n <li>ACAMERA_LENS_POSE_ROTATION</li>\n <li>ACAMERA_LENS_POSE_TRANSLATION</li>\n </ul>\n\n @see ACAMERA_LENS_POSE_REFERENCE\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_tag_ACAMERA_AUTOMOTIVE_LOCATION: acamera_metadata_tag = 1966080;
#[doc = " <p>Location of the cameras on the automotive devices.</p>\n\n <p>Type: byte (acamera_metadata_enum_android_automotive_location_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This enum defines the locations of the cameras relative to the vehicle body frame on\n <a href=\"https://source.android.com/devices/sensors/sensor-types#auto_axes\">the automotive sensor coordinate system</a>.\n If the system has FEATURE_AUTOMOTIVE, the camera will have this entry in its static\n metadata.</p>\n <ul>\n <li>INTERIOR is the inside of the vehicle body frame (or the passenger cabin).</li>\n <li>EXTERIOR is the outside of the vehicle body frame.</li>\n <li>EXTRA is the extra vehicle such as a trailer.</li>\n </ul>\n <p>Each side of the vehicle body frame on this coordinate system is defined as below:</p>\n <ul>\n <li>FRONT is where the Y-axis increases toward.</li>\n <li>REAR is where the Y-axis decreases toward.</li>\n <li>LEFT is where the X-axis decreases toward.</li>\n <li>RIGHT is where the X-axis increases toward.</li>\n </ul>\n <p>If the camera has either EXTERIOR_OTHER or EXTRA_OTHER, its static metadata will list\n the following entries, so that applications can determine the camera's exact location:</p>\n <ul>\n <li>ACAMERA_LENS_POSE_REFERENCE</li>\n <li>ACAMERA_LENS_POSE_ROTATION</li>\n <li>ACAMERA_LENS_POSE_TRANSLATION</li>\n </ul>\n\n @see ACAMERA_LENS_POSE_REFERENCE\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_tag_ACAMERA_AUTOMOTIVE_END: acamera_metadata_tag = 1966081;
#[doc = " <p>The direction of the camera faces relative to the vehicle body frame and the\n passenger seats.</p>\n\n <p>Type: byte[n] (acamera_metadata_enum_android_automotive_lens_facing_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This enum defines the lens facing characteristic of the cameras on the automotive\n devices with locations ACAMERA_AUTOMOTIVE_LOCATION defines.  If the system has\n FEATURE_AUTOMOTIVE, the camera will have this entry in its static metadata.</p>\n <p>When ACAMERA_AUTOMOTIVE_LOCATION is INTERIOR, this has one or more INTERIOR_*\n values or a single EXTERIOR_* value.  When this has more than one INTERIOR_*,\n the first value must be the one for the seat closest to the optical axis. If this\n contains INTERIOR_OTHER, all other values will be ineffective.</p>\n <p>When ACAMERA_AUTOMOTIVE_LOCATION is EXTERIOR_* or EXTRA, this has a single\n EXTERIOR_* value.</p>\n <p>If a camera has INTERIOR_OTHER or EXTERIOR_OTHER, or more than one camera is at the\n same location and facing the same direction, their static metadata will list the\n following entries, so that applications can determine their lenses' exact facing\n directions:</p>\n <ul>\n <li>ACAMERA_LENS_POSE_REFERENCE</li>\n <li>ACAMERA_LENS_POSE_ROTATION</li>\n <li>ACAMERA_LENS_POSE_TRANSLATION</li>\n </ul>\n\n @see ACAMERA_AUTOMOTIVE_LOCATION\n @see ACAMERA_LENS_POSE_REFERENCE\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_tag_ACAMERA_AUTOMOTIVE_LENS_FACING: acamera_metadata_tag = 2031616;
#[doc = " <p>The direction of the camera faces relative to the vehicle body frame and the\n passenger seats.</p>\n\n <p>Type: byte[n] (acamera_metadata_enum_android_automotive_lens_facing_t)</p>\n\n <p>This tag may appear in:\n <ul>\n   <li>ACameraMetadata from ACameraManager_getCameraCharacteristics</li>\n </ul></p>\n\n <p>This enum defines the lens facing characteristic of the cameras on the automotive\n devices with locations ACAMERA_AUTOMOTIVE_LOCATION defines.  If the system has\n FEATURE_AUTOMOTIVE, the camera will have this entry in its static metadata.</p>\n <p>When ACAMERA_AUTOMOTIVE_LOCATION is INTERIOR, this has one or more INTERIOR_*\n values or a single EXTERIOR_* value.  When this has more than one INTERIOR_*,\n the first value must be the one for the seat closest to the optical axis. If this\n contains INTERIOR_OTHER, all other values will be ineffective.</p>\n <p>When ACAMERA_AUTOMOTIVE_LOCATION is EXTERIOR_* or EXTRA, this has a single\n EXTERIOR_* value.</p>\n <p>If a camera has INTERIOR_OTHER or EXTERIOR_OTHER, or more than one camera is at the\n same location and facing the same direction, their static metadata will list the\n following entries, so that applications can determine their lenses' exact facing\n directions:</p>\n <ul>\n <li>ACAMERA_LENS_POSE_REFERENCE</li>\n <li>ACAMERA_LENS_POSE_ROTATION</li>\n <li>ACAMERA_LENS_POSE_TRANSLATION</li>\n </ul>\n\n @see ACAMERA_AUTOMOTIVE_LOCATION\n @see ACAMERA_LENS_POSE_REFERENCE\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_tag_ACAMERA_AUTOMOTIVE_LENS_END: acamera_metadata_tag = 2031617;
#[doc = " Main enum for camera metadata tags."]
pub type acamera_metadata_tag = ::std::os::raw::c_uint;
#[doc = " Main enum for camera metadata tags."]
pub use self::acamera_metadata_tag as acamera_metadata_tag_t;
#[doc = " <p>Use the ACAMERA_COLOR_CORRECTION_TRANSFORM matrix\n and ACAMERA_COLOR_CORRECTION_GAINS to do color conversion.</p>\n <p>All advanced white balance adjustments (not specified\n by our white balance pipeline) must be disabled.</p>\n <p>If AWB is enabled with <code>ACAMERA_CONTROL_AWB_MODE != OFF</code>, then\n TRANSFORM_MATRIX is ignored. The camera device will override\n this value to either FAST or HIGH_QUALITY.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM\n @see ACAMERA_CONTROL_AWB_MODE"]
pub const acamera_metadata_enum_acamera_color_correction_mode_ACAMERA_COLOR_CORRECTION_MODE_TRANSFORM_MATRIX : acamera_metadata_enum_acamera_color_correction_mode = 0 ;
#[doc = " <p>Color correction processing must not slow down\n capture rate relative to sensor raw output.</p>\n <p>Advanced white balance adjustments above and beyond\n the specified white balance pipeline may be applied.</p>\n <p>If AWB is enabled with <code>ACAMERA_CONTROL_AWB_MODE != OFF</code>, then\n the camera device uses the last frame's AWB values\n (or defaults if AWB has never been run).</p>\n\n @see ACAMERA_CONTROL_AWB_MODE"]
pub const acamera_metadata_enum_acamera_color_correction_mode_ACAMERA_COLOR_CORRECTION_MODE_FAST:
    acamera_metadata_enum_acamera_color_correction_mode = 1;
#[doc = " <p>Color correction processing operates at improved\n quality but the capture rate might be reduced (relative to sensor\n raw output rate)</p>\n <p>Advanced white balance adjustments above and beyond\n the specified white balance pipeline may be applied.</p>\n <p>If AWB is enabled with <code>ACAMERA_CONTROL_AWB_MODE != OFF</code>, then\n the camera device uses the last frame's AWB values\n (or defaults if AWB has never been run).</p>\n\n @see ACAMERA_CONTROL_AWB_MODE"]
pub const acamera_metadata_enum_acamera_color_correction_mode_ACAMERA_COLOR_CORRECTION_MODE_HIGH_QUALITY : acamera_metadata_enum_acamera_color_correction_mode = 2 ;
#[doc = " Enumeration definitions for the various entries that need them"]
pub type acamera_metadata_enum_acamera_color_correction_mode = ::std::os::raw::c_uint;
#[doc = " Enumeration definitions for the various entries that need them"]
pub use self::acamera_metadata_enum_acamera_color_correction_mode as acamera_metadata_enum_android_color_correction_mode_t;
#[doc = " <p>No aberration correction is applied.</p>"]
pub const acamera_metadata_enum_acamera_color_correction_aberration_mode_ACAMERA_COLOR_CORRECTION_ABERRATION_MODE_OFF : acamera_metadata_enum_acamera_color_correction_aberration_mode = 0 ;
#[doc = " <p>Aberration correction will not slow down capture rate\n relative to sensor raw output.</p>"]
pub const acamera_metadata_enum_acamera_color_correction_aberration_mode_ACAMERA_COLOR_CORRECTION_ABERRATION_MODE_FAST : acamera_metadata_enum_acamera_color_correction_aberration_mode = 1 ;
#[doc = " <p>Aberration correction operates at improved quality but the capture rate might be\n reduced (relative to sensor raw output rate)</p>"]
pub const acamera_metadata_enum_acamera_color_correction_aberration_mode_ACAMERA_COLOR_CORRECTION_ABERRATION_MODE_HIGH_QUALITY : acamera_metadata_enum_acamera_color_correction_aberration_mode = 2 ;
pub type acamera_metadata_enum_acamera_color_correction_aberration_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_color_correction_aberration_mode as acamera_metadata_enum_android_color_correction_aberration_mode_t;
#[doc = " <p>The camera device will not adjust exposure duration to\n avoid banding problems.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_antibanding_mode_ACAMERA_CONTROL_AE_ANTIBANDING_MODE_OFF : acamera_metadata_enum_acamera_control_ae_antibanding_mode = 0 ;
#[doc = " <p>The camera device will adjust exposure duration to\n avoid banding problems with 50Hz illumination sources.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_antibanding_mode_ACAMERA_CONTROL_AE_ANTIBANDING_MODE_50HZ : acamera_metadata_enum_acamera_control_ae_antibanding_mode = 1 ;
#[doc = " <p>The camera device will adjust exposure duration to\n avoid banding problems with 60Hz illumination\n sources.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_antibanding_mode_ACAMERA_CONTROL_AE_ANTIBANDING_MODE_60HZ : acamera_metadata_enum_acamera_control_ae_antibanding_mode = 2 ;
#[doc = " <p>The camera device will automatically adapt its\n antibanding routine to the current illumination\n condition. This is the default mode if AUTO is\n available on given camera device.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_antibanding_mode_ACAMERA_CONTROL_AE_ANTIBANDING_MODE_AUTO : acamera_metadata_enum_acamera_control_ae_antibanding_mode = 3 ;
pub type acamera_metadata_enum_acamera_control_ae_antibanding_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_ae_antibanding_mode as acamera_metadata_enum_android_control_ae_antibanding_mode_t;
#[doc = " <p>Auto-exposure lock is disabled; the AE algorithm\n is free to update its parameters.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_lock_ACAMERA_CONTROL_AE_LOCK_OFF:
    acamera_metadata_enum_acamera_control_ae_lock = 0;
#[doc = " <p>Auto-exposure lock is enabled; the AE algorithm\n must not update the exposure and sensitivity parameters\n while the lock is active.</p>\n <p>ACAMERA_CONTROL_AE_EXPOSURE_COMPENSATION setting changes\n will still take effect while auto-exposure is locked.</p>\n <p>Some rare LEGACY devices may not support\n this, in which case the value will always be overridden to OFF.</p>\n\n @see ACAMERA_CONTROL_AE_EXPOSURE_COMPENSATION"]
pub const acamera_metadata_enum_acamera_control_ae_lock_ACAMERA_CONTROL_AE_LOCK_ON:
    acamera_metadata_enum_acamera_control_ae_lock = 1;
pub type acamera_metadata_enum_acamera_control_ae_lock = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_ae_lock as acamera_metadata_enum_android_control_ae_lock_t;
#[doc = " <p>The camera device's autoexposure routine is disabled.</p>\n <p>The application-selected ACAMERA_SENSOR_EXPOSURE_TIME,\n ACAMERA_SENSOR_SENSITIVITY and\n ACAMERA_SENSOR_FRAME_DURATION are used by the camera\n device, along with ACAMERA_FLASH_* fields, if there's\n a flash unit for this camera device.</p>\n <p>Note that auto-white balance (AWB) and auto-focus (AF)\n behavior is device dependent when AE is in OFF mode.\n To have consistent behavior across different devices,\n it is recommended to either set AWB and AF to OFF mode\n or lock AWB and AF before setting AE to OFF.\n See ACAMERA_CONTROL_AWB_MODE, ACAMERA_CONTROL_AF_MODE,\n ACAMERA_CONTROL_AWB_LOCK, and ACAMERA_CONTROL_AF_TRIGGER\n for more details.</p>\n <p>LEGACY devices do not support the OFF mode and will\n override attempts to use this value to ON.</p>\n\n @see ACAMERA_CONTROL_AF_MODE\n @see ACAMERA_CONTROL_AF_TRIGGER\n @see ACAMERA_CONTROL_AWB_LOCK\n @see ACAMERA_CONTROL_AWB_MODE\n @see ACAMERA_SENSOR_EXPOSURE_TIME\n @see ACAMERA_SENSOR_FRAME_DURATION\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_enum_acamera_control_ae_mode_ACAMERA_CONTROL_AE_MODE_OFF:
    acamera_metadata_enum_acamera_control_ae_mode = 0;
#[doc = " <p>The camera device's autoexposure routine is active,\n with no flash control.</p>\n <p>The application's values for\n ACAMERA_SENSOR_EXPOSURE_TIME,\n ACAMERA_SENSOR_SENSITIVITY, and\n ACAMERA_SENSOR_FRAME_DURATION are ignored. The\n application has control over the various\n ACAMERA_FLASH_* fields.</p>\n\n @see ACAMERA_SENSOR_EXPOSURE_TIME\n @see ACAMERA_SENSOR_FRAME_DURATION\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_enum_acamera_control_ae_mode_ACAMERA_CONTROL_AE_MODE_ON:
    acamera_metadata_enum_acamera_control_ae_mode = 1;
#[doc = " <p>Like ON, except that the camera device also controls\n the camera's flash unit, firing it in low-light\n conditions.</p>\n <p>The flash may be fired during a precapture sequence\n (triggered by ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER) and\n may be fired for captures for which the\n ACAMERA_CONTROL_CAPTURE_INTENT field is set to\n STILL_CAPTURE</p>\n\n @see ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER\n @see ACAMERA_CONTROL_CAPTURE_INTENT"]
pub const acamera_metadata_enum_acamera_control_ae_mode_ACAMERA_CONTROL_AE_MODE_ON_AUTO_FLASH:
    acamera_metadata_enum_acamera_control_ae_mode = 2;
#[doc = " <p>Like ON, except that the camera device also controls\n the camera's flash unit, always firing it for still\n captures.</p>\n <p>The flash may be fired during a precapture sequence\n (triggered by ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER) and\n will always be fired for captures for which the\n ACAMERA_CONTROL_CAPTURE_INTENT field is set to\n STILL_CAPTURE</p>\n\n @see ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER\n @see ACAMERA_CONTROL_CAPTURE_INTENT"]
pub const acamera_metadata_enum_acamera_control_ae_mode_ACAMERA_CONTROL_AE_MODE_ON_ALWAYS_FLASH:
    acamera_metadata_enum_acamera_control_ae_mode = 3;
#[doc = " <p>Like ON_AUTO_FLASH, but with automatic red eye\n reduction.</p>\n <p>If deemed necessary by the camera device, a red eye\n reduction flash will fire during the precapture\n sequence.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_mode_ACAMERA_CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE : acamera_metadata_enum_acamera_control_ae_mode = 4 ;
#[doc = " <p>An external flash has been turned on.</p>\n <p>It informs the camera device that an external flash has been turned on, and that\n metering (and continuous focus if active) should be quickly recaculated to account\n for the external flash. Otherwise, this mode acts like ON.</p>\n <p>When the external flash is turned off, AE mode should be changed to one of the\n other available AE modes.</p>\n <p>If the camera device supports AE external flash mode, ACAMERA_CONTROL_AE_STATE must\n be FLASH_REQUIRED after the camera device finishes AE scan and it's too dark without\n flash.</p>\n\n @see ACAMERA_CONTROL_AE_STATE"]
pub const acamera_metadata_enum_acamera_control_ae_mode_ACAMERA_CONTROL_AE_MODE_ON_EXTERNAL_FLASH : acamera_metadata_enum_acamera_control_ae_mode = 5 ;
pub type acamera_metadata_enum_acamera_control_ae_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_ae_mode as acamera_metadata_enum_android_control_ae_mode_t;
#[doc = " <p>The trigger is idle.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_precapture_trigger_ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER_IDLE : acamera_metadata_enum_acamera_control_ae_precapture_trigger = 0 ;
#[doc = " <p>The precapture metering sequence will be started\n by the camera device.</p>\n <p>The exact effect of the precapture trigger depends on\n the current AE mode and state.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_precapture_trigger_ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER_START : acamera_metadata_enum_acamera_control_ae_precapture_trigger = 1 ;
#[doc = " <p>The camera device will cancel any currently active or completed\n precapture metering sequence, the auto-exposure routine will return to its\n initial state.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_precapture_trigger_ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER_CANCEL : acamera_metadata_enum_acamera_control_ae_precapture_trigger = 2 ;
pub type acamera_metadata_enum_acamera_control_ae_precapture_trigger = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_ae_precapture_trigger as acamera_metadata_enum_android_control_ae_precapture_trigger_t;
#[doc = " <p>The auto-focus routine does not control the lens;\n ACAMERA_LENS_FOCUS_DISTANCE is controlled by the\n application.</p>\n\n @see ACAMERA_LENS_FOCUS_DISTANCE"]
pub const acamera_metadata_enum_acamera_control_af_mode_ACAMERA_CONTROL_AF_MODE_OFF:
    acamera_metadata_enum_acamera_control_af_mode = 0;
#[doc = " <p>Basic automatic focus mode.</p>\n <p>In this mode, the lens does not move unless\n the autofocus trigger action is called. When that trigger\n is activated, AF will transition to ACTIVE_SCAN, then to\n the outcome of the scan (FOCUSED or NOT_FOCUSED).</p>\n <p>Always supported if lens is not fixed focus.</p>\n <p>Use ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE to determine if lens\n is fixed-focus.</p>\n <p>Triggering AF_CANCEL resets the lens position to default,\n and sets the AF state to INACTIVE.</p>\n\n @see ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE"]
pub const acamera_metadata_enum_acamera_control_af_mode_ACAMERA_CONTROL_AF_MODE_AUTO:
    acamera_metadata_enum_acamera_control_af_mode = 1;
#[doc = " <p>Close-up focusing mode.</p>\n <p>In this mode, the lens does not move unless the\n autofocus trigger action is called. When that trigger is\n activated, AF will transition to ACTIVE_SCAN, then to\n the outcome of the scan (FOCUSED or NOT_FOCUSED). This\n mode is optimized for focusing on objects very close to\n the camera.</p>\n <p>When that trigger is activated, AF will transition to\n ACTIVE_SCAN, then to the outcome of the scan (FOCUSED or\n NOT_FOCUSED). Triggering cancel AF resets the lens\n position to default, and sets the AF state to\n INACTIVE.</p>"]
pub const acamera_metadata_enum_acamera_control_af_mode_ACAMERA_CONTROL_AF_MODE_MACRO:
    acamera_metadata_enum_acamera_control_af_mode = 2;
#[doc = " <p>In this mode, the AF algorithm modifies the lens\n position continually to attempt to provide a\n constantly-in-focus image stream.</p>\n <p>The focusing behavior should be suitable for good quality\n video recording; typically this means slower focus\n movement and no overshoots. When the AF trigger is not\n involved, the AF algorithm should start in INACTIVE state,\n and then transition into PASSIVE_SCAN and PASSIVE_FOCUSED\n states as appropriate. When the AF trigger is activated,\n the algorithm should immediately transition into\n AF_FOCUSED or AF_NOT_FOCUSED as appropriate, and lock the\n lens position until a cancel AF trigger is received.</p>\n <p>Once cancel is received, the algorithm should transition\n back to INACTIVE and resume passive scan. Note that this\n behavior is not identical to CONTINUOUS_PICTURE, since an\n ongoing PASSIVE_SCAN must immediately be\n canceled.</p>"]
pub const acamera_metadata_enum_acamera_control_af_mode_ACAMERA_CONTROL_AF_MODE_CONTINUOUS_VIDEO:
    acamera_metadata_enum_acamera_control_af_mode = 3;
#[doc = " <p>In this mode, the AF algorithm modifies the lens\n position continually to attempt to provide a\n constantly-in-focus image stream.</p>\n <p>The focusing behavior should be suitable for still image\n capture; typically this means focusing as fast as\n possible. When the AF trigger is not involved, the AF\n algorithm should start in INACTIVE state, and then\n transition into PASSIVE_SCAN and PASSIVE_FOCUSED states as\n appropriate as it attempts to maintain focus. When the AF\n trigger is activated, the algorithm should finish its\n PASSIVE_SCAN if active, and then transition into\n AF_FOCUSED or AF_NOT_FOCUSED as appropriate, and lock the\n lens position until a cancel AF trigger is received.</p>\n <p>When the AF cancel trigger is activated, the algorithm\n should transition back to INACTIVE and then act as if it\n has just been started.</p>"]
pub const acamera_metadata_enum_acamera_control_af_mode_ACAMERA_CONTROL_AF_MODE_CONTINUOUS_PICTURE : acamera_metadata_enum_acamera_control_af_mode = 4 ;
#[doc = " <p>Extended depth of field (digital focus) mode.</p>\n <p>The camera device will produce images with an extended\n depth of field automatically; no special focusing\n operations need to be done before taking a picture.</p>\n <p>AF triggers are ignored, and the AF state will always be\n INACTIVE.</p>"]
pub const acamera_metadata_enum_acamera_control_af_mode_ACAMERA_CONTROL_AF_MODE_EDOF:
    acamera_metadata_enum_acamera_control_af_mode = 5;
pub type acamera_metadata_enum_acamera_control_af_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_af_mode as acamera_metadata_enum_android_control_af_mode_t;
#[doc = " <p>The trigger is idle.</p>"]
pub const acamera_metadata_enum_acamera_control_af_trigger_ACAMERA_CONTROL_AF_TRIGGER_IDLE:
    acamera_metadata_enum_acamera_control_af_trigger = 0;
#[doc = " <p>Autofocus will trigger now.</p>"]
pub const acamera_metadata_enum_acamera_control_af_trigger_ACAMERA_CONTROL_AF_TRIGGER_START:
    acamera_metadata_enum_acamera_control_af_trigger = 1;
#[doc = " <p>Autofocus will return to its initial\n state, and cancel any currently active trigger.</p>"]
pub const acamera_metadata_enum_acamera_control_af_trigger_ACAMERA_CONTROL_AF_TRIGGER_CANCEL:
    acamera_metadata_enum_acamera_control_af_trigger = 2;
pub type acamera_metadata_enum_acamera_control_af_trigger = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_af_trigger as acamera_metadata_enum_android_control_af_trigger_t;
#[doc = " <p>Auto-white balance lock is disabled; the AWB\n algorithm is free to update its parameters if in AUTO\n mode.</p>"]
pub const acamera_metadata_enum_acamera_control_awb_lock_ACAMERA_CONTROL_AWB_LOCK_OFF:
    acamera_metadata_enum_acamera_control_awb_lock = 0;
#[doc = " <p>Auto-white balance lock is enabled; the AWB\n algorithm will not update its parameters while the lock\n is active.</p>"]
pub const acamera_metadata_enum_acamera_control_awb_lock_ACAMERA_CONTROL_AWB_LOCK_ON:
    acamera_metadata_enum_acamera_control_awb_lock = 1;
pub type acamera_metadata_enum_acamera_control_awb_lock = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_awb_lock as acamera_metadata_enum_android_control_awb_lock_t;
#[doc = " <p>The camera device's auto-white balance routine is disabled.</p>\n <p>The application-selected color transform matrix\n (ACAMERA_COLOR_CORRECTION_TRANSFORM) and gains\n (ACAMERA_COLOR_CORRECTION_GAINS) are used by the camera\n device for manual white balance control.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM"]
pub const acamera_metadata_enum_acamera_control_awb_mode_ACAMERA_CONTROL_AWB_MODE_OFF:
    acamera_metadata_enum_acamera_control_awb_mode = 0;
#[doc = " <p>The camera device's auto-white balance routine is active.</p>\n <p>The application's values for ACAMERA_COLOR_CORRECTION_TRANSFORM\n and ACAMERA_COLOR_CORRECTION_GAINS are ignored.\n For devices that support the MANUAL_POST_PROCESSING capability, the\n values used by the camera device for the transform and gains\n will be available in the capture result for this request.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM"]
pub const acamera_metadata_enum_acamera_control_awb_mode_ACAMERA_CONTROL_AWB_MODE_AUTO:
    acamera_metadata_enum_acamera_control_awb_mode = 1;
#[doc = " <p>The camera device's auto-white balance routine is disabled;\n the camera device uses incandescent light as the assumed scene\n illumination for white balance.</p>\n <p>While the exact white balance transforms are up to the\n camera device, they will approximately match the CIE\n standard illuminant A.</p>\n <p>The application's values for ACAMERA_COLOR_CORRECTION_TRANSFORM\n and ACAMERA_COLOR_CORRECTION_GAINS are ignored.\n For devices that support the MANUAL_POST_PROCESSING capability, the\n values used by the camera device for the transform and gains\n will be available in the capture result for this request.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM"]
pub const acamera_metadata_enum_acamera_control_awb_mode_ACAMERA_CONTROL_AWB_MODE_INCANDESCENT:
    acamera_metadata_enum_acamera_control_awb_mode = 2;
#[doc = " <p>The camera device's auto-white balance routine is disabled;\n the camera device uses fluorescent light as the assumed scene\n illumination for white balance.</p>\n <p>While the exact white balance transforms are up to the\n camera device, they will approximately match the CIE\n standard illuminant F2.</p>\n <p>The application's values for ACAMERA_COLOR_CORRECTION_TRANSFORM\n and ACAMERA_COLOR_CORRECTION_GAINS are ignored.\n For devices that support the MANUAL_POST_PROCESSING capability, the\n values used by the camera device for the transform and gains\n will be available in the capture result for this request.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM"]
pub const acamera_metadata_enum_acamera_control_awb_mode_ACAMERA_CONTROL_AWB_MODE_FLUORESCENT:
    acamera_metadata_enum_acamera_control_awb_mode = 3;
#[doc = " <p>The camera device's auto-white balance routine is disabled;\n the camera device uses warm fluorescent light as the assumed scene\n illumination for white balance.</p>\n <p>While the exact white balance transforms are up to the\n camera device, they will approximately match the CIE\n standard illuminant F4.</p>\n <p>The application's values for ACAMERA_COLOR_CORRECTION_TRANSFORM\n and ACAMERA_COLOR_CORRECTION_GAINS are ignored.\n For devices that support the MANUAL_POST_PROCESSING capability, the\n values used by the camera device for the transform and gains\n will be available in the capture result for this request.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM"]
pub const acamera_metadata_enum_acamera_control_awb_mode_ACAMERA_CONTROL_AWB_MODE_WARM_FLUORESCENT : acamera_metadata_enum_acamera_control_awb_mode = 4 ;
#[doc = " <p>The camera device's auto-white balance routine is disabled;\n the camera device uses daylight light as the assumed scene\n illumination for white balance.</p>\n <p>While the exact white balance transforms are up to the\n camera device, they will approximately match the CIE\n standard illuminant D65.</p>\n <p>The application's values for ACAMERA_COLOR_CORRECTION_TRANSFORM\n and ACAMERA_COLOR_CORRECTION_GAINS are ignored.\n For devices that support the MANUAL_POST_PROCESSING capability, the\n values used by the camera device for the transform and gains\n will be available in the capture result for this request.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM"]
pub const acamera_metadata_enum_acamera_control_awb_mode_ACAMERA_CONTROL_AWB_MODE_DAYLIGHT:
    acamera_metadata_enum_acamera_control_awb_mode = 5;
#[doc = " <p>The camera device's auto-white balance routine is disabled;\n the camera device uses cloudy daylight light as the assumed scene\n illumination for white balance.</p>\n <p>The application's values for ACAMERA_COLOR_CORRECTION_TRANSFORM\n and ACAMERA_COLOR_CORRECTION_GAINS are ignored.\n For devices that support the MANUAL_POST_PROCESSING capability, the\n values used by the camera device for the transform and gains\n will be available in the capture result for this request.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM"]
pub const acamera_metadata_enum_acamera_control_awb_mode_ACAMERA_CONTROL_AWB_MODE_CLOUDY_DAYLIGHT : acamera_metadata_enum_acamera_control_awb_mode = 6 ;
#[doc = " <p>The camera device's auto-white balance routine is disabled;\n the camera device uses twilight light as the assumed scene\n illumination for white balance.</p>\n <p>The application's values for ACAMERA_COLOR_CORRECTION_TRANSFORM\n and ACAMERA_COLOR_CORRECTION_GAINS are ignored.\n For devices that support the MANUAL_POST_PROCESSING capability, the\n values used by the camera device for the transform and gains\n will be available in the capture result for this request.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM"]
pub const acamera_metadata_enum_acamera_control_awb_mode_ACAMERA_CONTROL_AWB_MODE_TWILIGHT:
    acamera_metadata_enum_acamera_control_awb_mode = 7;
#[doc = " <p>The camera device's auto-white balance routine is disabled;\n the camera device uses shade light as the assumed scene\n illumination for white balance.</p>\n <p>The application's values for ACAMERA_COLOR_CORRECTION_TRANSFORM\n and ACAMERA_COLOR_CORRECTION_GAINS are ignored.\n For devices that support the MANUAL_POST_PROCESSING capability, the\n values used by the camera device for the transform and gains\n will be available in the capture result for this request.</p>\n\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM"]
pub const acamera_metadata_enum_acamera_control_awb_mode_ACAMERA_CONTROL_AWB_MODE_SHADE:
    acamera_metadata_enum_acamera_control_awb_mode = 8;
pub type acamera_metadata_enum_acamera_control_awb_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_awb_mode as acamera_metadata_enum_android_control_awb_mode_t;
#[doc = " <p>The goal of this request doesn't fall into the other\n categories. The camera device will default to preview-like\n behavior.</p>"]
pub const acamera_metadata_enum_acamera_control_capture_intent_ACAMERA_CONTROL_CAPTURE_INTENT_CUSTOM : acamera_metadata_enum_acamera_control_capture_intent = 0 ;
#[doc = " <p>This request is for a preview-like use case.</p>\n <p>The precapture trigger may be used to start off a metering\n w/flash sequence.</p>"]
pub const acamera_metadata_enum_acamera_control_capture_intent_ACAMERA_CONTROL_CAPTURE_INTENT_PREVIEW : acamera_metadata_enum_acamera_control_capture_intent = 1 ;
#[doc = " <p>This request is for a still capture-type\n use case.</p>\n <p>If the flash unit is under automatic control, it may fire as needed.</p>"]
pub const acamera_metadata_enum_acamera_control_capture_intent_ACAMERA_CONTROL_CAPTURE_INTENT_STILL_CAPTURE : acamera_metadata_enum_acamera_control_capture_intent = 2 ;
#[doc = " <p>This request is for a video recording\n use case.</p>"]
pub const acamera_metadata_enum_acamera_control_capture_intent_ACAMERA_CONTROL_CAPTURE_INTENT_VIDEO_RECORD : acamera_metadata_enum_acamera_control_capture_intent = 3 ;
#[doc = " <p>This request is for a video snapshot (still\n image while recording video) use case.</p>\n <p>The camera device should take the highest-quality image\n possible (given the other settings) without disrupting the\n frame rate of video recording.  </p>"]
pub const acamera_metadata_enum_acamera_control_capture_intent_ACAMERA_CONTROL_CAPTURE_INTENT_VIDEO_SNAPSHOT : acamera_metadata_enum_acamera_control_capture_intent = 4 ;
#[doc = " <p>This request is for a ZSL usecase; the\n application will stream full-resolution images and\n reprocess one or several later for a final\n capture.</p>"]
pub const acamera_metadata_enum_acamera_control_capture_intent_ACAMERA_CONTROL_CAPTURE_INTENT_ZERO_SHUTTER_LAG : acamera_metadata_enum_acamera_control_capture_intent = 5 ;
#[doc = " <p>This request is for manual capture use case where\n the applications want to directly control the capture parameters.</p>\n <p>For example, the application may wish to manually control\n ACAMERA_SENSOR_EXPOSURE_TIME, ACAMERA_SENSOR_SENSITIVITY, etc.</p>\n\n @see ACAMERA_SENSOR_EXPOSURE_TIME\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_enum_acamera_control_capture_intent_ACAMERA_CONTROL_CAPTURE_INTENT_MANUAL : acamera_metadata_enum_acamera_control_capture_intent = 6 ;
#[doc = " <p>This request is for a motion tracking use case, where\n the application will use camera and inertial sensor data to\n locate and track objects in the world.</p>\n <p>The camera device auto-exposure routine will limit the exposure time\n of the camera to no more than 20 milliseconds, to minimize motion blur.</p>"]
pub const acamera_metadata_enum_acamera_control_capture_intent_ACAMERA_CONTROL_CAPTURE_INTENT_MOTION_TRACKING : acamera_metadata_enum_acamera_control_capture_intent = 7 ;
pub type acamera_metadata_enum_acamera_control_capture_intent = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_capture_intent as acamera_metadata_enum_android_control_capture_intent_t;
#[doc = " <p>No color effect will be applied.</p>"]
pub const acamera_metadata_enum_acamera_control_effect_mode_ACAMERA_CONTROL_EFFECT_MODE_OFF:
    acamera_metadata_enum_acamera_control_effect_mode = 0;
#[doc = " <p>A \"monocolor\" effect where the image is mapped into\n a single color.</p>\n <p>This will typically be grayscale.</p>"]
pub const acamera_metadata_enum_acamera_control_effect_mode_ACAMERA_CONTROL_EFFECT_MODE_MONO:
    acamera_metadata_enum_acamera_control_effect_mode = 1;
#[doc = " <p>A \"photo-negative\" effect where the image's colors\n are inverted.</p>"]
pub const acamera_metadata_enum_acamera_control_effect_mode_ACAMERA_CONTROL_EFFECT_MODE_NEGATIVE:
    acamera_metadata_enum_acamera_control_effect_mode = 2;
#[doc = " <p>A \"solarisation\" effect (Sabattier effect) where the\n image is wholly or partially reversed in\n tone.</p>"]
pub const acamera_metadata_enum_acamera_control_effect_mode_ACAMERA_CONTROL_EFFECT_MODE_SOLARIZE:
    acamera_metadata_enum_acamera_control_effect_mode = 3;
#[doc = " <p>A \"sepia\" effect where the image is mapped into warm\n gray, red, and brown tones.</p>"]
pub const acamera_metadata_enum_acamera_control_effect_mode_ACAMERA_CONTROL_EFFECT_MODE_SEPIA:
    acamera_metadata_enum_acamera_control_effect_mode = 4;
#[doc = " <p>A \"posterization\" effect where the image uses\n discrete regions of tone rather than a continuous\n gradient of tones.</p>"]
pub const acamera_metadata_enum_acamera_control_effect_mode_ACAMERA_CONTROL_EFFECT_MODE_POSTERIZE : acamera_metadata_enum_acamera_control_effect_mode = 5 ;
#[doc = " <p>A \"whiteboard\" effect where the image is typically displayed\n as regions of white, with black or grey details.</p>"]
pub const acamera_metadata_enum_acamera_control_effect_mode_ACAMERA_CONTROL_EFFECT_MODE_WHITEBOARD : acamera_metadata_enum_acamera_control_effect_mode = 6 ;
#[doc = " <p>A \"blackboard\" effect where the image is typically displayed\n as regions of black, with white or grey details.</p>"]
pub const acamera_metadata_enum_acamera_control_effect_mode_ACAMERA_CONTROL_EFFECT_MODE_BLACKBOARD : acamera_metadata_enum_acamera_control_effect_mode = 7 ;
#[doc = " <p>An \"aqua\" effect where a blue hue is added to the image.</p>"]
pub const acamera_metadata_enum_acamera_control_effect_mode_ACAMERA_CONTROL_EFFECT_MODE_AQUA:
    acamera_metadata_enum_acamera_control_effect_mode = 8;
pub type acamera_metadata_enum_acamera_control_effect_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_effect_mode as acamera_metadata_enum_android_control_effect_mode_t;
#[doc = " <p>Full application control of pipeline.</p>\n <p>All control by the device's metering and focusing (3A)\n routines is disabled, and no other settings in\n ACAMERA_CONTROL_* have any effect, except that\n ACAMERA_CONTROL_CAPTURE_INTENT may be used by the camera\n device to select post-processing values for processing\n blocks that do not allow for manual control, or are not\n exposed by the camera API.</p>\n <p>However, the camera device's 3A routines may continue to\n collect statistics and update their internal state so that\n when control is switched to AUTO mode, good control values\n can be immediately applied.</p>\n\n @see ACAMERA_CONTROL_CAPTURE_INTENT"]
pub const acamera_metadata_enum_acamera_control_mode_ACAMERA_CONTROL_MODE_OFF:
    acamera_metadata_enum_acamera_control_mode = 0;
#[doc = " <p>Use settings for each individual 3A routine.</p>\n <p>Manual control of capture parameters is disabled. All\n controls in ACAMERA_CONTROL_* besides sceneMode take\n effect.</p>"]
pub const acamera_metadata_enum_acamera_control_mode_ACAMERA_CONTROL_MODE_AUTO:
    acamera_metadata_enum_acamera_control_mode = 1;
#[doc = " <p>Use a specific scene mode.</p>\n <p>Enabling this disables control.aeMode, control.awbMode and\n control.afMode controls; the camera device will ignore\n those settings while USE_SCENE_MODE is active (except for\n FACE_PRIORITY scene mode). Other control entries are still active.\n This setting can only be used if scene mode is supported (i.e.\n ACAMERA_CONTROL_AVAILABLE_SCENE_MODES\n contain some modes other than DISABLED).</p>\n <p>For extended scene modes such as BOKEH, please use USE_EXTENDED_SCENE_MODE instead.</p>\n\n @see ACAMERA_CONTROL_AVAILABLE_SCENE_MODES"]
pub const acamera_metadata_enum_acamera_control_mode_ACAMERA_CONTROL_MODE_USE_SCENE_MODE:
    acamera_metadata_enum_acamera_control_mode = 2;
#[doc = " <p>Same as OFF mode, except that this capture will not be\n used by camera device background auto-exposure, auto-white balance and\n auto-focus algorithms (3A) to update their statistics.</p>\n <p>Specifically, the 3A routines are locked to the last\n values set from a request with AUTO, OFF, or\n USE_SCENE_MODE, and any statistics or state updates\n collected from manual captures with OFF_KEEP_STATE will be\n discarded by the camera device.</p>"]
pub const acamera_metadata_enum_acamera_control_mode_ACAMERA_CONTROL_MODE_OFF_KEEP_STATE:
    acamera_metadata_enum_acamera_control_mode = 3;
#[doc = " <p>Use a specific extended scene mode.</p>\n <p>When extended scene mode is on, the camera device may override certain control\n parameters, such as targetFpsRange, AE, AWB, and AF modes, to achieve best power and\n quality tradeoffs. Only the mandatory stream combinations of LIMITED hardware level\n are guaranteed.</p>\n <p>This setting can only be used if extended scene mode is supported (i.e.\n android.control.availableExtendedSceneModes\n contains some modes other than DISABLED).</p>"]
pub const acamera_metadata_enum_acamera_control_mode_ACAMERA_CONTROL_MODE_USE_EXTENDED_SCENE_MODE : acamera_metadata_enum_acamera_control_mode = 4 ;
pub type acamera_metadata_enum_acamera_control_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_mode as acamera_metadata_enum_android_control_mode_t;
#[doc = " <p>Indicates that no scene modes are set for a given capture request.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_DISABLED:
    acamera_metadata_enum_acamera_control_scene_mode = 0;
#[doc = " <p>If face detection support exists, use face\n detection data for auto-focus, auto-white balance, and\n auto-exposure routines.</p>\n <p>If face detection statistics are disabled\n (i.e. ACAMERA_STATISTICS_FACE_DETECT_MODE is set to OFF),\n this should still operate correctly (but will not return\n face detection statistics to the framework).</p>\n <p>Unlike the other scene modes, ACAMERA_CONTROL_AE_MODE,\n ACAMERA_CONTROL_AWB_MODE, and ACAMERA_CONTROL_AF_MODE\n remain active when FACE_PRIORITY is set.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_AF_MODE\n @see ACAMERA_CONTROL_AWB_MODE\n @see ACAMERA_STATISTICS_FACE_DETECT_MODE"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_FACE_PRIORITY : acamera_metadata_enum_acamera_control_scene_mode = 1 ;
#[doc = " <p>Optimized for photos of quickly moving objects.</p>\n <p>Similar to SPORTS.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_ACTION:
    acamera_metadata_enum_acamera_control_scene_mode = 2;
#[doc = " <p>Optimized for still photos of people.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_PORTRAIT:
    acamera_metadata_enum_acamera_control_scene_mode = 3;
#[doc = " <p>Optimized for photos of distant macroscopic objects.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_LANDSCAPE:
    acamera_metadata_enum_acamera_control_scene_mode = 4;
#[doc = " <p>Optimized for low-light settings.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_NIGHT:
    acamera_metadata_enum_acamera_control_scene_mode = 5;
#[doc = " <p>Optimized for still photos of people in low-light\n settings.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_NIGHT_PORTRAIT : acamera_metadata_enum_acamera_control_scene_mode = 6 ;
#[doc = " <p>Optimized for dim, indoor settings where flash must\n remain off.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_THEATRE:
    acamera_metadata_enum_acamera_control_scene_mode = 7;
#[doc = " <p>Optimized for bright, outdoor beach settings.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_BEACH:
    acamera_metadata_enum_acamera_control_scene_mode = 8;
#[doc = " <p>Optimized for bright, outdoor settings containing snow.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_SNOW:
    acamera_metadata_enum_acamera_control_scene_mode = 9;
#[doc = " <p>Optimized for scenes of the setting sun.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_SUNSET:
    acamera_metadata_enum_acamera_control_scene_mode = 10;
#[doc = " <p>Optimized to avoid blurry photos due to small amounts of\n device motion (for example: due to hand shake).</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_STEADYPHOTO : acamera_metadata_enum_acamera_control_scene_mode = 11 ;
#[doc = " <p>Optimized for nighttime photos of fireworks.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_FIREWORKS:
    acamera_metadata_enum_acamera_control_scene_mode = 12;
#[doc = " <p>Optimized for photos of quickly moving people.</p>\n <p>Similar to ACTION.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_SPORTS:
    acamera_metadata_enum_acamera_control_scene_mode = 13;
#[doc = " <p>Optimized for dim, indoor settings with multiple moving\n people.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_PARTY:
    acamera_metadata_enum_acamera_control_scene_mode = 14;
#[doc = " <p>Optimized for dim settings where the main light source\n is a candle.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_CANDLELIGHT : acamera_metadata_enum_acamera_control_scene_mode = 15 ;
#[doc = " <p>Optimized for accurately capturing a photo of barcode\n for use by camera applications that wish to read the\n barcode value.</p>"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_BARCODE:
    acamera_metadata_enum_acamera_control_scene_mode = 16;
#[doc = " <p>Turn on a device-specific high dynamic range (HDR) mode.</p>\n <p>In this scene mode, the camera device captures images\n that keep a larger range of scene illumination levels\n visible in the final image. For example, when taking a\n picture of a object in front of a bright window, both\n the object and the scene through the window may be\n visible when using HDR mode, while in normal AUTO mode,\n one or the other may be poorly exposed. As a tradeoff,\n HDR mode generally takes much longer to capture a single\n image, has no user control, and may have other artifacts\n depending on the HDR method used.</p>\n <p>Therefore, HDR captures operate at a much slower rate\n than regular captures.</p>\n <p>In this mode, on LIMITED or FULL devices, when a request\n is made with a ACAMERA_CONTROL_CAPTURE_INTENT of\n STILL_CAPTURE, the camera device will capture an image\n using a high dynamic range capture technique.  On LEGACY\n devices, captures that target a JPEG-format output will\n be captured with HDR, and the capture intent is not\n relevant.</p>\n <p>The HDR capture may involve the device capturing a burst\n of images internally and combining them into one, or it\n may involve the device using specialized high dynamic\n range capture hardware. In all cases, a single image is\n produced in response to a capture request submitted\n while in HDR mode.</p>\n <p>Since substantial post-processing is generally needed to\n produce an HDR image, only YUV, PRIVATE, and JPEG\n outputs are supported for LIMITED/FULL device HDR\n captures, and only JPEG outputs are supported for LEGACY\n HDR captures. Using a RAW output for HDR capture is not\n supported.</p>\n <p>Some devices may also support always-on HDR, which\n applies HDR processing at full frame rate.  For these\n devices, intents other than STILL_CAPTURE will also\n produce an HDR output with no frame rate impact compared\n to normal operation, though the quality may be lower\n than for STILL_CAPTURE intents.</p>\n <p>If SCENE_MODE_HDR is used with unsupported output types\n or capture intents, the images captured will be as if\n the SCENE_MODE was not enabled at all.</p>\n\n @see ACAMERA_CONTROL_CAPTURE_INTENT"]
pub const acamera_metadata_enum_acamera_control_scene_mode_ACAMERA_CONTROL_SCENE_MODE_HDR:
    acamera_metadata_enum_acamera_control_scene_mode = 18;
pub type acamera_metadata_enum_acamera_control_scene_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_scene_mode as acamera_metadata_enum_android_control_scene_mode_t;
#[doc = " <p>Video stabilization is disabled.</p>"]
pub const acamera_metadata_enum_acamera_control_video_stabilization_mode_ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE_OFF : acamera_metadata_enum_acamera_control_video_stabilization_mode = 0 ;
#[doc = " <p>Video stabilization is enabled.</p>"]
pub const acamera_metadata_enum_acamera_control_video_stabilization_mode_ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE_ON : acamera_metadata_enum_acamera_control_video_stabilization_mode = 1 ;
#[doc = " <p>Preview stabilization, where the preview in addition to all other non-RAW streams are\n stabilized with the same quality of stabilization, is enabled. This mode aims to give\n clients a 'what you see is what you get' effect. In this mode, the FoV reduction will\n be a maximum of 20 % both horizontally and vertically\n (10% from left, right, top, bottom) for the given zoom ratio / crop region.\n The resultant FoV will also be the same across all processed streams\n (that have the same aspect ratio).</p>"]
pub const acamera_metadata_enum_acamera_control_video_stabilization_mode_ACAMERA_CONTROL_VIDEO_STABILIZATION_MODE_PREVIEW_STABILIZATION : acamera_metadata_enum_acamera_control_video_stabilization_mode = 2 ;
pub type acamera_metadata_enum_acamera_control_video_stabilization_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_video_stabilization_mode as acamera_metadata_enum_android_control_video_stabilization_mode_t;
#[doc = " <p>AE is off or recently reset.</p>\n <p>When a camera device is opened, it starts in\n this state. This is a transient state, the camera device may skip reporting\n this state in capture result.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_state_ACAMERA_CONTROL_AE_STATE_INACTIVE:
    acamera_metadata_enum_acamera_control_ae_state = 0;
#[doc = " <p>AE doesn't yet have a good set of control values\n for the current scene.</p>\n <p>This is a transient state, the camera device may skip\n reporting this state in capture result.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_state_ACAMERA_CONTROL_AE_STATE_SEARCHING:
    acamera_metadata_enum_acamera_control_ae_state = 1;
#[doc = " <p>AE has a good set of control values for the\n current scene.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_state_ACAMERA_CONTROL_AE_STATE_CONVERGED:
    acamera_metadata_enum_acamera_control_ae_state = 2;
#[doc = " <p>AE has been locked.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_state_ACAMERA_CONTROL_AE_STATE_LOCKED:
    acamera_metadata_enum_acamera_control_ae_state = 3;
#[doc = " <p>AE has a good set of control values, but flash\n needs to be fired for good quality still\n capture.</p>"]
pub const acamera_metadata_enum_acamera_control_ae_state_ACAMERA_CONTROL_AE_STATE_FLASH_REQUIRED:
    acamera_metadata_enum_acamera_control_ae_state = 4;
#[doc = " <p>AE has been asked to do a precapture sequence\n and is currently executing it.</p>\n <p>Precapture can be triggered through setting\n ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER to START. Currently\n active and completed (if it causes camera device internal AE lock) precapture\n metering sequence can be canceled through setting\n ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER to CANCEL.</p>\n <p>Once PRECAPTURE completes, AE will transition to CONVERGED\n or FLASH_REQUIRED as appropriate. This is a transient\n state, the camera device may skip reporting this state in\n capture result.</p>\n\n @see ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER"]
pub const acamera_metadata_enum_acamera_control_ae_state_ACAMERA_CONTROL_AE_STATE_PRECAPTURE:
    acamera_metadata_enum_acamera_control_ae_state = 5;
pub type acamera_metadata_enum_acamera_control_ae_state = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_ae_state as acamera_metadata_enum_android_control_ae_state_t;
#[doc = " <p>AF is off or has not yet tried to scan/been asked\n to scan.</p>\n <p>When a camera device is opened, it starts in this\n state. This is a transient state, the camera device may\n skip reporting this state in capture\n result.</p>"]
pub const acamera_metadata_enum_acamera_control_af_state_ACAMERA_CONTROL_AF_STATE_INACTIVE:
    acamera_metadata_enum_acamera_control_af_state = 0;
#[doc = " <p>AF is currently performing an AF scan initiated the\n camera device in a continuous autofocus mode.</p>\n <p>Only used by CONTINUOUS_* AF modes. This is a transient\n state, the camera device may skip reporting this state in\n capture result.</p>"]
pub const acamera_metadata_enum_acamera_control_af_state_ACAMERA_CONTROL_AF_STATE_PASSIVE_SCAN:
    acamera_metadata_enum_acamera_control_af_state = 1;
#[doc = " <p>AF currently believes it is in focus, but may\n restart scanning at any time.</p>\n <p>Only used by CONTINUOUS_* AF modes. This is a transient\n state, the camera device may skip reporting this state in\n capture result.</p>"]
pub const acamera_metadata_enum_acamera_control_af_state_ACAMERA_CONTROL_AF_STATE_PASSIVE_FOCUSED : acamera_metadata_enum_acamera_control_af_state = 2 ;
#[doc = " <p>AF is performing an AF scan because it was\n triggered by AF trigger.</p>\n <p>Only used by AUTO or MACRO AF modes. This is a transient\n state, the camera device may skip reporting this state in\n capture result.</p>"]
pub const acamera_metadata_enum_acamera_control_af_state_ACAMERA_CONTROL_AF_STATE_ACTIVE_SCAN:
    acamera_metadata_enum_acamera_control_af_state = 3;
#[doc = " <p>AF believes it is focused correctly and has locked\n focus.</p>\n <p>This state is reached only after an explicit START AF trigger has been\n sent (ACAMERA_CONTROL_AF_TRIGGER), when good focus has been obtained.</p>\n <p>The lens will remain stationary until the AF mode (ACAMERA_CONTROL_AF_MODE) is changed or\n a new AF trigger is sent to the camera device (ACAMERA_CONTROL_AF_TRIGGER).</p>\n\n @see ACAMERA_CONTROL_AF_MODE\n @see ACAMERA_CONTROL_AF_TRIGGER"]
pub const acamera_metadata_enum_acamera_control_af_state_ACAMERA_CONTROL_AF_STATE_FOCUSED_LOCKED:
    acamera_metadata_enum_acamera_control_af_state = 4;
#[doc = " <p>AF has failed to focus successfully and has locked\n focus.</p>\n <p>This state is reached only after an explicit START AF trigger has been\n sent (ACAMERA_CONTROL_AF_TRIGGER), when good focus cannot be obtained.</p>\n <p>The lens will remain stationary until the AF mode (ACAMERA_CONTROL_AF_MODE) is changed or\n a new AF trigger is sent to the camera device (ACAMERA_CONTROL_AF_TRIGGER).</p>\n\n @see ACAMERA_CONTROL_AF_MODE\n @see ACAMERA_CONTROL_AF_TRIGGER"]
pub const acamera_metadata_enum_acamera_control_af_state_ACAMERA_CONTROL_AF_STATE_NOT_FOCUSED_LOCKED : acamera_metadata_enum_acamera_control_af_state = 5 ;
#[doc = " <p>AF finished a passive scan without finding focus,\n and may restart scanning at any time.</p>\n <p>Only used by CONTINUOUS_* AF modes. This is a transient state, the camera\n device may skip reporting this state in capture result.</p>\n <p>LEGACY camera devices do not support this state. When a passive\n scan has finished, it will always go to PASSIVE_FOCUSED.</p>"]
pub const acamera_metadata_enum_acamera_control_af_state_ACAMERA_CONTROL_AF_STATE_PASSIVE_UNFOCUSED : acamera_metadata_enum_acamera_control_af_state = 6 ;
pub type acamera_metadata_enum_acamera_control_af_state = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_af_state as acamera_metadata_enum_android_control_af_state_t;
#[doc = " <p>AWB is not in auto mode, or has not yet started metering.</p>\n <p>When a camera device is opened, it starts in this\n state. This is a transient state, the camera device may\n skip reporting this state in capture\n result.</p>"]
pub const acamera_metadata_enum_acamera_control_awb_state_ACAMERA_CONTROL_AWB_STATE_INACTIVE:
    acamera_metadata_enum_acamera_control_awb_state = 0;
#[doc = " <p>AWB doesn't yet have a good set of control\n values for the current scene.</p>\n <p>This is a transient state, the camera device\n may skip reporting this state in capture result.</p>"]
pub const acamera_metadata_enum_acamera_control_awb_state_ACAMERA_CONTROL_AWB_STATE_SEARCHING:
    acamera_metadata_enum_acamera_control_awb_state = 1;
#[doc = " <p>AWB has a good set of control values for the\n current scene.</p>"]
pub const acamera_metadata_enum_acamera_control_awb_state_ACAMERA_CONTROL_AWB_STATE_CONVERGED:
    acamera_metadata_enum_acamera_control_awb_state = 2;
#[doc = " <p>AWB has been locked.</p>"]
pub const acamera_metadata_enum_acamera_control_awb_state_ACAMERA_CONTROL_AWB_STATE_LOCKED:
    acamera_metadata_enum_acamera_control_awb_state = 3;
pub type acamera_metadata_enum_acamera_control_awb_state = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_awb_state as acamera_metadata_enum_android_control_awb_state_t;
pub const acamera_metadata_enum_acamera_control_ae_lock_available_ACAMERA_CONTROL_AE_LOCK_AVAILABLE_FALSE : acamera_metadata_enum_acamera_control_ae_lock_available = 0 ;
pub const acamera_metadata_enum_acamera_control_ae_lock_available_ACAMERA_CONTROL_AE_LOCK_AVAILABLE_TRUE : acamera_metadata_enum_acamera_control_ae_lock_available = 1 ;
pub type acamera_metadata_enum_acamera_control_ae_lock_available = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_ae_lock_available as acamera_metadata_enum_android_control_ae_lock_available_t;
pub const acamera_metadata_enum_acamera_control_awb_lock_available_ACAMERA_CONTROL_AWB_LOCK_AVAILABLE_FALSE : acamera_metadata_enum_acamera_control_awb_lock_available = 0 ;
pub const acamera_metadata_enum_acamera_control_awb_lock_available_ACAMERA_CONTROL_AWB_LOCK_AVAILABLE_TRUE : acamera_metadata_enum_acamera_control_awb_lock_available = 1 ;
pub type acamera_metadata_enum_acamera_control_awb_lock_available = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_awb_lock_available as acamera_metadata_enum_android_control_awb_lock_available_t;
#[doc = " <p>Requests with ACAMERA_CONTROL_CAPTURE_INTENT == STILL_CAPTURE must be captured\n after previous requests.</p>\n\n @see ACAMERA_CONTROL_CAPTURE_INTENT"]
pub const acamera_metadata_enum_acamera_control_enable_zsl_ACAMERA_CONTROL_ENABLE_ZSL_FALSE:
    acamera_metadata_enum_acamera_control_enable_zsl = 0;
#[doc = " <p>Requests with ACAMERA_CONTROL_CAPTURE_INTENT == STILL_CAPTURE may or may not be\n captured before previous requests.</p>\n\n @see ACAMERA_CONTROL_CAPTURE_INTENT"]
pub const acamera_metadata_enum_acamera_control_enable_zsl_ACAMERA_CONTROL_ENABLE_ZSL_TRUE:
    acamera_metadata_enum_acamera_control_enable_zsl = 1;
pub type acamera_metadata_enum_acamera_control_enable_zsl = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_enable_zsl as acamera_metadata_enum_android_control_enable_zsl_t;
#[doc = " <p>Scene change is not detected within the AF region(s).</p>"]
pub const acamera_metadata_enum_acamera_control_af_scene_change_ACAMERA_CONTROL_AF_SCENE_CHANGE_NOT_DETECTED : acamera_metadata_enum_acamera_control_af_scene_change = 0 ;
#[doc = " <p>Scene change is detected within the AF region(s).</p>"]
pub const acamera_metadata_enum_acamera_control_af_scene_change_ACAMERA_CONTROL_AF_SCENE_CHANGE_DETECTED : acamera_metadata_enum_acamera_control_af_scene_change = 1 ;
pub type acamera_metadata_enum_acamera_control_af_scene_change = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_af_scene_change as acamera_metadata_enum_android_control_af_scene_change_t;
#[doc = " <p>Extended scene mode is disabled.</p>"]
pub const acamera_metadata_enum_acamera_control_extended_scene_mode_ACAMERA_CONTROL_EXTENDED_SCENE_MODE_DISABLED : acamera_metadata_enum_acamera_control_extended_scene_mode = 0 ;
#[doc = " <p>High quality bokeh mode is enabled for all non-raw streams (including YUV,\n JPEG, and IMPLEMENTATION_DEFINED) when capture intent is STILL_CAPTURE. Due to the\n extra image processing, this mode may introduce additional stall to non-raw streams.\n This mode should be used in high quality still capture use case.</p>"]
pub const acamera_metadata_enum_acamera_control_extended_scene_mode_ACAMERA_CONTROL_EXTENDED_SCENE_MODE_BOKEH_STILL_CAPTURE : acamera_metadata_enum_acamera_control_extended_scene_mode = 1 ;
#[doc = " <p>Bokeh effect must not slow down capture rate relative to sensor raw output,\n and the effect is applied to all processed streams no larger than the maximum\n streaming dimension. This mode should be used if performance and power are a\n priority, such as video recording.</p>"]
pub const acamera_metadata_enum_acamera_control_extended_scene_mode_ACAMERA_CONTROL_EXTENDED_SCENE_MODE_BOKEH_CONTINUOUS : acamera_metadata_enum_acamera_control_extended_scene_mode = 2 ;
pub type acamera_metadata_enum_acamera_control_extended_scene_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_control_extended_scene_mode as acamera_metadata_enum_android_control_extended_scene_mode_t;
#[doc = " <p>No edge enhancement is applied.</p>"]
pub const acamera_metadata_enum_acamera_edge_mode_ACAMERA_EDGE_MODE_OFF:
    acamera_metadata_enum_acamera_edge_mode = 0;
#[doc = " <p>Apply edge enhancement at a quality level that does not slow down frame rate\n relative to sensor output. It may be the same as OFF if edge enhancement will\n slow down frame rate relative to sensor.</p>"]
pub const acamera_metadata_enum_acamera_edge_mode_ACAMERA_EDGE_MODE_FAST:
    acamera_metadata_enum_acamera_edge_mode = 1;
#[doc = " <p>Apply high-quality edge enhancement, at a cost of possibly reduced output frame rate.</p>"]
pub const acamera_metadata_enum_acamera_edge_mode_ACAMERA_EDGE_MODE_HIGH_QUALITY:
    acamera_metadata_enum_acamera_edge_mode = 2;
#[doc = " <p>Edge enhancement is applied at different\n levels for different output streams, based on resolution. Streams at maximum recording\n resolution (see {@link ACameraDevice_createCaptureSession })\n or below have edge enhancement applied, while higher-resolution streams have no edge\n enhancement applied. The level of edge enhancement for low-resolution streams is tuned\n so that frame rate is not impacted, and the quality is equal to or better than FAST\n (since it is only applied to lower-resolution outputs, quality may improve from FAST).</p>\n <p>This mode is intended to be used by applications operating in a zero-shutter-lag mode\n with YUV or PRIVATE reprocessing, where the application continuously captures\n high-resolution intermediate buffers into a circular buffer, from which a final image is\n produced via reprocessing when a user takes a picture.  For such a use case, the\n high-resolution buffers must not have edge enhancement applied to maximize efficiency of\n preview and to avoid double-applying enhancement when reprocessed, while low-resolution\n buffers (used for recording or preview, generally) need edge enhancement applied for\n reasonable preview quality.</p>\n <p>This mode is guaranteed to be supported by devices that support either the\n YUV_REPROCESSING or PRIVATE_REPROCESSING capabilities\n (ACAMERA_REQUEST_AVAILABLE_CAPABILITIES lists either of those capabilities) and it will\n be the default mode for CAMERA3_TEMPLATE_ZERO_SHUTTER_LAG template.</p>\n\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES"]
pub const acamera_metadata_enum_acamera_edge_mode_ACAMERA_EDGE_MODE_ZERO_SHUTTER_LAG:
    acamera_metadata_enum_acamera_edge_mode = 3;
pub type acamera_metadata_enum_acamera_edge_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_edge_mode as acamera_metadata_enum_android_edge_mode_t;
#[doc = " <p>Do not fire the flash for this capture.</p>"]
pub const acamera_metadata_enum_acamera_flash_mode_ACAMERA_FLASH_MODE_OFF:
    acamera_metadata_enum_acamera_flash_mode = 0;
#[doc = " <p>If the flash is available and charged, fire flash\n for this capture.</p>"]
pub const acamera_metadata_enum_acamera_flash_mode_ACAMERA_FLASH_MODE_SINGLE:
    acamera_metadata_enum_acamera_flash_mode = 1;
#[doc = " <p>Transition flash to continuously on.</p>"]
pub const acamera_metadata_enum_acamera_flash_mode_ACAMERA_FLASH_MODE_TORCH:
    acamera_metadata_enum_acamera_flash_mode = 2;
pub type acamera_metadata_enum_acamera_flash_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_flash_mode as acamera_metadata_enum_android_flash_mode_t;
#[doc = " <p>No flash on camera.</p>"]
pub const acamera_metadata_enum_acamera_flash_state_ACAMERA_FLASH_STATE_UNAVAILABLE:
    acamera_metadata_enum_acamera_flash_state = 0;
#[doc = " <p>Flash is charging and cannot be fired.</p>"]
pub const acamera_metadata_enum_acamera_flash_state_ACAMERA_FLASH_STATE_CHARGING:
    acamera_metadata_enum_acamera_flash_state = 1;
#[doc = " <p>Flash is ready to fire.</p>"]
pub const acamera_metadata_enum_acamera_flash_state_ACAMERA_FLASH_STATE_READY:
    acamera_metadata_enum_acamera_flash_state = 2;
#[doc = " <p>Flash fired for this capture.</p>"]
pub const acamera_metadata_enum_acamera_flash_state_ACAMERA_FLASH_STATE_FIRED:
    acamera_metadata_enum_acamera_flash_state = 3;
#[doc = " <p>Flash partially illuminated this frame.</p>\n <p>This is usually due to the next or previous frame having\n the flash fire, and the flash spilling into this capture\n due to hardware limitations.</p>"]
pub const acamera_metadata_enum_acamera_flash_state_ACAMERA_FLASH_STATE_PARTIAL:
    acamera_metadata_enum_acamera_flash_state = 4;
pub type acamera_metadata_enum_acamera_flash_state = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_flash_state as acamera_metadata_enum_android_flash_state_t;
pub const acamera_metadata_enum_acamera_flash_info_available_ACAMERA_FLASH_INFO_AVAILABLE_FALSE:
    acamera_metadata_enum_acamera_flash_info_available = 0;
pub const acamera_metadata_enum_acamera_flash_info_available_ACAMERA_FLASH_INFO_AVAILABLE_TRUE:
    acamera_metadata_enum_acamera_flash_info_available = 1;
pub type acamera_metadata_enum_acamera_flash_info_available = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_flash_info_available as acamera_metadata_enum_android_flash_info_available_t;
#[doc = " <p>No hot pixel correction is applied.</p>\n <p>The frame rate must not be reduced relative to sensor raw output\n for this option.</p>\n <p>The hotpixel map may be returned in ACAMERA_STATISTICS_HOT_PIXEL_MAP.</p>\n\n @see ACAMERA_STATISTICS_HOT_PIXEL_MAP"]
pub const acamera_metadata_enum_acamera_hot_pixel_mode_ACAMERA_HOT_PIXEL_MODE_OFF:
    acamera_metadata_enum_acamera_hot_pixel_mode = 0;
#[doc = " <p>Hot pixel correction is applied, without reducing frame\n rate relative to sensor raw output.</p>\n <p>The hotpixel map may be returned in ACAMERA_STATISTICS_HOT_PIXEL_MAP.</p>\n\n @see ACAMERA_STATISTICS_HOT_PIXEL_MAP"]
pub const acamera_metadata_enum_acamera_hot_pixel_mode_ACAMERA_HOT_PIXEL_MODE_FAST:
    acamera_metadata_enum_acamera_hot_pixel_mode = 1;
#[doc = " <p>High-quality hot pixel correction is applied, at a cost\n of possibly reduced frame rate relative to sensor raw output.</p>\n <p>The hotpixel map may be returned in ACAMERA_STATISTICS_HOT_PIXEL_MAP.</p>\n\n @see ACAMERA_STATISTICS_HOT_PIXEL_MAP"]
pub const acamera_metadata_enum_acamera_hot_pixel_mode_ACAMERA_HOT_PIXEL_MODE_HIGH_QUALITY:
    acamera_metadata_enum_acamera_hot_pixel_mode = 2;
pub type acamera_metadata_enum_acamera_hot_pixel_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_hot_pixel_mode as acamera_metadata_enum_android_hot_pixel_mode_t;
#[doc = " <p>Optical stabilization is unavailable.</p>"]
pub const acamera_metadata_enum_acamera_lens_optical_stabilization_mode_ACAMERA_LENS_OPTICAL_STABILIZATION_MODE_OFF : acamera_metadata_enum_acamera_lens_optical_stabilization_mode = 0 ;
#[doc = " <p>Optical stabilization is enabled.</p>"]
pub const acamera_metadata_enum_acamera_lens_optical_stabilization_mode_ACAMERA_LENS_OPTICAL_STABILIZATION_MODE_ON : acamera_metadata_enum_acamera_lens_optical_stabilization_mode = 1 ;
pub type acamera_metadata_enum_acamera_lens_optical_stabilization_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_lens_optical_stabilization_mode as acamera_metadata_enum_android_lens_optical_stabilization_mode_t;
#[doc = " <p>The camera device faces the same direction as the device's screen.</p>"]
pub const acamera_metadata_enum_acamera_lens_facing_ACAMERA_LENS_FACING_FRONT:
    acamera_metadata_enum_acamera_lens_facing = 0;
#[doc = " <p>The camera device faces the opposite direction as the device's screen.</p>"]
pub const acamera_metadata_enum_acamera_lens_facing_ACAMERA_LENS_FACING_BACK:
    acamera_metadata_enum_acamera_lens_facing = 1;
#[doc = " <p>The camera device is an external camera, and has no fixed facing relative to the\n device's screen.</p>"]
pub const acamera_metadata_enum_acamera_lens_facing_ACAMERA_LENS_FACING_EXTERNAL:
    acamera_metadata_enum_acamera_lens_facing = 2;
pub type acamera_metadata_enum_acamera_lens_facing = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_lens_facing as acamera_metadata_enum_android_lens_facing_t;
#[doc = " <p>The lens parameters (ACAMERA_LENS_FOCAL_LENGTH, ACAMERA_LENS_FOCUS_DISTANCE,\n ACAMERA_LENS_FILTER_DENSITY and ACAMERA_LENS_APERTURE) are not changing.</p>\n\n @see ACAMERA_LENS_APERTURE\n @see ACAMERA_LENS_FILTER_DENSITY\n @see ACAMERA_LENS_FOCAL_LENGTH\n @see ACAMERA_LENS_FOCUS_DISTANCE"]
pub const acamera_metadata_enum_acamera_lens_state_ACAMERA_LENS_STATE_STATIONARY:
    acamera_metadata_enum_acamera_lens_state = 0;
#[doc = " <p>One or several of the lens parameters\n (ACAMERA_LENS_FOCAL_LENGTH, ACAMERA_LENS_FOCUS_DISTANCE,\n ACAMERA_LENS_FILTER_DENSITY or ACAMERA_LENS_APERTURE) is\n currently changing.</p>\n\n @see ACAMERA_LENS_APERTURE\n @see ACAMERA_LENS_FILTER_DENSITY\n @see ACAMERA_LENS_FOCAL_LENGTH\n @see ACAMERA_LENS_FOCUS_DISTANCE"]
pub const acamera_metadata_enum_acamera_lens_state_ACAMERA_LENS_STATE_MOVING:
    acamera_metadata_enum_acamera_lens_state = 1;
pub type acamera_metadata_enum_acamera_lens_state = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_lens_state as acamera_metadata_enum_android_lens_state_t;
#[doc = " <p>The value of ACAMERA_LENS_POSE_TRANSLATION is relative to the optical center of\n the largest camera device facing the same direction as this camera.</p>\n <p>This is the default value for API levels before Android P.</p>\n\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_enum_acamera_lens_pose_reference_ACAMERA_LENS_POSE_REFERENCE_PRIMARY_CAMERA : acamera_metadata_enum_acamera_lens_pose_reference = 0 ;
#[doc = " <p>The value of ACAMERA_LENS_POSE_TRANSLATION is relative to the position of the\n primary gyroscope of this Android device.</p>\n\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_enum_acamera_lens_pose_reference_ACAMERA_LENS_POSE_REFERENCE_GYROSCOPE : acamera_metadata_enum_acamera_lens_pose_reference = 1 ;
#[doc = " <p>The camera device cannot represent the values of ACAMERA_LENS_POSE_TRANSLATION\n and ACAMERA_LENS_POSE_ROTATION accurately enough. One such example is a camera device\n on the cover of a foldable phone: in order to measure the pose translation and rotation,\n some kind of hinge position sensor would be needed.</p>\n <p>The value of ACAMERA_LENS_POSE_TRANSLATION must be all zeros, and\n ACAMERA_LENS_POSE_ROTATION must be values matching its default facing.</p>\n\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_enum_acamera_lens_pose_reference_ACAMERA_LENS_POSE_REFERENCE_UNDEFINED : acamera_metadata_enum_acamera_lens_pose_reference = 2 ;
#[doc = " <p>The value of ACAMERA_LENS_POSE_TRANSLATION is relative to the origin of the\n automotive sensor coordinate system, which is at the center of the rear axle.</p>\n\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_enum_acamera_lens_pose_reference_ACAMERA_LENS_POSE_REFERENCE_AUTOMOTIVE : acamera_metadata_enum_acamera_lens_pose_reference = 3 ;
pub type acamera_metadata_enum_acamera_lens_pose_reference = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_lens_pose_reference as acamera_metadata_enum_android_lens_pose_reference_t;
#[doc = " <p>The lens focus distance is not accurate, and the units used for\n ACAMERA_LENS_FOCUS_DISTANCE do not correspond to any physical units.</p>\n <p>Setting the lens to the same focus distance on separate occasions may\n result in a different real focus distance, depending on factors such\n as the orientation of the device, the age of the focusing mechanism,\n and the device temperature. The focus distance value will still be\n in the range of <code>[0, ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE]</code>, where 0\n represents the farthest focus.</p>\n\n @see ACAMERA_LENS_FOCUS_DISTANCE\n @see ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE"]
pub const acamera_metadata_enum_acamera_lens_info_focus_distance_calibration_ACAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_UNCALIBRATED : acamera_metadata_enum_acamera_lens_info_focus_distance_calibration = 0 ;
#[doc = " <p>The lens focus distance is measured in diopters.</p>\n <p>However, setting the lens to the same focus distance\n on separate occasions may result in a different real\n focus distance, depending on factors such as the\n orientation of the device, the age of the focusing\n mechanism, and the device temperature.</p>"]
pub const acamera_metadata_enum_acamera_lens_info_focus_distance_calibration_ACAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_APPROXIMATE : acamera_metadata_enum_acamera_lens_info_focus_distance_calibration = 1 ;
#[doc = " <p>The lens focus distance is measured in diopters, and\n is calibrated.</p>\n <p>The lens mechanism is calibrated so that setting the\n same focus distance is repeatable on multiple\n occasions with good accuracy, and the focus distance\n corresponds to the real physical distance to the plane\n of best focus.</p>"]
pub const acamera_metadata_enum_acamera_lens_info_focus_distance_calibration_ACAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_CALIBRATED : acamera_metadata_enum_acamera_lens_info_focus_distance_calibration = 2 ;
pub type acamera_metadata_enum_acamera_lens_info_focus_distance_calibration =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_lens_info_focus_distance_calibration as acamera_metadata_enum_android_lens_info_focus_distance_calibration_t;
#[doc = " <p>No noise reduction is applied.</p>"]
pub const acamera_metadata_enum_acamera_noise_reduction_mode_ACAMERA_NOISE_REDUCTION_MODE_OFF:
    acamera_metadata_enum_acamera_noise_reduction_mode = 0;
#[doc = " <p>Noise reduction is applied without reducing frame rate relative to sensor\n output. It may be the same as OFF if noise reduction will reduce frame rate\n relative to sensor.</p>"]
pub const acamera_metadata_enum_acamera_noise_reduction_mode_ACAMERA_NOISE_REDUCTION_MODE_FAST:
    acamera_metadata_enum_acamera_noise_reduction_mode = 1;
#[doc = " <p>High-quality noise reduction is applied, at the cost of possibly reduced frame\n rate relative to sensor output.</p>"]
pub const acamera_metadata_enum_acamera_noise_reduction_mode_ACAMERA_NOISE_REDUCTION_MODE_HIGH_QUALITY : acamera_metadata_enum_acamera_noise_reduction_mode = 2 ;
#[doc = " <p>MINIMAL noise reduction is applied without reducing frame rate relative to\n sensor output. </p>"]
pub const acamera_metadata_enum_acamera_noise_reduction_mode_ACAMERA_NOISE_REDUCTION_MODE_MINIMAL : acamera_metadata_enum_acamera_noise_reduction_mode = 3 ;
#[doc = " <p>Noise reduction is applied at different levels for different output streams,\n based on resolution. Streams at maximum recording resolution (see {@link ACameraDevice_createCaptureSession })\n or below have noise reduction applied, while higher-resolution streams have MINIMAL (if\n supported) or no noise reduction applied (if MINIMAL is not supported.) The degree of\n noise reduction for low-resolution streams is tuned so that frame rate is not impacted,\n and the quality is equal to or better than FAST (since it is only applied to\n lower-resolution outputs, quality may improve from FAST).</p>\n <p>This mode is intended to be used by applications operating in a zero-shutter-lag mode\n with YUV or PRIVATE reprocessing, where the application continuously captures\n high-resolution intermediate buffers into a circular buffer, from which a final image is\n produced via reprocessing when a user takes a picture.  For such a use case, the\n high-resolution buffers must not have noise reduction applied to maximize efficiency of\n preview and to avoid over-applying noise filtering when reprocessing, while\n low-resolution buffers (used for recording or preview, generally) need noise reduction\n applied for reasonable preview quality.</p>\n <p>This mode is guaranteed to be supported by devices that support either the\n YUV_REPROCESSING or PRIVATE_REPROCESSING capabilities\n (ACAMERA_REQUEST_AVAILABLE_CAPABILITIES lists either of those capabilities) and it will\n be the default mode for CAMERA3_TEMPLATE_ZERO_SHUTTER_LAG template.</p>\n\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES"]
pub const acamera_metadata_enum_acamera_noise_reduction_mode_ACAMERA_NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG : acamera_metadata_enum_acamera_noise_reduction_mode = 4 ;
pub type acamera_metadata_enum_acamera_noise_reduction_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_noise_reduction_mode as acamera_metadata_enum_android_noise_reduction_mode_t;
#[doc = " <p>The minimal set of capabilities that every camera\n device (regardless of ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL)\n supports.</p>\n <p>This capability is listed by all normal devices, and\n indicates that the camera device has a feature set\n that's comparable to the baseline requirements for the\n older android.hardware.Camera API.</p>\n <p>Devices with the DEPTH_OUTPUT capability might not list this\n capability, indicating that they support only depth measurement,\n not standard color output.</p>\n\n @see ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE : acamera_metadata_enum_acamera_request_available_capabilities = 0 ;
#[doc = " <p>The camera device can be manually controlled (3A algorithms such\n as auto-exposure, and auto-focus can be bypassed).\n The camera device supports basic manual control of the sensor image\n acquisition related stages. This means the following controls are\n guaranteed to be supported:</p>\n <ul>\n <li>Manual frame duration control<ul>\n <li>ACAMERA_SENSOR_FRAME_DURATION</li>\n <li>ACAMERA_SENSOR_INFO_MAX_FRAME_DURATION</li>\n </ul>\n </li>\n <li>Manual exposure control<ul>\n <li>ACAMERA_SENSOR_EXPOSURE_TIME</li>\n <li>ACAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE</li>\n </ul>\n </li>\n <li>Manual sensitivity control<ul>\n <li>ACAMERA_SENSOR_SENSITIVITY</li>\n <li>ACAMERA_SENSOR_INFO_SENSITIVITY_RANGE</li>\n </ul>\n </li>\n <li>Manual lens control (if the lens is adjustable)<ul>\n <li>ACAMERA_LENS_*</li>\n </ul>\n </li>\n <li>Manual flash control (if a flash unit is present)<ul>\n <li>ACAMERA_FLASH_*</li>\n </ul>\n </li>\n <li>Manual black level locking<ul>\n <li>ACAMERA_BLACK_LEVEL_LOCK</li>\n </ul>\n </li>\n <li>Auto exposure lock<ul>\n <li>ACAMERA_CONTROL_AE_LOCK</li>\n </ul>\n </li>\n </ul>\n <p>If any of the above 3A algorithms are enabled, then the camera\n device will accurately report the values applied by 3A in the\n result.</p>\n <p>A given camera device may also support additional manual sensor controls,\n but this capability only covers the above list of controls.</p>\n <p>If this is supported, android.scaler.streamConfigurationMap will\n additionally return a min frame duration that is greater than\n zero for each supported size-format combination.</p>\n <p>For camera devices with LOGICAL_MULTI_CAMERA capability, when the underlying active\n physical camera switches, exposureTime, sensitivity, and lens properties may change\n even if AE/AF is locked. However, the overall auto exposure and auto focus experience\n for users will be consistent. Refer to LOGICAL_MULTI_CAMERA capability for details.</p>\n\n @see ACAMERA_BLACK_LEVEL_LOCK\n @see ACAMERA_CONTROL_AE_LOCK\n @see ACAMERA_SENSOR_EXPOSURE_TIME\n @see ACAMERA_SENSOR_FRAME_DURATION\n @see ACAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE\n @see ACAMERA_SENSOR_INFO_MAX_FRAME_DURATION\n @see ACAMERA_SENSOR_INFO_SENSITIVITY_RANGE\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR : acamera_metadata_enum_acamera_request_available_capabilities = 1 ;
#[doc = " <p>The camera device post-processing stages can be manually controlled.\n The camera device supports basic manual control of the image post-processing\n stages. This means the following controls are guaranteed to be supported:</p>\n <ul>\n <li>\n <p>Manual tonemap control</p>\n <ul>\n <li>android.tonemap.curve</li>\n <li>ACAMERA_TONEMAP_MODE</li>\n <li>ACAMERA_TONEMAP_MAX_CURVE_POINTS</li>\n <li>ACAMERA_TONEMAP_GAMMA</li>\n <li>ACAMERA_TONEMAP_PRESET_CURVE</li>\n </ul>\n </li>\n <li>\n <p>Manual white balance control</p>\n <ul>\n <li>ACAMERA_COLOR_CORRECTION_TRANSFORM</li>\n <li>ACAMERA_COLOR_CORRECTION_GAINS</li>\n </ul>\n </li>\n <li>Manual lens shading map control<ul>\n <li>ACAMERA_SHADING_MODE</li>\n <li>ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE</li>\n <li>ACAMERA_STATISTICS_LENS_SHADING_MAP</li>\n <li>ACAMERA_LENS_INFO_SHADING_MAP_SIZE</li>\n </ul>\n </li>\n <li>Manual aberration correction control (if aberration correction is supported)<ul>\n <li>ACAMERA_COLOR_CORRECTION_ABERRATION_MODE</li>\n <li>ACAMERA_COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES</li>\n </ul>\n </li>\n <li>Auto white balance lock<ul>\n <li>ACAMERA_CONTROL_AWB_LOCK</li>\n </ul>\n </li>\n </ul>\n <p>If auto white balance is enabled, then the camera device\n will accurately report the values applied by AWB in the result.</p>\n <p>A given camera device may also support additional post-processing\n controls, but this capability only covers the above list of controls.</p>\n <p>For camera devices with LOGICAL_MULTI_CAMERA capability, when underlying active\n physical camera switches, tonemap, white balance, and shading map may change even if\n awb is locked. However, the overall post-processing experience for users will be\n consistent. Refer to LOGICAL_MULTI_CAMERA capability for details.</p>\n\n @see ACAMERA_COLOR_CORRECTION_ABERRATION_MODE\n @see ACAMERA_COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES\n @see ACAMERA_COLOR_CORRECTION_GAINS\n @see ACAMERA_COLOR_CORRECTION_TRANSFORM\n @see ACAMERA_CONTROL_AWB_LOCK\n @see ACAMERA_LENS_INFO_SHADING_MAP_SIZE\n @see ACAMERA_SHADING_MODE\n @see ACAMERA_STATISTICS_LENS_SHADING_MAP\n @see ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE\n @see ACAMERA_TONEMAP_GAMMA\n @see ACAMERA_TONEMAP_MAX_CURVE_POINTS\n @see ACAMERA_TONEMAP_MODE\n @see ACAMERA_TONEMAP_PRESET_CURVE"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING : acamera_metadata_enum_acamera_request_available_capabilities = 2 ;
#[doc = " <p>The camera device supports outputting RAW buffers and\n metadata for interpreting them.</p>\n <p>Devices supporting the RAW capability allow both for\n saving DNG files, and for direct application processing of\n raw sensor images.</p>\n <ul>\n <li>RAW_SENSOR is supported as an output format.</li>\n <li>The maximum available resolution for RAW_SENSOR streams\n   will match either the value in\n   ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE or\n   ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE.</li>\n <li>All DNG-related optional metadata entries are provided\n   by the camera device.</li>\n </ul>\n\n @see ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE\n @see ACAMERA_SENSOR_INFO_PRE_CORRECTION_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_RAW : acamera_metadata_enum_acamera_request_available_capabilities = 3 ;
#[doc = " <p>The camera device supports accurately reporting the sensor settings for many of\n the sensor controls while the built-in 3A algorithm is running.  This allows\n reporting of sensor settings even when these settings cannot be manually changed.</p>\n <p>The values reported for the following controls are guaranteed to be available\n in the CaptureResult, including when 3A is enabled:</p>\n <ul>\n <li>Exposure control<ul>\n <li>ACAMERA_SENSOR_EXPOSURE_TIME</li>\n </ul>\n </li>\n <li>Sensitivity control<ul>\n <li>ACAMERA_SENSOR_SENSITIVITY</li>\n </ul>\n </li>\n <li>Lens controls (if the lens is adjustable)<ul>\n <li>ACAMERA_LENS_FOCUS_DISTANCE</li>\n <li>ACAMERA_LENS_APERTURE</li>\n </ul>\n </li>\n </ul>\n <p>This capability is a subset of the MANUAL_SENSOR control capability, and will\n always be included if the MANUAL_SENSOR capability is available.</p>\n\n @see ACAMERA_LENS_APERTURE\n @see ACAMERA_LENS_FOCUS_DISTANCE\n @see ACAMERA_SENSOR_EXPOSURE_TIME\n @see ACAMERA_SENSOR_SENSITIVITY"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS : acamera_metadata_enum_acamera_request_available_capabilities = 5 ;
#[doc = " <p>The camera device supports capturing high-resolution images at &gt;= 20 frames per\n second, in at least the uncompressed YUV format, when post-processing settings are\n set to FAST. Additionally, all image resolutions less than 24 megapixels can be\n captured at &gt;= 10 frames per second. Here, 'high resolution' means at least 8\n megapixels, or the maximum resolution of the device, whichever is smaller.</p>\n <p>More specifically, this means that at least one output {@link AIMAGE_FORMAT_YUV_420_888 } size listed in\n {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS }\n is larger or equal to the 'high resolution' defined above, and can be captured at at\n least 20 fps.  For the largest {@link AIMAGE_FORMAT_YUV_420_888 } size listed in\n {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS },\n camera device can capture this size for at least 10 frames per second if the size is\n less than 24 megapixels. Also the ACAMERA_CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES entry\n lists at least one FPS range where the minimum FPS is &gt;= 1 / minimumFrameDuration\n for the largest YUV_420_888 size.</p>\n <p>If the device supports the {@link AIMAGE_FORMAT_RAW10 }, {@link AIMAGE_FORMAT_RAW12 }, {@link AIMAGE_FORMAT_Y8 }, then those can also be\n captured at the same rate as the maximum-size YUV_420_888 resolution is.</p>\n <p>In addition, the ACAMERA_SYNC_MAX_LATENCY field is guaranteed to have a value between 0\n and 4, inclusive. ACAMERA_CONTROL_AE_LOCK_AVAILABLE and ACAMERA_CONTROL_AWB_LOCK_AVAILABLE\n are also guaranteed to be <code>true</code> so burst capture with these two locks ON yields\n consistent image output.</p>\n\n @see ACAMERA_CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES\n @see ACAMERA_CONTROL_AE_LOCK_AVAILABLE\n @see ACAMERA_CONTROL_AWB_LOCK_AVAILABLE\n @see ACAMERA_SYNC_MAX_LATENCY"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE : acamera_metadata_enum_acamera_request_available_capabilities = 6 ;
#[doc = " <p>The camera device can produce depth measurements from its field of view.</p>\n <p>This capability requires the camera device to support the following:</p>\n <ul>\n <li>{@link AIMAGE_FORMAT_DEPTH16 } is supported as\n   an output format.</li>\n <li>{@link AIMAGE_FORMAT_DEPTH_POINT_CLOUD } is\n   optionally supported as an output format.</li>\n <li>This camera device, and all camera devices with the same ACAMERA_LENS_FACING, will\n   list the following calibration metadata entries in both {@link ACameraManager_getCameraCharacteristics }\n   and {@link ACameraCaptureSession_captureCallback_result }:<ul>\n <li>ACAMERA_LENS_POSE_TRANSLATION</li>\n <li>ACAMERA_LENS_POSE_ROTATION</li>\n <li>ACAMERA_LENS_INTRINSIC_CALIBRATION</li>\n <li>ACAMERA_LENS_DISTORTION</li>\n </ul>\n </li>\n <li>The ACAMERA_DEPTH_DEPTH_IS_EXCLUSIVE entry is listed by this device.</li>\n <li>As of Android P, the ACAMERA_LENS_POSE_REFERENCE entry is listed by this device.</li>\n <li>A LIMITED camera with only the DEPTH_OUTPUT capability does not have to support\n   normal YUV_420_888, Y8, JPEG, and PRIV-format outputs. It only has to support the\n   DEPTH16 format.</li>\n </ul>\n <p>Generally, depth output operates at a slower frame rate than standard color capture,\n so the DEPTH16 and DEPTH_POINT_CLOUD formats will commonly have a stall duration that\n should be accounted for (see {@link ACAMERA_DEPTH_AVAILABLE_DEPTH_STALL_DURATIONS }).\n On a device that supports both depth and color-based output, to enable smooth preview,\n using a repeating burst is recommended, where a depth-output target is only included\n once every N frames, where N is the ratio between preview output rate and depth output\n rate, including depth stall time.</p>\n\n @see ACAMERA_DEPTH_DEPTH_IS_EXCLUSIVE\n @see ACAMERA_LENS_DISTORTION\n @see ACAMERA_LENS_FACING\n @see ACAMERA_LENS_INTRINSIC_CALIBRATION\n @see ACAMERA_LENS_POSE_REFERENCE\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT : acamera_metadata_enum_acamera_request_available_capabilities = 8 ;
#[doc = " <p>The camera device supports the MOTION_TRACKING value for\n ACAMERA_CONTROL_CAPTURE_INTENT, which limits maximum exposure time to 20 ms.</p>\n <p>This limits the motion blur of capture images, resulting in better image tracking\n results for use cases such as image stabilization or augmented reality.</p>\n\n @see ACAMERA_CONTROL_CAPTURE_INTENT"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_MOTION_TRACKING : acamera_metadata_enum_acamera_request_available_capabilities = 10 ;
#[doc = " <p>The camera device is a logical camera backed by two or more physical cameras.</p>\n <p>In API level 28, the physical cameras must also be exposed to the application via\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraManager.html#getCameraIdList\">CameraManager#getCameraIdList</a>.</p>\n <p>Starting from API level 29:</p>\n <ul>\n <li>Some or all physical cameras may not be independently exposed to the application,\n in which case the physical camera IDs will not be available in\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraManager.html#getCameraIdList\">CameraManager#getCameraIdList</a>. But the\n application can still query the physical cameras' characteristics by calling\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraManager.html#getCameraCharacteristics\">CameraManager#getCameraCharacteristics</a>.</li>\n <li>If a physical camera is hidden from camera ID list, the mandatory stream\n combinations for that physical camera must be supported through the logical camera\n using physical streams. One exception is that in API level 30, a physical camera\n may become unavailable via\n {@link ACameraManager_PhysicalCameraAvailabilityCallback }\n callback.</li>\n </ul>\n <p>Combinations of logical and physical streams, or physical streams from different\n physical cameras are not guaranteed. However, if the camera device supports\n {@link ACameraDevice_isSessionConfigurationSupported },\n application must be able to query whether a stream combination involving physical\n streams is supported by calling\n {@link ACameraDevice_isSessionConfigurationSupported }.</p>\n <p>Camera application shouldn't assume that there are at most 1 rear camera and 1 front\n camera in the system. For an application that switches between front and back cameras,\n the recommendation is to switch between the first rear camera and the first front\n camera in the list of supported camera devices.</p>\n <p>This capability requires the camera device to support the following:</p>\n <ul>\n <li>The IDs of underlying physical cameras are returned via\n   <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#getPhysicalCameraIds\">CameraCharacteristics#getPhysicalCameraIds</a>.</li>\n <li>This camera device must list static metadata\n   ACAMERA_LOGICAL_MULTI_CAMERA_SENSOR_SYNC_TYPE in\n   <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html\">CameraCharacteristics</a>.</li>\n <li>The underlying physical cameras' static metadata must list the following entries,\n   so that the application can correlate pixels from the physical streams:<ul>\n <li>ACAMERA_LENS_POSE_REFERENCE</li>\n <li>ACAMERA_LENS_POSE_ROTATION</li>\n <li>ACAMERA_LENS_POSE_TRANSLATION</li>\n <li>ACAMERA_LENS_INTRINSIC_CALIBRATION</li>\n <li>ACAMERA_LENS_DISTORTION</li>\n </ul>\n </li>\n <li>The SENSOR_INFO_TIMESTAMP_SOURCE of the logical device and physical devices must be\n   the same.</li>\n <li>The logical camera must be LIMITED or higher device.</li>\n </ul>\n <p>A logical camera device's dynamic metadata may contain\n ACAMERA_LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID to notify the application of the current\n active physical camera Id. An active physical camera is the physical camera from which\n the logical camera's main image data outputs (YUV or RAW) and metadata come from.\n In addition, this serves as an indication which physical camera is used to output to\n a RAW stream, or in case only physical cameras support RAW, which physical RAW stream\n the application should request.</p>\n <p>Logical camera's static metadata tags below describe the default active physical\n camera. An active physical camera is default if it's used when application directly\n uses requests built from a template. All templates will default to the same active\n physical camera.</p>\n <ul>\n <li>ACAMERA_SENSOR_INFO_SENSITIVITY_RANGE</li>\n <li>ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT</li>\n <li>ACAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE</li>\n <li>ACAMERA_SENSOR_INFO_MAX_FRAME_DURATION</li>\n <li>ACAMERA_SENSOR_INFO_PHYSICAL_SIZE</li>\n <li>ACAMERA_SENSOR_INFO_WHITE_LEVEL</li>\n <li>ACAMERA_SENSOR_INFO_LENS_SHADING_APPLIED</li>\n <li>ACAMERA_SENSOR_REFERENCE_ILLUMINANT1</li>\n <li>ACAMERA_SENSOR_REFERENCE_ILLUMINANT2</li>\n <li>ACAMERA_SENSOR_CALIBRATION_TRANSFORM1</li>\n <li>ACAMERA_SENSOR_CALIBRATION_TRANSFORM2</li>\n <li>ACAMERA_SENSOR_COLOR_TRANSFORM1</li>\n <li>ACAMERA_SENSOR_COLOR_TRANSFORM2</li>\n <li>ACAMERA_SENSOR_FORWARD_MATRIX1</li>\n <li>ACAMERA_SENSOR_FORWARD_MATRIX2</li>\n <li>ACAMERA_SENSOR_BLACK_LEVEL_PATTERN</li>\n <li>ACAMERA_SENSOR_MAX_ANALOG_SENSITIVITY</li>\n <li>ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS</li>\n <li>ACAMERA_SENSOR_AVAILABLE_TEST_PATTERN_MODES</li>\n <li>ACAMERA_LENS_INFO_HYPERFOCAL_DISTANCE</li>\n <li>ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE</li>\n <li>ACAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION</li>\n <li>ACAMERA_LENS_POSE_ROTATION</li>\n <li>ACAMERA_LENS_POSE_TRANSLATION</li>\n <li>ACAMERA_LENS_INTRINSIC_CALIBRATION</li>\n <li>ACAMERA_LENS_POSE_REFERENCE</li>\n <li>ACAMERA_LENS_DISTORTION</li>\n </ul>\n <p>The field of view of non-RAW physical streams must not be smaller than that of the\n non-RAW logical streams, or the maximum field-of-view of the physical camera,\n whichever is smaller. The application should check the physical capture result\n metadata for how the physical streams are cropped or zoomed. More specifically, given\n the physical camera result metadata, the effective horizontal field-of-view of the\n physical camera is:</p>\n <pre><code>fov = 2 * atan2(cropW * sensorW / (2 * zoomRatio * activeArrayW), focalLength)\n </code></pre>\n <p>where the equation parameters are the physical camera's crop region width, physical\n sensor width, zoom ratio, active array width, and focal length respectively. Typically\n the physical stream of active physical camera has the same field-of-view as the\n logical streams. However, the same may not be true for physical streams from\n non-active physical cameras. For example, if the logical camera has a wide-ultrawide\n configuration where the wide lens is the default, when the crop region is set to the\n logical camera's active array size, (and the zoom ratio set to 1.0 starting from\n Android 11), a physical stream for the ultrawide camera may prefer outputting images\n with larger field-of-view than that of the wide camera for better stereo matching\n margin or more robust motion tracking. At the same time, the physical non-RAW streams'\n field of view must not be smaller than the requested crop region and zoom ratio, as\n long as it's within the physical lens' capability. For example, for a logical camera\n with wide-tele lens configuration where the wide lens is the default, if the logical\n camera's crop region is set to maximum size, and zoom ratio set to 1.0, the physical\n stream for the tele lens will be configured to its maximum size crop region (no zoom).</p>\n <p><em>Deprecated:</em> Prior to Android 11, the field of view of all non-RAW physical streams\n cannot be larger than that of non-RAW logical streams. If the logical camera has a\n wide-ultrawide lens configuration where the wide lens is the default, when the logical\n camera's crop region is set to maximum size, the FOV of the physical streams for the\n ultrawide lens will be the same as the logical stream, by making the crop region\n smaller than its active array size to compensate for the smaller focal length.</p>\n <p>For a logical camera, typically the underlying physical cameras have different RAW\n capabilities (such as resolution or CFA pattern). There are two ways for the\n application to capture RAW images from the logical camera:</p>\n <ul>\n <li>If the logical camera has RAW capability, the application can create and use RAW\n streams in the same way as before. In case a RAW stream is configured, to maintain\n backward compatibility, the camera device makes sure the default active physical\n camera remains active and does not switch to other physical cameras. (One exception\n is that, if the logical camera consists of identical image sensors and advertises\n multiple focalLength due to different lenses, the camera device may generate RAW\n images from different physical cameras based on the focalLength being set by the\n application.) This backward-compatible approach usually results in loss of optical\n zoom, to telephoto lens or to ultrawide lens.</li>\n <li>Alternatively, if supported by the device,\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/MultiResolutionImageReader.html\">MultiResolutionImageReader</a>\n can be used to capture RAW images from one of the underlying physical cameras (\n depending on current zoom level). Because different physical cameras may have\n different RAW characteristics, the application needs to use the characteristics\n and result metadata of the active physical camera for the relevant RAW metadata.</li>\n </ul>\n <p>The capture request and result metadata tags required for backward compatible camera\n functionalities will be solely based on the logical camera capability. On the other\n hand, the use of manual capture controls (sensor or post-processing) with a\n logical camera may result in unexpected behavior when the HAL decides to switch\n between physical cameras with different characteristics under the hood. For example,\n when the application manually sets exposure time and sensitivity while zooming in,\n the brightness of the camera images may suddenly change because HAL switches from one\n physical camera to the other.</p>\n\n @see ACAMERA_LENS_DISTORTION\n @see ACAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION\n @see ACAMERA_LENS_INFO_HYPERFOCAL_DISTANCE\n @see ACAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE\n @see ACAMERA_LENS_INTRINSIC_CALIBRATION\n @see ACAMERA_LENS_POSE_REFERENCE\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION\n @see ACAMERA_LOGICAL_MULTI_CAMERA_ACTIVE_PHYSICAL_ID\n @see ACAMERA_LOGICAL_MULTI_CAMERA_SENSOR_SYNC_TYPE\n @see ACAMERA_SENSOR_AVAILABLE_TEST_PATTERN_MODES\n @see ACAMERA_SENSOR_BLACK_LEVEL_PATTERN\n @see ACAMERA_SENSOR_CALIBRATION_TRANSFORM1\n @see ACAMERA_SENSOR_CALIBRATION_TRANSFORM2\n @see ACAMERA_SENSOR_COLOR_TRANSFORM1\n @see ACAMERA_SENSOR_COLOR_TRANSFORM2\n @see ACAMERA_SENSOR_FORWARD_MATRIX1\n @see ACAMERA_SENSOR_FORWARD_MATRIX2\n @see ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT\n @see ACAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE\n @see ACAMERA_SENSOR_INFO_LENS_SHADING_APPLIED\n @see ACAMERA_SENSOR_INFO_MAX_FRAME_DURATION\n @see ACAMERA_SENSOR_INFO_PHYSICAL_SIZE\n @see ACAMERA_SENSOR_INFO_SENSITIVITY_RANGE\n @see ACAMERA_SENSOR_INFO_WHITE_LEVEL\n @see ACAMERA_SENSOR_MAX_ANALOG_SENSITIVITY\n @see ACAMERA_SENSOR_OPTICAL_BLACK_REGIONS\n @see ACAMERA_SENSOR_REFERENCE_ILLUMINANT1\n @see ACAMERA_SENSOR_REFERENCE_ILLUMINANT2"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_LOGICAL_MULTI_CAMERA : acamera_metadata_enum_acamera_request_available_capabilities = 11 ;
#[doc = " <p>The camera device is a monochrome camera that doesn't contain a color filter array,\n and for YUV_420_888 stream, the pixel values on U and V planes are all 128.</p>\n <p>A MONOCHROME camera must support the guaranteed stream combinations required for\n its device level and capabilities. Additionally, if the monochrome camera device\n supports Y8 format, all mandatory stream combination requirements related to {@link AIMAGE_FORMAT_YUV_420_888 YUV_420_888} apply\n to {@link AIMAGE_FORMAT_Y8 Y8} as well. There are no\n mandatory stream combination requirements with regard to\n {@link AIMAGE_FORMAT_Y8 Y8} for Bayer camera devices.</p>\n <p>Starting from Android Q, the SENSOR_INFO_COLOR_FILTER_ARRANGEMENT of a MONOCHROME\n camera will be either MONO or NIR.</p>"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_MONOCHROME : acamera_metadata_enum_acamera_request_available_capabilities = 12 ;
#[doc = " <p>The camera device is capable of writing image data into a region of memory\n inaccessible to Android userspace or the Android kernel, and only accessible to\n trusted execution environments (TEE).</p>"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_SECURE_IMAGE_DATA : acamera_metadata_enum_acamera_request_available_capabilities = 13 ;
#[doc = " <p>The camera device is only accessible by Android's system components and privileged\n applications. Processes need to have the android.permission.SYSTEM_CAMERA in\n addition to android.permission.CAMERA in order to connect to this camera device.</p>"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_SYSTEM_CAMERA : acamera_metadata_enum_acamera_request_available_capabilities = 14 ;
#[doc = " <p>This camera device is capable of producing ultra high resolution images in\n addition to the image sizes described in the\n android.scaler.streamConfigurationMap.\n It can operate in 'default' mode and 'max resolution' mode. It generally does this\n by binning pixels in 'default' mode and not binning them in 'max resolution' mode.\n <code>android.scaler.streamConfigurationMap</code> describes the streams supported in 'default'\n mode.\n The stream configurations supported in 'max resolution' mode are described by\n <code>android.scaler.streamConfigurationMapMaximumResolution</code>.\n The maximum resolution mode pixel array size of a camera device\n (<code>ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE</code>) with this capability,\n will be at least 24 megapixels.</p>\n\n @see ACAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR : acamera_metadata_enum_acamera_request_available_capabilities = 16 ;
#[doc = " <p>The camera device supports selecting a per-stream use case via\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/params/OutputConfiguration.html#setStreamUseCase\">OutputConfiguration#setStreamUseCase</a>\n so that the device can optimize camera pipeline parameters such as tuning, sensor\n mode, or ISP settings for a specific user scenario.\n Some sample usages of this capability are:</p>\n <ul>\n <li>Distinguish high quality YUV captures from a regular YUV stream where\n   the image quality may not be as good as the JPEG stream, or</li>\n <li>Use one stream to serve multiple purposes: viewfinder, video recording and\n   still capture. This is common with applications that wish to apply edits equally\n   to preview, saved images, and saved videos.</li>\n </ul>\n <p>This capability requires the camera device to support the following\n stream use cases:</p>\n <ul>\n <li>DEFAULT for backward compatibility where the application doesn't set\n   a stream use case</li>\n <li>PREVIEW for live viewfinder and in-app image analysis</li>\n <li>STILL_CAPTURE for still photo capture</li>\n <li>VIDEO_RECORD for recording video clips</li>\n <li>PREVIEW_VIDEO_STILL for one single stream used for viewfinder, video\n   recording, and still capture.</li>\n <li>VIDEO_CALL for long running video calls</li>\n </ul>\n <p><a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics.html#SCALER_AVAILABLE_STREAM_USE_CASES\">CameraCharacteristics#SCALER_AVAILABLE_STREAM_USE_CASES</a>\n lists all of the supported stream use cases.</p>\n <p>Refer to <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraDevice.html#createCaptureSession\">CameraDevice#createCaptureSession</a> for the\n mandatory stream combinations involving stream use cases, which can also be queried\n via <a href=\"https://developer.android.com/reference/android/hardware/camera2/params/MandatoryStreamCombination.html\">MandatoryStreamCombination</a>.</p>"]
pub const acamera_metadata_enum_acamera_request_available_capabilities_ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_STREAM_USE_CASE : acamera_metadata_enum_acamera_request_available_capabilities = 19 ;
pub type acamera_metadata_enum_acamera_request_available_capabilities = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_request_available_capabilities as acamera_metadata_enum_android_request_available_capabilities_t;
#[doc = " <p>8-bit SDR profile which is the default for all non 10-bit output capable devices.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_STANDARD : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 1 ;
#[doc = " <p>10-bit pixel samples encoded using the Hybrid log-gamma transfer function.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_HLG10 : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 2 ;
#[doc = " <p>10-bit pixel samples encoded using the SMPTE ST 2084 transfer function.\n This profile utilizes internal static metadata to increase the quality\n of the capture.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_HDR10 : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 4 ;
#[doc = " <p>10-bit pixel samples encoded using the SMPTE ST 2084 transfer function.\n In contrast to HDR10, this profile uses internal per-frame metadata\n to further enhance the quality of the capture.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_HDR10_PLUS : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 8 ;
#[doc = " <p>This is a camera mode for Dolby Vision capture optimized for a more scene\n accurate capture. This would typically differ from what a specific device\n might want to tune for a consumer optimized Dolby Vision general capture.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_DOLBY_VISION_10B_HDR_REF : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 16 ;
#[doc = " <p>This is the power optimized mode for 10-bit Dolby Vision HDR Reference Mode.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_DOLBY_VISION_10B_HDR_REF_PO : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 32 ;
#[doc = " <p>This is the camera mode for the default Dolby Vision capture mode for the\n specific device. This would be tuned by each specific device for consumer\n pleasing results that resonate with their particular audience. We expect\n that each specific device would have a different look for their default\n Dolby Vision capture.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_DOLBY_VISION_10B_HDR_OEM : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 64 ;
#[doc = " <p>This is the power optimized mode for 10-bit Dolby Vision HDR device specific\n capture Mode.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_DOLBY_VISION_10B_HDR_OEM_PO : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 128 ;
#[doc = " <p>This is the 8-bit version of the Dolby Vision reference capture mode optimized\n for scene accuracy.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_DOLBY_VISION_8B_HDR_REF : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 256 ;
#[doc = " <p>This is the power optimized mode for 8-bit Dolby Vision HDR Reference Mode.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_DOLBY_VISION_8B_HDR_REF_PO : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 512 ;
#[doc = " <p>This is the 8-bit version of device specific tuned and optimized Dolby Vision\n capture mode.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_DOLBY_VISION_8B_HDR_OEM : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 1024 ;
#[doc = " <p>This is the power optimized mode for 8-bit Dolby Vision HDR device specific\n capture Mode.</p>"]
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_DOLBY_VISION_8B_HDR_OEM_PO : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 2048 ;
pub const acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map_ACAMERA_REQUEST_AVAILABLE_DYNAMIC_RANGE_PROFILES_MAP_MAX : acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map = 4096 ;
pub type acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_request_available_dynamic_range_profiles_map as acamera_metadata_enum_android_request_available_dynamic_range_profiles_map_t;
pub const acamera_metadata_enum_acamera_scaler_available_stream_configurations_ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS_OUTPUT : acamera_metadata_enum_acamera_scaler_available_stream_configurations = 0 ;
pub const acamera_metadata_enum_acamera_scaler_available_stream_configurations_ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS_INPUT : acamera_metadata_enum_acamera_scaler_available_stream_configurations = 1 ;
pub type acamera_metadata_enum_acamera_scaler_available_stream_configurations =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_scaler_available_stream_configurations as acamera_metadata_enum_android_scaler_available_stream_configurations_t;
#[doc = " <p>The camera device only supports centered crop regions.</p>"]
pub const acamera_metadata_enum_acamera_scaler_cropping_type_ACAMERA_SCALER_CROPPING_TYPE_CENTER_ONLY : acamera_metadata_enum_acamera_scaler_cropping_type = 0 ;
#[doc = " <p>The camera device supports arbitrarily chosen crop regions.</p>"]
pub const acamera_metadata_enum_acamera_scaler_cropping_type_ACAMERA_SCALER_CROPPING_TYPE_FREEFORM : acamera_metadata_enum_acamera_scaler_cropping_type = 1 ;
pub type acamera_metadata_enum_acamera_scaler_cropping_type = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_scaler_cropping_type as acamera_metadata_enum_android_scaler_cropping_type_t;
#[doc = " <p>Preview must only include non-stalling processed stream configurations with\n output formats like\n {@link AIMAGE_FORMAT_YUV_420_888 },\n {@link AIMAGE_FORMAT_PRIVATE }, etc.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_PREVIEW : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 0 ;
#[doc = " <p>Video record must include stream configurations that match the advertised\n supported media profiles <a href=\"https://developer.android.com/reference/android/media/CamcorderProfile.html\">CamcorderProfile</a> with\n IMPLEMENTATION_DEFINED format.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_RECORD : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 1 ;
#[doc = " <p>Video snapshot must include stream configurations at least as big as\n the maximum RECORD resolutions and only with\n {@link AIMAGE_FORMAT_JPEG JPEG output format}.\n Additionally the configurations shouldn't cause preview glitches and also be able to\n run at 30 fps.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_VIDEO_SNAPSHOT : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 2 ;
#[doc = " <p>Recommended snapshot stream configurations must include at least one with\n size close to ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE and\n {@link AIMAGE_FORMAT_JPEG JPEG output format}.\n Taking into account restrictions on aspect ratio, alignment etc. the area of the\n maximum suggested size shouldnt be less than 97% of the sensor array size area.</p>\n\n @see ACAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_SNAPSHOT : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 3 ;
#[doc = " <p>If supported, recommended input stream configurations must only be advertised with\n ZSL along with other processed and/or stalling output formats.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_ZSL : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 4 ;
#[doc = " <p>If supported, recommended raw stream configurations must only include RAW based\n output formats.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_RAW : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 5 ;
#[doc = " <p>If supported, the recommended low latency stream configurations must have\n end-to-end latency that does not exceed 200 ms. under standard operating conditions\n (reasonable light levels, not loaded system) and using template\n TEMPLATE_STILL_CAPTURE. This is primarily for listing configurations for the\n {@link AIMAGE_FORMAT_JPEG JPEG output format}\n however other supported output formats can be added as well.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_LOW_LATENCY_SNAPSHOT : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 6 ;
#[doc = " <p>If supported, the recommended low latency stream configurations must have\n end-to-end latency that does not exceed 200 ms. under standard operating conditions\n (reasonable light levels, not loaded system) and using template\n TEMPLATE_STILL_CAPTURE. This is primarily for listing configurations for the\n {@link AIMAGE_FORMAT_JPEG JPEG output format}\n however other supported output formats can be added as well.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_PUBLIC_END : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 7 ;
#[doc = " <p>If supported, the recommended 10-bit output stream configurations must include\n a subset of the advertised <a href=\"https://developer.android.com/reference/android/graphics/ImageFormat.html#YCBCR_P010\">ImageFormat#YCBCR_P010</a> and\n <a href=\"https://developer.android.com/reference/android/graphics/ImageFormat.html#PRIVATE\">ImageFormat#PRIVATE</a> outputs that are optimized for power\n and performance when registered along with a supported 10-bit dynamic range profile.\n see android.hardware.camera2.params.OutputConfiguration#setDynamicRangeProfile for\n details.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_10BIT_OUTPUT : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 8 ;
#[doc = " <p>If supported, the recommended 10-bit output stream configurations must include\n a subset of the advertised <a href=\"https://developer.android.com/reference/android/graphics/ImageFormat.html#YCBCR_P010\">ImageFormat#YCBCR_P010</a> and\n <a href=\"https://developer.android.com/reference/android/graphics/ImageFormat.html#PRIVATE\">ImageFormat#PRIVATE</a> outputs that are optimized for power\n and performance when registered along with a supported 10-bit dynamic range profile.\n see android.hardware.camera2.params.OutputConfiguration#setDynamicRangeProfile for\n details.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_PUBLIC_END_3_8 : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 9 ;
#[doc = " <p>Vendor defined use cases. These depend on the vendor implementation.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations_ACAMERA_SCALER_AVAILABLE_RECOMMENDED_STREAM_CONFIGURATIONS_VENDOR_START : acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations = 24 ;
pub type acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_scaler_available_recommended_stream_configurations as acamera_metadata_enum_android_scaler_available_recommended_stream_configurations_t;
#[doc = " <p>No rotate and crop is applied. Processed outputs are in the sensor orientation.</p>"]
pub const acamera_metadata_enum_acamera_scaler_rotate_and_crop_ACAMERA_SCALER_ROTATE_AND_CROP_NONE : acamera_metadata_enum_acamera_scaler_rotate_and_crop = 0 ;
#[doc = " <p>Processed images are rotated by 90 degrees clockwise, and then cropped\n to the original aspect ratio.</p>"]
pub const acamera_metadata_enum_acamera_scaler_rotate_and_crop_ACAMERA_SCALER_ROTATE_AND_CROP_90:
    acamera_metadata_enum_acamera_scaler_rotate_and_crop = 1;
#[doc = " <p>Processed images are rotated by 180 degrees.  Since the aspect ratio does not\n change, no cropping is performed.</p>"]
pub const acamera_metadata_enum_acamera_scaler_rotate_and_crop_ACAMERA_SCALER_ROTATE_AND_CROP_180 : acamera_metadata_enum_acamera_scaler_rotate_and_crop = 2 ;
#[doc = " <p>Processed images are rotated by 270 degrees clockwise, and then cropped\n to the original aspect ratio.</p>"]
pub const acamera_metadata_enum_acamera_scaler_rotate_and_crop_ACAMERA_SCALER_ROTATE_AND_CROP_270 : acamera_metadata_enum_acamera_scaler_rotate_and_crop = 3 ;
#[doc = " <p>The camera API automatically selects the best concrete value for\n rotate-and-crop based on the application's support for resizability and the current\n multi-window mode.</p>\n <p>If the application does not support resizing but the display mode for its main\n Activity is not in a typical orientation, the camera API will set <code>ROTATE_AND_CROP_90</code>\n or some other supported rotation value, depending on device configuration,\n to ensure preview and captured images are correctly shown to the user. Otherwise,\n <code>ROTATE_AND_CROP_NONE</code> will be selected.</p>\n <p>When a value other than NONE is selected, several metadata fields will also be parsed\n differently to ensure that coordinates are correctly handled for features like drawing\n face detection boxes or passing in tap-to-focus coordinates.  The camera API will\n convert positions in the active array coordinate system to/from the cropped-and-rotated\n coordinate system to make the operation transparent for applications.</p>\n <p>No coordinate mapping will be done when the application selects a non-AUTO mode.</p>"]
pub const acamera_metadata_enum_acamera_scaler_rotate_and_crop_ACAMERA_SCALER_ROTATE_AND_CROP_AUTO : acamera_metadata_enum_acamera_scaler_rotate_and_crop = 4 ;
pub type acamera_metadata_enum_acamera_scaler_rotate_and_crop = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_scaler_rotate_and_crop as acamera_metadata_enum_android_scaler_rotate_and_crop_t;
pub const acamera_metadata_enum_acamera_scaler_physical_camera_multi_resolution_stream_configurations_ACAMERA_SCALER_PHYSICAL_CAMERA_MULTI_RESOLUTION_STREAM_CONFIGURATIONS_OUTPUT : acamera_metadata_enum_acamera_scaler_physical_camera_multi_resolution_stream_configurations = 0 ;
pub const acamera_metadata_enum_acamera_scaler_physical_camera_multi_resolution_stream_configurations_ACAMERA_SCALER_PHYSICAL_CAMERA_MULTI_RESOLUTION_STREAM_CONFIGURATIONS_INPUT : acamera_metadata_enum_acamera_scaler_physical_camera_multi_resolution_stream_configurations = 1 ;
pub type acamera_metadata_enum_acamera_scaler_physical_camera_multi_resolution_stream_configurations =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_scaler_physical_camera_multi_resolution_stream_configurations as acamera_metadata_enum_android_scaler_physical_camera_multi_resolution_stream_configurations_t;
pub const acamera_metadata_enum_acamera_scaler_available_stream_configurations_maximum_resolution_ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION_OUTPUT : acamera_metadata_enum_acamera_scaler_available_stream_configurations_maximum_resolution = 0 ;
pub const acamera_metadata_enum_acamera_scaler_available_stream_configurations_maximum_resolution_ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION_INPUT : acamera_metadata_enum_acamera_scaler_available_stream_configurations_maximum_resolution = 1 ;
pub type acamera_metadata_enum_acamera_scaler_available_stream_configurations_maximum_resolution =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_scaler_available_stream_configurations_maximum_resolution as acamera_metadata_enum_android_scaler_available_stream_configurations_maximum_resolution_t;
pub const acamera_metadata_enum_acamera_scaler_multi_resolution_stream_supported_ACAMERA_SCALER_MULTI_RESOLUTION_STREAM_SUPPORTED_FALSE : acamera_metadata_enum_acamera_scaler_multi_resolution_stream_supported = 0 ;
pub const acamera_metadata_enum_acamera_scaler_multi_resolution_stream_supported_ACAMERA_SCALER_MULTI_RESOLUTION_STREAM_SUPPORTED_TRUE : acamera_metadata_enum_acamera_scaler_multi_resolution_stream_supported = 1 ;
pub type acamera_metadata_enum_acamera_scaler_multi_resolution_stream_supported =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_scaler_multi_resolution_stream_supported as acamera_metadata_enum_android_scaler_multi_resolution_stream_supported_t;
#[doc = " <p>Default stream use case.</p>\n <p>This use case is the same as when the application doesn't set any use case for\n the stream. The camera device uses the properties of the output target, such as\n format, dataSpace, or surface class type, to optimize the image processing pipeline.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_stream_use_cases_ACAMERA_SCALER_AVAILABLE_STREAM_USE_CASES_DEFAULT : acamera_metadata_enum_acamera_scaler_available_stream_use_cases = 0 ;
#[doc = " <p>Live stream shown to the user.</p>\n <p>Optimized for performance and usability as a viewfinder, but not necessarily for\n image quality. The output is not meant to be persisted as saved images or video.</p>\n <p>No stall if ACAMERA_CONTROL_* are set to FAST. There may be stall if\n they are set to HIGH_QUALITY. This use case has the same behavior as the\n default SurfaceView and SurfaceTexture targets. Additionally, this use case can be\n used for in-app image analysis.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_stream_use_cases_ACAMERA_SCALER_AVAILABLE_STREAM_USE_CASES_PREVIEW : acamera_metadata_enum_acamera_scaler_available_stream_use_cases = 1 ;
#[doc = " <p>Still photo capture.</p>\n <p>Optimized for high-quality high-resolution capture, and not expected to maintain\n preview-like frame rates.</p>\n <p>The stream may have stalls regardless of whether ACAMERA_CONTROL_* is HIGH_QUALITY.\n This use case has the same behavior as the default JPEG and RAW related formats.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_stream_use_cases_ACAMERA_SCALER_AVAILABLE_STREAM_USE_CASES_STILL_CAPTURE : acamera_metadata_enum_acamera_scaler_available_stream_use_cases = 2 ;
#[doc = " <p>Recording video clips.</p>\n <p>Optimized for high-quality video capture, including high-quality image stabilization\n if supported by the device and enabled by the application. As a result, may produce\n output frames with a substantial lag from real time, to allow for highest-quality\n stabilization or other processing. As such, such an output is not suitable for drawing\n to screen directly, and is expected to be persisted to disk or similar for later\n playback or processing. Only streams that set the VIDEO_RECORD use case are guaranteed\n to have video stabilization applied when the video stabilization control is set\n to ON, as opposed to PREVIEW_STABILIZATION.</p>\n <p>This use case has the same behavior as the default MediaRecorder and MediaCodec\n targets.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_stream_use_cases_ACAMERA_SCALER_AVAILABLE_STREAM_USE_CASES_VIDEO_RECORD : acamera_metadata_enum_acamera_scaler_available_stream_use_cases = 3 ;
#[doc = " <p>One single stream used for combined purposes of preview, video, and still capture.</p>\n <p>For such multi-purpose streams, the camera device aims to make the best tradeoff\n between the individual use cases. For example, the STILL_CAPTURE use case by itself\n may have stalls for achieving best image quality. But if combined with PREVIEW and\n VIDEO_RECORD, the camera device needs to trade off the additional image processing\n for speed so that preview and video recording aren't slowed down.</p>\n <p>Similarly, VIDEO_RECORD may produce frames with a substantial lag, but\n PREVIEW_VIDEO_STILL must have minimal output delay. This means that to enable video\n stabilization with this use case, the device must support and the app must select the\n PREVIEW_STABILIZATION mode for video stabilization.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_stream_use_cases_ACAMERA_SCALER_AVAILABLE_STREAM_USE_CASES_PREVIEW_VIDEO_STILL : acamera_metadata_enum_acamera_scaler_available_stream_use_cases = 4 ;
#[doc = " <p>Long-running video call optimized for both power efficiency and video quality.</p>\n <p>The camera sensor may run in a lower-resolution mode to reduce power consumption\n at the cost of some image and digital zoom quality. Unlike VIDEO_RECORD, VIDEO_CALL\n outputs are expected to work in dark conditions, so are usually accompanied with\n variable frame rate settings to allow sufficient exposure time in low light.</p>"]
pub const acamera_metadata_enum_acamera_scaler_available_stream_use_cases_ACAMERA_SCALER_AVAILABLE_STREAM_USE_CASES_VIDEO_CALL : acamera_metadata_enum_acamera_scaler_available_stream_use_cases = 5 ;
pub type acamera_metadata_enum_acamera_scaler_available_stream_use_cases = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_scaler_available_stream_use_cases as acamera_metadata_enum_android_scaler_available_stream_use_cases_t;
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_DAYLIGHT : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 1 ;
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_FLUORESCENT : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 2 ;
#[doc = " <p>Incandescent light</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_TUNGSTEN : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 3 ;
#[doc = " <p>Incandescent light</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_FLASH : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 4 ;
#[doc = " <p>Incandescent light</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_FINE_WEATHER : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 9 ;
#[doc = " <p>Incandescent light</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_CLOUDY_WEATHER : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 10 ;
#[doc = " <p>Incandescent light</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_SHADE : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 11 ;
#[doc = " <p>D 5700 - 7100K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_DAYLIGHT_FLUORESCENT : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 12 ;
#[doc = " <p>N 4600 - 5400K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_DAY_WHITE_FLUORESCENT : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 13 ;
#[doc = " <p>W 3900 - 4500K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_COOL_WHITE_FLUORESCENT : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 14 ;
#[doc = " <p>WW 3200 - 3700K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_WHITE_FLUORESCENT : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 15 ;
#[doc = " <p>WW 3200 - 3700K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_STANDARD_A : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 17 ;
#[doc = " <p>WW 3200 - 3700K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_STANDARD_B : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 18 ;
#[doc = " <p>WW 3200 - 3700K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_STANDARD_C : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 19 ;
#[doc = " <p>WW 3200 - 3700K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_D55 : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 20 ;
#[doc = " <p>WW 3200 - 3700K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_D65 : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 21 ;
#[doc = " <p>WW 3200 - 3700K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_D75 : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 22 ;
#[doc = " <p>WW 3200 - 3700K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_D50 : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 23 ;
#[doc = " <p>WW 3200 - 3700K</p>"]
pub const acamera_metadata_enum_acamera_sensor_reference_illuminant1_ACAMERA_SENSOR_REFERENCE_ILLUMINANT1_ISO_STUDIO_TUNGSTEN : acamera_metadata_enum_acamera_sensor_reference_illuminant1 = 24 ;
pub type acamera_metadata_enum_acamera_sensor_reference_illuminant1 = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_sensor_reference_illuminant1 as acamera_metadata_enum_android_sensor_reference_illuminant1_t;
#[doc = " <p>No test pattern mode is used, and the camera\n device returns captures from the image sensor.</p>\n <p>This is the default if the key is not set.</p>"]
pub const acamera_metadata_enum_acamera_sensor_test_pattern_mode_ACAMERA_SENSOR_TEST_PATTERN_MODE_OFF : acamera_metadata_enum_acamera_sensor_test_pattern_mode = 0 ;
#[doc = " <p>Each pixel in <code>[R, G_even, G_odd, B]</code> is replaced by its\n respective color channel provided in\n ACAMERA_SENSOR_TEST_PATTERN_DATA.</p>\n <p>For example:</p>\n <pre><code>ACAMERA_SENSOR_TEST_PATTERN_DATA = [0, 0xFFFFFFFF, 0xFFFFFFFF, 0]\n </code></pre>\n <p>All green pixels are 100% green. All red/blue pixels are black.</p>\n <pre><code>ACAMERA_SENSOR_TEST_PATTERN_DATA = [0xFFFFFFFF, 0, 0xFFFFFFFF, 0]\n </code></pre>\n <p>All red pixels are 100% red. Only the odd green pixels\n are 100% green. All blue pixels are 100% black.</p>\n\n @see ACAMERA_SENSOR_TEST_PATTERN_DATA"]
pub const acamera_metadata_enum_acamera_sensor_test_pattern_mode_ACAMERA_SENSOR_TEST_PATTERN_MODE_SOLID_COLOR : acamera_metadata_enum_acamera_sensor_test_pattern_mode = 1 ;
#[doc = " <p>All pixel data is replaced with an 8-bar color pattern.</p>\n <p>The vertical bars (left-to-right) are as follows:</p>\n <ul>\n <li>100% white</li>\n <li>yellow</li>\n <li>cyan</li>\n <li>green</li>\n <li>magenta</li>\n <li>red</li>\n <li>blue</li>\n <li>black</li>\n </ul>\n <p>In general the image would look like the following:</p>\n <pre><code>W Y C G M R B K\n W Y C G M R B K\n W Y C G M R B K\n W Y C G M R B K\n W Y C G M R B K\n . . . . . . . .\n . . . . . . . .\n . . . . . . . .\n\n (B = Blue, K = Black)\n </code></pre>\n <p>Each bar should take up 1/8 of the sensor pixel array width.\n When this is not possible, the bar size should be rounded\n down to the nearest integer and the pattern can repeat\n on the right side.</p>\n <p>Each bar's height must always take up the full sensor\n pixel array height.</p>\n <p>Each pixel in this test pattern must be set to either\n 0% intensity or 100% intensity.</p>"]
pub const acamera_metadata_enum_acamera_sensor_test_pattern_mode_ACAMERA_SENSOR_TEST_PATTERN_MODE_COLOR_BARS : acamera_metadata_enum_acamera_sensor_test_pattern_mode = 2 ;
#[doc = " <p>The test pattern is similar to COLOR_BARS, except that\n each bar should start at its specified color at the top,\n and fade to gray at the bottom.</p>\n <p>Furthermore each bar is further subdivided into a left and\n right half. The left half should have a smooth gradient,\n and the right half should have a quantized gradient.</p>\n <p>In particular, the right half's should consist of blocks of the\n same color for 1/16th active sensor pixel array width.</p>\n <p>The least significant bits in the quantized gradient should\n be copied from the most significant bits of the smooth gradient.</p>\n <p>The height of each bar should always be a multiple of 128.\n When this is not the case, the pattern should repeat at the bottom\n of the image.</p>"]
pub const acamera_metadata_enum_acamera_sensor_test_pattern_mode_ACAMERA_SENSOR_TEST_PATTERN_MODE_COLOR_BARS_FADE_TO_GRAY : acamera_metadata_enum_acamera_sensor_test_pattern_mode = 3 ;
#[doc = " <p>All pixel data is replaced by a pseudo-random sequence\n generated from a PN9 512-bit sequence (typically implemented\n in hardware with a linear feedback shift register).</p>\n <p>The generator should be reset at the beginning of each frame,\n and thus each subsequent raw frame with this test pattern should\n be exactly the same as the last.</p>"]
pub const acamera_metadata_enum_acamera_sensor_test_pattern_mode_ACAMERA_SENSOR_TEST_PATTERN_MODE_PN9 : acamera_metadata_enum_acamera_sensor_test_pattern_mode = 4 ;
#[doc = " <p>The first custom test pattern. All custom patterns that are\n available only on this camera device are at least this numeric\n value.</p>\n <p>All of the custom test patterns will be static\n (that is the raw image must not vary from frame to frame).</p>"]
pub const acamera_metadata_enum_acamera_sensor_test_pattern_mode_ACAMERA_SENSOR_TEST_PATTERN_MODE_CUSTOM1 : acamera_metadata_enum_acamera_sensor_test_pattern_mode = 256 ;
pub type acamera_metadata_enum_acamera_sensor_test_pattern_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_sensor_test_pattern_mode as acamera_metadata_enum_android_sensor_test_pattern_mode_t;
#[doc = " <p>This is the default sensor pixel mode. This is the only sensor pixel mode\n supported unless a camera device advertises\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>.</p>"]
pub const acamera_metadata_enum_acamera_sensor_pixel_mode_ACAMERA_SENSOR_PIXEL_MODE_DEFAULT:
    acamera_metadata_enum_acamera_sensor_pixel_mode = 0;
#[doc = " <p>This sensor pixel mode is offered by devices with capability\n <a href=\"https://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR\">CameraMetadata#REQUEST_AVAILABLE_CAPABILITIES_ULTRA_HIGH_RESOLUTION_SENSOR</a>.\n In this mode, sensors typically do not bin pixels, as a result can offer larger\n image sizes.</p>"]
pub const acamera_metadata_enum_acamera_sensor_pixel_mode_ACAMERA_SENSOR_PIXEL_MODE_MAXIMUM_RESOLUTION : acamera_metadata_enum_acamera_sensor_pixel_mode = 1 ;
pub type acamera_metadata_enum_acamera_sensor_pixel_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_sensor_pixel_mode as acamera_metadata_enum_android_sensor_pixel_mode_t;
#[doc = " <p>The <code>RAW</code> targets in this capture have ACAMERA_SENSOR_INFO_BINNING_FACTOR as the\n bayer pattern.</p>\n\n @see ACAMERA_SENSOR_INFO_BINNING_FACTOR"]
pub const acamera_metadata_enum_acamera_sensor_raw_binning_factor_used_ACAMERA_SENSOR_RAW_BINNING_FACTOR_USED_TRUE : acamera_metadata_enum_acamera_sensor_raw_binning_factor_used = 0 ;
#[doc = " <p>The <code>RAW</code> targets have a regular bayer pattern in this capture.</p>"]
pub const acamera_metadata_enum_acamera_sensor_raw_binning_factor_used_ACAMERA_SENSOR_RAW_BINNING_FACTOR_USED_FALSE : acamera_metadata_enum_acamera_sensor_raw_binning_factor_used = 1 ;
pub type acamera_metadata_enum_acamera_sensor_raw_binning_factor_used = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_sensor_raw_binning_factor_used as acamera_metadata_enum_android_sensor_raw_binning_factor_used_t;
pub const acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement_ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_RGGB : acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement = 0 ;
pub const acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement_ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_GRBG : acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement = 1 ;
pub const acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement_ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_GBRG : acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement = 2 ;
pub const acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement_ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_BGGR : acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement = 3 ;
#[doc = " <p>Sensor is not Bayer; output has 3 16-bit\n values for each pixel, instead of just 1 16-bit value\n per pixel.</p>"]
pub const acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement_ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_RGB : acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement = 4 ;
#[doc = " <p>Sensor doesn't have any Bayer color filter.\n Such sensor captures visible light in monochrome. The exact weighting and\n wavelengths captured is not specified, but generally only includes the visible\n frequencies. This value implies a MONOCHROME camera.</p>"]
pub const acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement_ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_MONO : acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement = 5 ;
#[doc = " <p>Sensor has a near infrared filter capturing light with wavelength between\n roughly 750nm and 1400nm, and the same filter covers the whole sensor array. This\n value implies a MONOCHROME camera.</p>"]
pub const acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement_ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_NIR : acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement = 6 ;
pub type acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_sensor_info_color_filter_arrangement as acamera_metadata_enum_android_sensor_info_color_filter_arrangement_t;
#[doc = " <p>Timestamps from ACAMERA_SENSOR_TIMESTAMP are in nanoseconds and monotonic, but can\n not be compared to timestamps from other subsystems (e.g. accelerometer, gyro etc.),\n or other instances of the same or different camera devices in the same system with\n accuracy. However, the timestamps are roughly in the same timebase as\n <a href=\"https://developer.android.com/reference/android/os/SystemClock.html#uptimeMillis\">SystemClock#uptimeMillis</a>.  The accuracy is sufficient for tasks\n like A/V synchronization for video recording, at least, and the timestamps can be\n directly used together with timestamps from the audio subsystem for that task.</p>\n <p>Timestamps between streams and results for a single camera instance are comparable,\n and the timestamps for all buffers and the result metadata generated by a single\n capture are identical.</p>\n\n @see ACAMERA_SENSOR_TIMESTAMP"]
pub const acamera_metadata_enum_acamera_sensor_info_timestamp_source_ACAMERA_SENSOR_INFO_TIMESTAMP_SOURCE_UNKNOWN : acamera_metadata_enum_acamera_sensor_info_timestamp_source = 0 ;
#[doc = " <p>Timestamps from ACAMERA_SENSOR_TIMESTAMP are in the same timebase as\n <a href=\"https://developer.android.com/reference/android/os/SystemClock.html#elapsedRealtimeNanos\">SystemClock#elapsedRealtimeNanos</a>,\n and they can be compared to other timestamps using that base.</p>\n <p>When buffers from a REALTIME device are passed directly to a video encoder from the\n camera, automatic compensation is done to account for differing timebases of the\n audio and camera subsystems.  If the application is receiving buffers and then later\n sending them to a video encoder or other application where they are compared with\n audio subsystem timestamps or similar, this compensation is not present.  In those\n cases, applications need to adjust the timestamps themselves.  Since <a href=\"https://developer.android.com/reference/android/os/SystemClock.html#elapsedRealtimeNanos\">SystemClock#elapsedRealtimeNanos</a> and <a href=\"https://developer.android.com/reference/android/os/SystemClock.html#uptimeMillis\">SystemClock#uptimeMillis</a> only diverge while the device is asleep, an\n offset between the two sources can be measured once per active session and applied\n to timestamps for sufficient accuracy for A/V sync.</p>\n\n @see ACAMERA_SENSOR_TIMESTAMP"]
pub const acamera_metadata_enum_acamera_sensor_info_timestamp_source_ACAMERA_SENSOR_INFO_TIMESTAMP_SOURCE_REALTIME : acamera_metadata_enum_acamera_sensor_info_timestamp_source = 1 ;
pub type acamera_metadata_enum_acamera_sensor_info_timestamp_source = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_sensor_info_timestamp_source as acamera_metadata_enum_android_sensor_info_timestamp_source_t;
pub const acamera_metadata_enum_acamera_sensor_info_lens_shading_applied_ACAMERA_SENSOR_INFO_LENS_SHADING_APPLIED_FALSE : acamera_metadata_enum_acamera_sensor_info_lens_shading_applied = 0 ;
pub const acamera_metadata_enum_acamera_sensor_info_lens_shading_applied_ACAMERA_SENSOR_INFO_LENS_SHADING_APPLIED_TRUE : acamera_metadata_enum_acamera_sensor_info_lens_shading_applied = 1 ;
pub type acamera_metadata_enum_acamera_sensor_info_lens_shading_applied = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_sensor_info_lens_shading_applied as acamera_metadata_enum_android_sensor_info_lens_shading_applied_t;
#[doc = " <p>No lens shading correction is applied.</p>"]
pub const acamera_metadata_enum_acamera_shading_mode_ACAMERA_SHADING_MODE_OFF:
    acamera_metadata_enum_acamera_shading_mode = 0;
#[doc = " <p>Apply lens shading corrections, without slowing\n frame rate relative to sensor raw output</p>"]
pub const acamera_metadata_enum_acamera_shading_mode_ACAMERA_SHADING_MODE_FAST:
    acamera_metadata_enum_acamera_shading_mode = 1;
#[doc = " <p>Apply high-quality lens shading correction, at the\n cost of possibly reduced frame rate.</p>"]
pub const acamera_metadata_enum_acamera_shading_mode_ACAMERA_SHADING_MODE_HIGH_QUALITY:
    acamera_metadata_enum_acamera_shading_mode = 2;
pub type acamera_metadata_enum_acamera_shading_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_shading_mode as acamera_metadata_enum_android_shading_mode_t;
#[doc = " <p>Do not include face detection statistics in capture\n results.</p>"]
pub const acamera_metadata_enum_acamera_statistics_face_detect_mode_ACAMERA_STATISTICS_FACE_DETECT_MODE_OFF : acamera_metadata_enum_acamera_statistics_face_detect_mode = 0 ;
#[doc = " <p>Return face rectangle and confidence values only.</p>"]
pub const acamera_metadata_enum_acamera_statistics_face_detect_mode_ACAMERA_STATISTICS_FACE_DETECT_MODE_SIMPLE : acamera_metadata_enum_acamera_statistics_face_detect_mode = 1 ;
#[doc = " <p>Return all face\n metadata.</p>\n <p>In this mode, face rectangles, scores, landmarks, and face IDs are all valid.</p>"]
pub const acamera_metadata_enum_acamera_statistics_face_detect_mode_ACAMERA_STATISTICS_FACE_DETECT_MODE_FULL : acamera_metadata_enum_acamera_statistics_face_detect_mode = 2 ;
pub type acamera_metadata_enum_acamera_statistics_face_detect_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_statistics_face_detect_mode as acamera_metadata_enum_android_statistics_face_detect_mode_t;
#[doc = " <p>Hot pixel map production is disabled.</p>"]
pub const acamera_metadata_enum_acamera_statistics_hot_pixel_map_mode_ACAMERA_STATISTICS_HOT_PIXEL_MAP_MODE_OFF : acamera_metadata_enum_acamera_statistics_hot_pixel_map_mode = 0 ;
#[doc = " <p>Hot pixel map production is enabled.</p>"]
pub const acamera_metadata_enum_acamera_statistics_hot_pixel_map_mode_ACAMERA_STATISTICS_HOT_PIXEL_MAP_MODE_ON : acamera_metadata_enum_acamera_statistics_hot_pixel_map_mode = 1 ;
pub type acamera_metadata_enum_acamera_statistics_hot_pixel_map_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_statistics_hot_pixel_map_mode as acamera_metadata_enum_android_statistics_hot_pixel_map_mode_t;
#[doc = " <p>The camera device does not detect any flickering illumination\n in the current scene.</p>"]
pub const acamera_metadata_enum_acamera_statistics_scene_flicker_ACAMERA_STATISTICS_SCENE_FLICKER_NONE : acamera_metadata_enum_acamera_statistics_scene_flicker = 0 ;
#[doc = " <p>The camera device detects illumination flickering at 50Hz\n in the current scene.</p>"]
pub const acamera_metadata_enum_acamera_statistics_scene_flicker_ACAMERA_STATISTICS_SCENE_FLICKER_50HZ : acamera_metadata_enum_acamera_statistics_scene_flicker = 1 ;
#[doc = " <p>The camera device detects illumination flickering at 60Hz\n in the current scene.</p>"]
pub const acamera_metadata_enum_acamera_statistics_scene_flicker_ACAMERA_STATISTICS_SCENE_FLICKER_60HZ : acamera_metadata_enum_acamera_statistics_scene_flicker = 2 ;
pub type acamera_metadata_enum_acamera_statistics_scene_flicker = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_statistics_scene_flicker as acamera_metadata_enum_android_statistics_scene_flicker_t;
#[doc = " <p>Do not include a lens shading map in the capture result.</p>"]
pub const acamera_metadata_enum_acamera_statistics_lens_shading_map_mode_ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE_OFF : acamera_metadata_enum_acamera_statistics_lens_shading_map_mode = 0 ;
#[doc = " <p>Include a lens shading map in the capture result.</p>"]
pub const acamera_metadata_enum_acamera_statistics_lens_shading_map_mode_ACAMERA_STATISTICS_LENS_SHADING_MAP_MODE_ON : acamera_metadata_enum_acamera_statistics_lens_shading_map_mode = 1 ;
pub type acamera_metadata_enum_acamera_statistics_lens_shading_map_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_statistics_lens_shading_map_mode as acamera_metadata_enum_android_statistics_lens_shading_map_mode_t;
#[doc = " <p>Do not include OIS data in the capture result.</p>"]
pub const acamera_metadata_enum_acamera_statistics_ois_data_mode_ACAMERA_STATISTICS_OIS_DATA_MODE_OFF : acamera_metadata_enum_acamera_statistics_ois_data_mode = 0 ;
#[doc = " <p>Include OIS data in the capture result.</p>\n <p>ACAMERA_STATISTICS_OIS_TIMESTAMPS, ACAMERA_STATISTICS_OIS_X_SHIFTS,\n and ACAMERA_STATISTICS_OIS_Y_SHIFTS provide OIS data in the output result metadata.</p>\n\n @see ACAMERA_STATISTICS_OIS_TIMESTAMPS\n @see ACAMERA_STATISTICS_OIS_X_SHIFTS\n @see ACAMERA_STATISTICS_OIS_Y_SHIFTS"]
pub const acamera_metadata_enum_acamera_statistics_ois_data_mode_ACAMERA_STATISTICS_OIS_DATA_MODE_ON : acamera_metadata_enum_acamera_statistics_ois_data_mode = 1 ;
pub type acamera_metadata_enum_acamera_statistics_ois_data_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_statistics_ois_data_mode as acamera_metadata_enum_android_statistics_ois_data_mode_t;
#[doc = " <p>Use the tone mapping curve specified in\n the ACAMERA_TONEMAPCURVE_* entries.</p>\n <p>All color enhancement and tonemapping must be disabled, except\n for applying the tonemapping curve specified by\n android.tonemap.curve.</p>\n <p>Must not slow down frame rate relative to raw\n sensor output.</p>"]
pub const acamera_metadata_enum_acamera_tonemap_mode_ACAMERA_TONEMAP_MODE_CONTRAST_CURVE:
    acamera_metadata_enum_acamera_tonemap_mode = 0;
#[doc = " <p>Advanced gamma mapping and color enhancement may be applied, without\n reducing frame rate compared to raw sensor output.</p>"]
pub const acamera_metadata_enum_acamera_tonemap_mode_ACAMERA_TONEMAP_MODE_FAST:
    acamera_metadata_enum_acamera_tonemap_mode = 1;
#[doc = " <p>High-quality gamma mapping and color enhancement will be applied, at\n the cost of possibly reduced frame rate compared to raw sensor output.</p>"]
pub const acamera_metadata_enum_acamera_tonemap_mode_ACAMERA_TONEMAP_MODE_HIGH_QUALITY:
    acamera_metadata_enum_acamera_tonemap_mode = 2;
#[doc = " <p>Use the gamma value specified in ACAMERA_TONEMAP_GAMMA to perform\n tonemapping.</p>\n <p>All color enhancement and tonemapping must be disabled, except\n for applying the tonemapping curve specified by ACAMERA_TONEMAP_GAMMA.</p>\n <p>Must not slow down frame rate relative to raw sensor output.</p>\n\n @see ACAMERA_TONEMAP_GAMMA"]
pub const acamera_metadata_enum_acamera_tonemap_mode_ACAMERA_TONEMAP_MODE_GAMMA_VALUE:
    acamera_metadata_enum_acamera_tonemap_mode = 3;
#[doc = " <p>Use the preset tonemapping curve specified in\n ACAMERA_TONEMAP_PRESET_CURVE to perform tonemapping.</p>\n <p>All color enhancement and tonemapping must be disabled, except\n for applying the tonemapping curve specified by\n ACAMERA_TONEMAP_PRESET_CURVE.</p>\n <p>Must not slow down frame rate relative to raw sensor output.</p>\n\n @see ACAMERA_TONEMAP_PRESET_CURVE"]
pub const acamera_metadata_enum_acamera_tonemap_mode_ACAMERA_TONEMAP_MODE_PRESET_CURVE:
    acamera_metadata_enum_acamera_tonemap_mode = 4;
pub type acamera_metadata_enum_acamera_tonemap_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_tonemap_mode as acamera_metadata_enum_android_tonemap_mode_t;
#[doc = " <p>Tonemapping curve is defined by sRGB</p>"]
pub const acamera_metadata_enum_acamera_tonemap_preset_curve_ACAMERA_TONEMAP_PRESET_CURVE_SRGB:
    acamera_metadata_enum_acamera_tonemap_preset_curve = 0;
#[doc = " <p>Tonemapping curve is defined by ITU-R BT.709</p>"]
pub const acamera_metadata_enum_acamera_tonemap_preset_curve_ACAMERA_TONEMAP_PRESET_CURVE_REC709:
    acamera_metadata_enum_acamera_tonemap_preset_curve = 1;
pub type acamera_metadata_enum_acamera_tonemap_preset_curve = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_tonemap_preset_curve as acamera_metadata_enum_android_tonemap_preset_curve_t;
#[doc = " <p>This camera device does not have enough capabilities to qualify as a <code>FULL</code> device or\n better.</p>\n <p>Only the stream configurations listed in the <code>LEGACY</code> and <code>LIMITED</code> tables in the\n {@link ACameraDevice_createCaptureSession createCaptureSession} documentation are guaranteed to be supported.</p>\n <p>All <code>LIMITED</code> devices support the <code>BACKWARDS_COMPATIBLE</code> capability, indicating basic\n support for color image capture. The only exception is that the device may\n alternatively support only the <code>DEPTH_OUTPUT</code> capability, if it can only output depth\n measurements and not color images.</p>\n <p><code>LIMITED</code> devices and above require the use of ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER\n to lock exposure metering (and calculate flash power, for cameras with flash) before\n capturing a high-quality still image.</p>\n <p>A <code>LIMITED</code> device that only lists the <code>BACKWARDS_COMPATIBLE</code> capability is only\n required to support full-automatic operation and post-processing (<code>OFF</code> is not\n supported for ACAMERA_CONTROL_AE_MODE, ACAMERA_CONTROL_AF_MODE, or\n ACAMERA_CONTROL_AWB_MODE)</p>\n <p>Additional capabilities may optionally be supported by a <code>LIMITED</code>-level device, and\n can be checked for in ACAMERA_REQUEST_AVAILABLE_CAPABILITIES.</p>\n\n @see ACAMERA_CONTROL_AE_MODE\n @see ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER\n @see ACAMERA_CONTROL_AF_MODE\n @see ACAMERA_CONTROL_AWB_MODE\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES"]
pub const acamera_metadata_enum_acamera_info_supported_hardware_level_ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED : acamera_metadata_enum_acamera_info_supported_hardware_level = 0 ;
#[doc = " <p>This camera device is capable of supporting advanced imaging applications.</p>\n <p>The stream configurations listed in the <code>FULL</code>, <code>LEGACY</code> and <code>LIMITED</code> tables in the\n {@link ACameraDevice_createCaptureSession createCaptureSession} documentation are guaranteed to be supported.</p>\n <p>A <code>FULL</code> device will support below capabilities:</p>\n <ul>\n <li><code>BURST_CAPTURE</code> capability (ACAMERA_REQUEST_AVAILABLE_CAPABILITIES contains\n   <code>BURST_CAPTURE</code>)</li>\n <li>Per frame control (ACAMERA_SYNC_MAX_LATENCY <code>==</code> PER_FRAME_CONTROL)</li>\n <li>Manual sensor control (ACAMERA_REQUEST_AVAILABLE_CAPABILITIES contains <code>MANUAL_SENSOR</code>)</li>\n <li>Manual post-processing control (ACAMERA_REQUEST_AVAILABLE_CAPABILITIES contains\n   <code>MANUAL_POST_PROCESSING</code>)</li>\n <li>The required exposure time range defined in ACAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE</li>\n <li>The required maxFrameDuration defined in ACAMERA_SENSOR_INFO_MAX_FRAME_DURATION</li>\n </ul>\n <p>Note:\n Pre-API level 23, FULL devices also supported arbitrary cropping region\n (ACAMERA_SCALER_CROPPING_TYPE <code>== FREEFORM</code>); this requirement was relaxed in API level\n 23, and <code>FULL</code> devices may only support <code>CENTERED</code> cropping.</p>\n\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES\n @see ACAMERA_SCALER_CROPPING_TYPE\n @see ACAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE\n @see ACAMERA_SENSOR_INFO_MAX_FRAME_DURATION\n @see ACAMERA_SYNC_MAX_LATENCY"]
pub const acamera_metadata_enum_acamera_info_supported_hardware_level_ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_FULL : acamera_metadata_enum_acamera_info_supported_hardware_level = 1 ;
#[doc = " <p>This camera device is running in backward compatibility mode.</p>\n <p>Only the stream configurations listed in the <code>LEGACY</code> table in the {@link ACameraDevice_createCaptureSession createCaptureSession} documentation are supported.</p>\n <p>A <code>LEGACY</code> device does not support per-frame control, manual sensor control, manual\n post-processing, arbitrary cropping regions, and has relaxed performance constraints.\n No additional capabilities beyond <code>BACKWARD_COMPATIBLE</code> will ever be listed by a\n <code>LEGACY</code> device in ACAMERA_REQUEST_AVAILABLE_CAPABILITIES.</p>\n <p>In addition, the ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER is not functional on <code>LEGACY</code>\n devices. Instead, every request that includes a JPEG-format output target is treated\n as triggering a still capture, internally executing a precapture trigger.  This may\n fire the flash for flash power metering during precapture, and then fire the flash\n for the final capture, if a flash is available on the device and the AE mode is set to\n enable the flash.</p>\n <p>Devices that initially shipped with Android version <a href=\"https://developer.android.com/reference/android/os/Build.VERSION_CODES.html#Q\">Q</a> or newer will not include any LEGACY-level devices.</p>\n\n @see ACAMERA_CONTROL_AE_PRECAPTURE_TRIGGER\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES"]
pub const acamera_metadata_enum_acamera_info_supported_hardware_level_ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY : acamera_metadata_enum_acamera_info_supported_hardware_level = 2 ;
#[doc = " <p>This camera device is capable of YUV reprocessing and RAW data capture, in addition to\n FULL-level capabilities.</p>\n <p>The stream configurations listed in the <code>LEVEL_3</code>, <code>RAW</code>, <code>FULL</code>, <code>LEGACY</code> and\n <code>LIMITED</code> tables in the {@link ACameraDevice_createCaptureSession createCaptureSession} documentation are guaranteed to be supported.</p>\n <p>The following additional capabilities are guaranteed to be supported:</p>\n <ul>\n <li><code>YUV_REPROCESSING</code> capability (ACAMERA_REQUEST_AVAILABLE_CAPABILITIES contains\n   <code>YUV_REPROCESSING</code>)</li>\n <li><code>RAW</code> capability (ACAMERA_REQUEST_AVAILABLE_CAPABILITIES contains\n   <code>RAW</code>)</li>\n </ul>\n\n @see ACAMERA_REQUEST_AVAILABLE_CAPABILITIES"]
pub const acamera_metadata_enum_acamera_info_supported_hardware_level_ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_3 : acamera_metadata_enum_acamera_info_supported_hardware_level = 3 ;
#[doc = " <p>This camera device is backed by an external camera connected to this Android device.</p>\n <p>The device has capability identical to a LIMITED level device, with the following\n exceptions:</p>\n <ul>\n <li>The device may not report lens/sensor related information such as<ul>\n <li>ACAMERA_LENS_FOCAL_LENGTH</li>\n <li>ACAMERA_LENS_INFO_HYPERFOCAL_DISTANCE</li>\n <li>ACAMERA_SENSOR_INFO_PHYSICAL_SIZE</li>\n <li>ACAMERA_SENSOR_INFO_WHITE_LEVEL</li>\n <li>ACAMERA_SENSOR_BLACK_LEVEL_PATTERN</li>\n <li>ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT</li>\n <li>ACAMERA_SENSOR_ROLLING_SHUTTER_SKEW</li>\n </ul>\n </li>\n <li>The device will report 0 for ACAMERA_SENSOR_ORIENTATION</li>\n <li>The device has less guarantee on stable framerate, as the framerate partly depends\n   on the external camera being used.</li>\n </ul>\n\n @see ACAMERA_LENS_FOCAL_LENGTH\n @see ACAMERA_LENS_INFO_HYPERFOCAL_DISTANCE\n @see ACAMERA_SENSOR_BLACK_LEVEL_PATTERN\n @see ACAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT\n @see ACAMERA_SENSOR_INFO_PHYSICAL_SIZE\n @see ACAMERA_SENSOR_INFO_WHITE_LEVEL\n @see ACAMERA_SENSOR_ORIENTATION\n @see ACAMERA_SENSOR_ROLLING_SHUTTER_SKEW"]
pub const acamera_metadata_enum_acamera_info_supported_hardware_level_ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_EXTERNAL : acamera_metadata_enum_acamera_info_supported_hardware_level = 4 ;
pub type acamera_metadata_enum_acamera_info_supported_hardware_level = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_info_supported_hardware_level as acamera_metadata_enum_android_info_supported_hardware_level_t;
pub const acamera_metadata_enum_acamera_black_level_lock_ACAMERA_BLACK_LEVEL_LOCK_OFF:
    acamera_metadata_enum_acamera_black_level_lock = 0;
pub const acamera_metadata_enum_acamera_black_level_lock_ACAMERA_BLACK_LEVEL_LOCK_ON:
    acamera_metadata_enum_acamera_black_level_lock = 1;
pub type acamera_metadata_enum_acamera_black_level_lock = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_black_level_lock as acamera_metadata_enum_android_black_level_lock_t;
#[doc = " <p>The current result is not yet fully synchronized to any request.</p>\n <p>Synchronization is in progress, and reading metadata from this\n result may include a mix of data that have taken effect since the\n last synchronization time.</p>\n <p>In some future result, within ACAMERA_SYNC_MAX_LATENCY frames,\n this value will update to the actual frame number frame number\n the result is guaranteed to be synchronized to (as long as the\n request settings remain constant).</p>\n\n @see ACAMERA_SYNC_MAX_LATENCY"]
pub const acamera_metadata_enum_acamera_sync_frame_number_ACAMERA_SYNC_FRAME_NUMBER_CONVERGING:
    acamera_metadata_enum_acamera_sync_frame_number = -1;
#[doc = " <p>The current result's synchronization status is unknown.</p>\n <p>The result may have already converged, or it may be in\n progress.  Reading from this result may include some mix\n of settings from past requests.</p>\n <p>After a settings change, the new settings will eventually all\n take effect for the output buffers and results. However, this\n value will not change when that happens. Altering settings\n rapidly may provide outcomes using mixes of settings from recent\n requests.</p>\n <p>This value is intended primarily for backwards compatibility with\n the older camera implementations (for android.hardware.Camera).</p>"]
pub const acamera_metadata_enum_acamera_sync_frame_number_ACAMERA_SYNC_FRAME_NUMBER_UNKNOWN:
    acamera_metadata_enum_acamera_sync_frame_number = -2;
pub type acamera_metadata_enum_acamera_sync_frame_number = ::std::os::raw::c_int;
pub use self::acamera_metadata_enum_acamera_sync_frame_number as acamera_metadata_enum_android_sync_frame_number_t;
#[doc = " <p>Every frame has the requests immediately applied.</p>\n <p>Changing controls over multiple requests one after another will\n produce results that have those controls applied atomically\n each frame.</p>\n <p>All FULL capability devices will have this as their maxLatency.</p>"]
pub const acamera_metadata_enum_acamera_sync_max_latency_ACAMERA_SYNC_MAX_LATENCY_PER_FRAME_CONTROL : acamera_metadata_enum_acamera_sync_max_latency = 0 ;
#[doc = " <p>Each new frame has some subset (potentially the entire set)\n of the past requests applied to the camera settings.</p>\n <p>By submitting a series of identical requests, the camera device\n will eventually have the camera settings applied, but it is\n unknown when that exact point will be.</p>\n <p>All LEGACY capability devices will have this as their maxLatency.</p>"]
pub const acamera_metadata_enum_acamera_sync_max_latency_ACAMERA_SYNC_MAX_LATENCY_UNKNOWN:
    acamera_metadata_enum_acamera_sync_max_latency = -1;
pub type acamera_metadata_enum_acamera_sync_max_latency = ::std::os::raw::c_int;
pub use self::acamera_metadata_enum_acamera_sync_max_latency as acamera_metadata_enum_android_sync_max_latency_t;
pub const acamera_metadata_enum_acamera_depth_available_depth_stream_configurations_ACAMERA_DEPTH_AVAILABLE_DEPTH_STREAM_CONFIGURATIONS_OUTPUT : acamera_metadata_enum_acamera_depth_available_depth_stream_configurations = 0 ;
pub const acamera_metadata_enum_acamera_depth_available_depth_stream_configurations_ACAMERA_DEPTH_AVAILABLE_DEPTH_STREAM_CONFIGURATIONS_INPUT : acamera_metadata_enum_acamera_depth_available_depth_stream_configurations = 1 ;
pub type acamera_metadata_enum_acamera_depth_available_depth_stream_configurations =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_depth_available_depth_stream_configurations as acamera_metadata_enum_android_depth_available_depth_stream_configurations_t;
pub const acamera_metadata_enum_acamera_depth_depth_is_exclusive_ACAMERA_DEPTH_DEPTH_IS_EXCLUSIVE_FALSE : acamera_metadata_enum_acamera_depth_depth_is_exclusive = 0 ;
pub const acamera_metadata_enum_acamera_depth_depth_is_exclusive_ACAMERA_DEPTH_DEPTH_IS_EXCLUSIVE_TRUE : acamera_metadata_enum_acamera_depth_depth_is_exclusive = 1 ;
pub type acamera_metadata_enum_acamera_depth_depth_is_exclusive = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_depth_depth_is_exclusive as acamera_metadata_enum_android_depth_depth_is_exclusive_t;
pub const acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations_ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STREAM_CONFIGURATIONS_OUTPUT : acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations = 0 ;
pub const acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations_ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STREAM_CONFIGURATIONS_INPUT : acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations = 1 ;
pub type acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations as acamera_metadata_enum_android_depth_available_dynamic_depth_stream_configurations_t;
pub const acamera_metadata_enum_acamera_depth_available_depth_stream_configurations_maximum_resolution_ACAMERA_DEPTH_AVAILABLE_DEPTH_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION_OUTPUT : acamera_metadata_enum_acamera_depth_available_depth_stream_configurations_maximum_resolution = 0 ;
pub const acamera_metadata_enum_acamera_depth_available_depth_stream_configurations_maximum_resolution_ACAMERA_DEPTH_AVAILABLE_DEPTH_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION_INPUT : acamera_metadata_enum_acamera_depth_available_depth_stream_configurations_maximum_resolution = 1 ;
pub type acamera_metadata_enum_acamera_depth_available_depth_stream_configurations_maximum_resolution =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_depth_available_depth_stream_configurations_maximum_resolution as acamera_metadata_enum_android_depth_available_depth_stream_configurations_maximum_resolution_t;
pub const acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations_maximum_resolution_ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION_OUTPUT : acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations_maximum_resolution = 0 ;
pub const acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations_maximum_resolution_ACAMERA_DEPTH_AVAILABLE_DYNAMIC_DEPTH_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION_INPUT : acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations_maximum_resolution = 1 ;
pub type acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations_maximum_resolution =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_depth_available_dynamic_depth_stream_configurations_maximum_resolution as acamera_metadata_enum_android_depth_available_dynamic_depth_stream_configurations_maximum_resolution_t;
#[doc = " <p>A software mechanism is used to synchronize between the physical cameras. As a result,\n the timestamp of an image from a physical stream is only an approximation of the\n image sensor start-of-exposure time.</p>"]
pub const acamera_metadata_enum_acamera_logical_multi_camera_sensor_sync_type_ACAMERA_LOGICAL_MULTI_CAMERA_SENSOR_SYNC_TYPE_APPROXIMATE : acamera_metadata_enum_acamera_logical_multi_camera_sensor_sync_type = 0 ;
#[doc = " <p>The camera device supports frame timestamp synchronization at the hardware level,\n and the timestamp of a physical stream image accurately reflects its\n start-of-exposure time.</p>"]
pub const acamera_metadata_enum_acamera_logical_multi_camera_sensor_sync_type_ACAMERA_LOGICAL_MULTI_CAMERA_SENSOR_SYNC_TYPE_CALIBRATED : acamera_metadata_enum_acamera_logical_multi_camera_sensor_sync_type = 1 ;
pub type acamera_metadata_enum_acamera_logical_multi_camera_sensor_sync_type =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_logical_multi_camera_sensor_sync_type as acamera_metadata_enum_android_logical_multi_camera_sensor_sync_type_t;
#[doc = " <p>No distortion correction is applied.</p>"]
pub const acamera_metadata_enum_acamera_distortion_correction_mode_ACAMERA_DISTORTION_CORRECTION_MODE_OFF : acamera_metadata_enum_acamera_distortion_correction_mode = 0 ;
#[doc = " <p>Lens distortion correction is applied without reducing frame rate\n relative to sensor output. It may be the same as OFF if distortion correction would\n reduce frame rate relative to sensor.</p>"]
pub const acamera_metadata_enum_acamera_distortion_correction_mode_ACAMERA_DISTORTION_CORRECTION_MODE_FAST : acamera_metadata_enum_acamera_distortion_correction_mode = 1 ;
#[doc = " <p>High-quality distortion correction is applied, at the cost of\n possibly reduced frame rate relative to sensor output.</p>"]
pub const acamera_metadata_enum_acamera_distortion_correction_mode_ACAMERA_DISTORTION_CORRECTION_MODE_HIGH_QUALITY : acamera_metadata_enum_acamera_distortion_correction_mode = 2 ;
pub type acamera_metadata_enum_acamera_distortion_correction_mode = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_distortion_correction_mode as acamera_metadata_enum_android_distortion_correction_mode_t;
pub const acamera_metadata_enum_acamera_heic_available_heic_stream_configurations_ACAMERA_HEIC_AVAILABLE_HEIC_STREAM_CONFIGURATIONS_OUTPUT : acamera_metadata_enum_acamera_heic_available_heic_stream_configurations = 0 ;
pub const acamera_metadata_enum_acamera_heic_available_heic_stream_configurations_ACAMERA_HEIC_AVAILABLE_HEIC_STREAM_CONFIGURATIONS_INPUT : acamera_metadata_enum_acamera_heic_available_heic_stream_configurations = 1 ;
pub type acamera_metadata_enum_acamera_heic_available_heic_stream_configurations =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_heic_available_heic_stream_configurations as acamera_metadata_enum_android_heic_available_heic_stream_configurations_t;
pub const acamera_metadata_enum_acamera_heic_available_heic_stream_configurations_maximum_resolution_ACAMERA_HEIC_AVAILABLE_HEIC_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION_OUTPUT : acamera_metadata_enum_acamera_heic_available_heic_stream_configurations_maximum_resolution = 0 ;
pub const acamera_metadata_enum_acamera_heic_available_heic_stream_configurations_maximum_resolution_ACAMERA_HEIC_AVAILABLE_HEIC_STREAM_CONFIGURATIONS_MAXIMUM_RESOLUTION_INPUT : acamera_metadata_enum_acamera_heic_available_heic_stream_configurations_maximum_resolution = 1 ;
pub type acamera_metadata_enum_acamera_heic_available_heic_stream_configurations_maximum_resolution =
    ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_heic_available_heic_stream_configurations_maximum_resolution as acamera_metadata_enum_android_heic_available_heic_stream_configurations_maximum_resolution_t;
#[doc = " <p>The camera device exists inside of the vehicle cabin.</p>"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_INTERIOR:
    acamera_metadata_enum_acamera_automotive_location = 0;
#[doc = " <p>The camera exists outside of the vehicle body frame but not exactly on one of the\n exterior locations this enum defines.  The applications should determine the exact\n location from ACAMERA_LENS_POSE_TRANSLATION.</p>\n\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_EXTERIOR_OTHER : acamera_metadata_enum_acamera_automotive_location = 1 ;
#[doc = " <p>The camera device exists outside of the vehicle body frame and on its front side.</p>"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_EXTERIOR_FRONT : acamera_metadata_enum_acamera_automotive_location = 2 ;
#[doc = " <p>The camera device exists outside of the vehicle body frame and on its rear side.</p>"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_EXTERIOR_REAR : acamera_metadata_enum_acamera_automotive_location = 3 ;
#[doc = " <p>The camera device exists outside and on left side of the vehicle body frame.</p>"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_EXTERIOR_LEFT : acamera_metadata_enum_acamera_automotive_location = 4 ;
#[doc = " <p>The camera device exists outside and on right side of the vehicle body frame.</p>"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_EXTERIOR_RIGHT : acamera_metadata_enum_acamera_automotive_location = 5 ;
#[doc = " <p>The camera device exists on an extra vehicle, such as the trailer, but not exactly\n on one of front, rear, left, or right side.  Applications should determine the exact\n location from ACAMERA_LENS_POSE_TRANSLATION.</p>\n\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_EXTRA_OTHER : acamera_metadata_enum_acamera_automotive_location = 6 ;
#[doc = " <p>The camera device exists outside of the extra vehicle's body frame and on its front\n side.</p>"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_EXTRA_FRONT : acamera_metadata_enum_acamera_automotive_location = 7 ;
#[doc = " <p>The camera device exists outside of the extra vehicle's body frame and on its rear\n side.</p>"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_EXTRA_REAR : acamera_metadata_enum_acamera_automotive_location = 8 ;
#[doc = " <p>The camera device exists outside and on left side of the extra vehicle body.</p>"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_EXTRA_LEFT : acamera_metadata_enum_acamera_automotive_location = 9 ;
#[doc = " <p>The camera device exists outside and on right side of the extra vehicle body.</p>"]
pub const acamera_metadata_enum_acamera_automotive_location_ACAMERA_AUTOMOTIVE_LOCATION_EXTRA_RIGHT : acamera_metadata_enum_acamera_automotive_location = 10 ;
pub type acamera_metadata_enum_acamera_automotive_location = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_automotive_location as acamera_metadata_enum_android_automotive_location_t;
#[doc = " <p>The camera device faces the outside of the vehicle body frame but not exactly\n one of the exterior sides defined by this enum.  Applications should determine\n the exact facing direction from ACAMERA_LENS_POSE_ROTATION and\n ACAMERA_LENS_POSE_TRANSLATION.</p>\n\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_EXTERIOR_OTHER : acamera_metadata_enum_acamera_automotive_lens_facing = 0 ;
#[doc = " <p>The camera device faces the front of the vehicle body frame.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_EXTERIOR_FRONT : acamera_metadata_enum_acamera_automotive_lens_facing = 1 ;
#[doc = " <p>The camera device faces the rear of the vehicle body frame.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_EXTERIOR_REAR : acamera_metadata_enum_acamera_automotive_lens_facing = 2 ;
#[doc = " <p>The camera device faces the left side of the vehicle body frame.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_EXTERIOR_LEFT : acamera_metadata_enum_acamera_automotive_lens_facing = 3 ;
#[doc = " <p>The camera device faces the right side of the vehicle body frame.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_EXTERIOR_RIGHT : acamera_metadata_enum_acamera_automotive_lens_facing = 4 ;
#[doc = " <p>The camera device faces the inside of the vehicle body frame but not exactly\n one of seats described by this enum.  Applications should determine the exact\n facing direction from ACAMERA_LENS_POSE_ROTATION and ACAMERA_LENS_POSE_TRANSLATION.</p>\n\n @see ACAMERA_LENS_POSE_ROTATION\n @see ACAMERA_LENS_POSE_TRANSLATION"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_INTERIOR_OTHER : acamera_metadata_enum_acamera_automotive_lens_facing = 5 ;
#[doc = " <p>The camera device faces the left side seat of the first row.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_INTERIOR_SEAT_ROW_1_LEFT : acamera_metadata_enum_acamera_automotive_lens_facing = 6 ;
#[doc = " <p>The camera device faces the center seat of the first row.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_INTERIOR_SEAT_ROW_1_CENTER : acamera_metadata_enum_acamera_automotive_lens_facing = 7 ;
#[doc = " <p>The camera device faces the right seat of the first row.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_INTERIOR_SEAT_ROW_1_RIGHT : acamera_metadata_enum_acamera_automotive_lens_facing = 8 ;
#[doc = " <p>The camera device faces the left side seat of the second row.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_INTERIOR_SEAT_ROW_2_LEFT : acamera_metadata_enum_acamera_automotive_lens_facing = 9 ;
#[doc = " <p>The camera device faces the center seat of the second row.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_INTERIOR_SEAT_ROW_2_CENTER : acamera_metadata_enum_acamera_automotive_lens_facing = 10 ;
#[doc = " <p>The camera device faces the right side seat of the second row.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_INTERIOR_SEAT_ROW_2_RIGHT : acamera_metadata_enum_acamera_automotive_lens_facing = 11 ;
#[doc = " <p>The camera device faces the left side seat of the third row.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_INTERIOR_SEAT_ROW_3_LEFT : acamera_metadata_enum_acamera_automotive_lens_facing = 12 ;
#[doc = " <p>The camera device faces the center seat of the third row.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_INTERIOR_SEAT_ROW_3_CENTER : acamera_metadata_enum_acamera_automotive_lens_facing = 13 ;
#[doc = " <p>The camera device faces the right seat of the third row.</p>"]
pub const acamera_metadata_enum_acamera_automotive_lens_facing_ACAMERA_AUTOMOTIVE_LENS_FACING_INTERIOR_SEAT_ROW_3_RIGHT : acamera_metadata_enum_acamera_automotive_lens_facing = 14 ;
pub type acamera_metadata_enum_acamera_automotive_lens_facing = ::std::os::raw::c_uint;
pub use self::acamera_metadata_enum_acamera_automotive_lens_facing as acamera_metadata_enum_android_automotive_lens_facing_t;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraMetadata {
    _unused: [u8; 0],
}
#[doc = " Unsigned 8-bit integer (uint8_t)"]
pub const ACAMERA_TYPE_BYTE: _bindgen_ty_23 = 0;
#[doc = " Signed 32-bit integer (int32_t)"]
pub const ACAMERA_TYPE_INT32: _bindgen_ty_23 = 1;
#[doc = " 32-bit float (float)"]
pub const ACAMERA_TYPE_FLOAT: _bindgen_ty_23 = 2;
#[doc = " Signed 64-bit integer (int64_t)"]
pub const ACAMERA_TYPE_INT64: _bindgen_ty_23 = 3;
#[doc = " 64-bit float (double)"]
pub const ACAMERA_TYPE_DOUBLE: _bindgen_ty_23 = 4;
#[doc = " A 64-bit fraction (ACameraMetadata_rational)"]
pub const ACAMERA_TYPE_RATIONAL: _bindgen_ty_23 = 5;
#[doc = " Number of type fields"]
pub const ACAMERA_NUM_TYPES: _bindgen_ty_23 = 6;
#[doc = " Possible data types of a metadata entry.\n\n Keep in sync with system/media/include/system/camera_metadata.h"]
pub type _bindgen_ty_23 = ::std::os::raw::c_uint;
#[doc = " Definition of rational data type in {@link ACameraMetadata}."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraMetadata_rational {
    pub numerator: i32,
    pub denominator: i32,
}
#[test]
fn bindgen_test_layout_ACameraMetadata_rational() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraMetadata_rational> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraMetadata_rational>(),
        8usize,
        concat!("Size of: ", stringify!(ACameraMetadata_rational))
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraMetadata_rational>(),
        4usize,
        concat!("Alignment of ", stringify!(ACameraMetadata_rational))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).numerator) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_rational),
            "::",
            stringify!(numerator)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).denominator) as usize - ptr as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_rational),
            "::",
            stringify!(denominator)
        )
    );
}
#[doc = " A single camera metadata entry.\n\n <p>Each entry is an array of values, though many metadata fields may only have 1 entry in the\n array.</p>"]
#[repr(C)]
#[derive(Copy, Clone)]
pub struct ACameraMetadata_entry {
    #[doc = " The tag identifying the entry.\n\n <p> It is one of the values defined in {@link NdkCameraMetadataTags.h}, and defines how the\n entry should be interpreted and which parts of the API provide it.\n See {@link NdkCameraMetadataTags.h} for more details. </p>"]
    pub tag: u32,
    #[doc = " The data type of this metadata entry.\n\n <p>Must be one of ACAMERA_TYPE_* enum values defined above. A particular tag always has the\n same type.</p>"]
    pub type_: u8,
    #[doc = " Count of elements (NOT count of bytes) in this metadata entry."]
    pub count: u32,
    pub data: ACameraMetadata_entry__bindgen_ty_1,
}
#[doc = " Pointer to the data held in this metadata entry.\n\n <p>The type field above defines which union member pointer is valid. The count field above\n defines the length of the data in number of elements.</p>"]
#[repr(C)]
#[derive(Copy, Clone)]
pub union ACameraMetadata_entry__bindgen_ty_1 {
    pub u8_: *mut u8,
    pub i32_: *mut i32,
    pub f: *mut f32,
    pub i64_: *mut i64,
    pub d: *mut f64,
    pub r: *mut ACameraMetadata_rational,
}
#[test]
fn bindgen_test_layout_ACameraMetadata_entry__bindgen_ty_1() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraMetadata_entry__bindgen_ty_1> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraMetadata_entry__bindgen_ty_1>(),
        8usize,
        concat!("Size of: ", stringify!(ACameraMetadata_entry__bindgen_ty_1))
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraMetadata_entry__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(ACameraMetadata_entry__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).u8_) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_entry__bindgen_ty_1),
            "::",
            stringify!(u8_)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).i32_) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_entry__bindgen_ty_1),
            "::",
            stringify!(i32_)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).f) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_entry__bindgen_ty_1),
            "::",
            stringify!(f)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).i64_) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_entry__bindgen_ty_1),
            "::",
            stringify!(i64_)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).d) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_entry__bindgen_ty_1),
            "::",
            stringify!(d)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).r) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_entry__bindgen_ty_1),
            "::",
            stringify!(r)
        )
    );
}
#[test]
fn bindgen_test_layout_ACameraMetadata_entry() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraMetadata_entry> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraMetadata_entry>(),
        24usize,
        concat!("Size of: ", stringify!(ACameraMetadata_entry))
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraMetadata_entry>(),
        8usize,
        concat!("Alignment of ", stringify!(ACameraMetadata_entry))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).tag) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_entry),
            "::",
            stringify!(tag)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).type_) as usize - ptr as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_entry),
            "::",
            stringify!(type_)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).count) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_entry),
            "::",
            stringify!(count)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).data) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_entry),
            "::",
            stringify!(data)
        )
    );
}
#[doc = " A single read-only camera metadata entry.\n\n <p>Each entry is an array of values, though many metadata fields may only have 1 entry in the\n array.</p>"]
#[repr(C)]
#[derive(Copy, Clone)]
pub struct ACameraMetadata_const_entry {
    #[doc = " The tag identifying the entry.\n\n <p> It is one of the values defined in {@link NdkCameraMetadataTags.h}, and defines how the\n entry should be interpreted and which parts of the API provide it.\n See {@link NdkCameraMetadataTags.h} for more details. </p>"]
    pub tag: u32,
    #[doc = " The data type of this metadata entry.\n\n <p>Must be one of ACAMERA_TYPE_* enum values defined above. A particular tag always has the\n same type.</p>"]
    pub type_: u8,
    #[doc = " Count of elements (NOT count of bytes) in this metadata entry."]
    pub count: u32,
    pub data: ACameraMetadata_const_entry__bindgen_ty_1,
}
#[doc = " Pointer to the data held in this metadata entry.\n\n <p>The type field above defines which union member pointer is valid. The count field above\n defines the length of the data in number of elements.</p>"]
#[repr(C)]
#[derive(Copy, Clone)]
pub union ACameraMetadata_const_entry__bindgen_ty_1 {
    pub u8_: *const u8,
    pub i32_: *const i32,
    pub f: *const f32,
    pub i64_: *const i64,
    pub d: *const f64,
    pub r: *const ACameraMetadata_rational,
}
#[test]
fn bindgen_test_layout_ACameraMetadata_const_entry__bindgen_ty_1() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraMetadata_const_entry__bindgen_ty_1> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraMetadata_const_entry__bindgen_ty_1>(),
        8usize,
        concat!(
            "Size of: ",
            stringify!(ACameraMetadata_const_entry__bindgen_ty_1)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraMetadata_const_entry__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(ACameraMetadata_const_entry__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).u8_) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_const_entry__bindgen_ty_1),
            "::",
            stringify!(u8_)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).i32_) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_const_entry__bindgen_ty_1),
            "::",
            stringify!(i32_)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).f) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_const_entry__bindgen_ty_1),
            "::",
            stringify!(f)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).i64_) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_const_entry__bindgen_ty_1),
            "::",
            stringify!(i64_)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).d) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_const_entry__bindgen_ty_1),
            "::",
            stringify!(d)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).r) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_const_entry__bindgen_ty_1),
            "::",
            stringify!(r)
        )
    );
}
#[test]
fn bindgen_test_layout_ACameraMetadata_const_entry() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraMetadata_const_entry> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraMetadata_const_entry>(),
        24usize,
        concat!("Size of: ", stringify!(ACameraMetadata_const_entry))
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraMetadata_const_entry>(),
        8usize,
        concat!("Alignment of ", stringify!(ACameraMetadata_const_entry))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).tag) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_const_entry),
            "::",
            stringify!(tag)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).type_) as usize - ptr as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_const_entry),
            "::",
            stringify!(type_)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).count) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_const_entry),
            "::",
            stringify!(count)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).data) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraMetadata_const_entry),
            "::",
            stringify!(data)
        )
    );
}
extern "C" {
    #[doc = " Get a metadata entry from an input {@link ACameraMetadata}.\n\n <p>The memory of the data field in the returned entry is managed by camera framework. Do not\n attempt to free it.</p>\n\n @param metadata the {@link ACameraMetadata} of interest.\n @param tag the tag value of the camera metadata entry to be get.\n @param entry the output {@link ACameraMetadata_const_entry} will be filled here if the method\n        call succeeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if metadata or entry is NULL.</li>\n         <li>{@link ACAMERA_ERROR_METADATA_NOT_FOUND} if input metadata does not contain an entry\n             of input tag value.</li></ul>"]
    pub fn ACameraMetadata_getConstEntry(
        metadata: *const ACameraMetadata,
        tag: u32,
        entry: *mut ACameraMetadata_const_entry,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " List all the entry tags in input {@link ACameraMetadata}.\n\n @param metadata the {@link ACameraMetadata} of interest.\n @param numEntries number of metadata entries in input {@link ACameraMetadata}\n @param tags the tag values of the metadata entries. Length of tags is returned in numEntries\n             argument. The memory is managed by ACameraMetadata itself and must NOT be free/delete\n             by application. Do NOT access tags after calling ACameraMetadata_free.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if metadata, numEntries or tags is NULL.</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons.</li></ul>"]
    pub fn ACameraMetadata_getAllTags(
        metadata: *const ACameraMetadata,
        numEntries: *mut i32,
        tags: *mut *const u32,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Create a copy of input {@link ACameraMetadata}.\n\n <p>The returned ACameraMetadata must be freed by the application by {@link ACameraMetadata_free}\n after application is done using it.</p>\n\n @param src the input {@link ACameraMetadata} to be copied.\n\n @return a valid ACameraMetadata pointer or NULL if the input metadata cannot be copied."]
    pub fn ACameraMetadata_copy(src: *const ACameraMetadata) -> *mut ACameraMetadata;
}
extern "C" {
    #[doc = " Free a {@link ACameraMetadata} structure.\n\n @param metadata the {@link ACameraMetadata} to be freed."]
    pub fn ACameraMetadata_free(metadata: *mut ACameraMetadata);
}
extern "C" {
    #[doc = " Helper function to check if a camera is logical multi-camera.\n\n <p> Check whether a camera device is a logical multi-camera based on its\n static metadata. If it is, also returns its physical sub camera Ids.</p>\n\n @param staticMetadata the static metadata of the camera being checked.\n @param numPhysicalCameras returns the number of physical cameras.\n @param physicalCameraIds returns the array of physical camera Ids backing this logical\n                          camera device. Note that this pointer is only valid\n                          during the lifetime of the staticMetadata object.\n\n @return true if this is a logical multi-camera, false otherwise."]
    pub fn ACameraMetadata_isLogicalMultiCamera(
        staticMetadata: *const ACameraMetadata,
        numPhysicalCameras: *mut usize,
        physicalCameraIds: *mut *const *const ::std::os::raw::c_char,
    ) -> bool;
}
extern "C" {
    #[doc = " Return a {@link ACameraMetadata} that references the same data as\n <a href=\"/reference/android/hardware/camera2/CameraMetadata\">\n     android.hardware.camera2.CameraMetadata</a> from Java API. (e.g., a\n <a href=\"/reference/android/hardware/camera2/CameraCharacteristics\">\n     android.hardware.camera2.CameraCharacteristics</a>\n or <a href=\"/reference/android/hardware/camera2/CaptureResult\">\n     android.hardware.camera2.CaptureResult</a>).\n\n <p>The returned ACameraMetadata must be freed by the application by {@link ACameraMetadata_free}\n after application is done using it.</p>\n\n <p>The ACameraMetadata maintains a reference count to the underlying data, so\n it can be used independently of the Java object, and it remains valid even if\n the Java metadata is garbage collected.\n\n @param env the JNI environment.\n @param cameraMetadata the source <a href=\"/reference/android/hardware/camera2/CameraMetadata\">\nandroid.hardware.camera2.CameraMetadata </a>from which the\n                       returned {@link ACameraMetadata} is a view.\n\n @return a valid ACameraMetadata pointer or NULL if cameraMetadata is null or not a valid\n         instance of <a href=\"android/hardware/camera2/CameraMetadata\">\n         android.hardware.camera2.CameraMetadata</a>.\n"]
    pub fn ACameraMetadata_fromCameraMetadata(
        env: *mut JNIEnv,
        cameraMetadata: jobject,
    ) -> *mut ACameraMetadata;
}
pub type ACameraWindowType = ANativeWindow;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraOutputTargets {
    _unused: [u8; 0],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraOutputTarget {
    _unused: [u8; 0],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACaptureRequest {
    _unused: [u8; 0],
}
extern "C" {
    #[doc = " Create a ACameraOutputTarget object.\n\n <p>The ACameraOutputTarget is used in {@link ACaptureRequest_addTarget} method to add an output\n {@link ANativeWindow} to ACaptureRequest. Use {@link ACameraOutputTarget_free} to free the object\n and its memory after application no longer needs the {@link ACameraOutputTarget}.</p>\n\n @param window the {@link ANativeWindow} to be associated with the {@link ACameraOutputTarget}\n @param output the output {@link ACameraOutputTarget} will be stored here if the\n                  method call succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds. The created ACameraOutputTarget will\n                                be filled in the output argument.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if window or output is NULL.</li></ul>\n\n @see ACaptureRequest_addTarget"]
    pub fn ACameraOutputTarget_create(
        window: *mut ACameraWindowType,
        output: *mut *mut ACameraOutputTarget,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Free a ACameraOutputTarget object.\n\n @param output the {@link ACameraOutputTarget} to be freed.\n\n @see ACameraOutputTarget_create"]
    pub fn ACameraOutputTarget_free(output: *mut ACameraOutputTarget);
}
extern "C" {
    #[doc = " Add an {@link ACameraOutputTarget} object to {@link ACaptureRequest}.\n\n @param request the {@link ACaptureRequest} of interest.\n @param output the output {@link ACameraOutputTarget} to be added to capture request.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request or output is NULL.</li></ul>"]
    pub fn ACaptureRequest_addTarget(
        request: *mut ACaptureRequest,
        output: *const ACameraOutputTarget,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Remove an {@link ACameraOutputTarget} object from {@link ACaptureRequest}.\n\n <p>This method has no effect if the ACameraOutputTarget does not exist in ACaptureRequest.</p>\n\n @param request the {@link ACaptureRequest} of interest.\n @param output the output {@link ACameraOutputTarget} to be removed from capture request.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request or output is NULL.</li></ul>"]
    pub fn ACaptureRequest_removeTarget(
        request: *mut ACaptureRequest,
        output: *const ACameraOutputTarget,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Get a metadata entry from input {@link ACaptureRequest}.\n\n <p>The memory of the data field in returned entry is managed by camera framework. Do not\n attempt to free it.</p>\n\n @param request the {@link ACaptureRequest} of interest.\n @param tag the tag value of the camera metadata entry to be get.\n @param entry the output {@link ACameraMetadata_const_entry} will be filled here if the method\n        call succeeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if metadata or entry is NULL.</li>\n         <li>{@link ACAMERA_ERROR_METADATA_NOT_FOUND} if the capture request does not contain an\n             entry of input tag value.</li></ul>"]
    pub fn ACaptureRequest_getConstEntry(
        request: *const ACaptureRequest,
        tag: u32,
        entry: *mut ACameraMetadata_const_entry,
    ) -> camera_status_t;
}
extern "C" {
    pub fn ACaptureRequest_getAllTags(
        request: *const ACaptureRequest,
        numTags: *mut i32,
        tags: *mut *const u32,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with unsigned 8 bits data type.\n\n <p>Set count to 0 and data to NULL to remove a tag from the capture request.</p>\n\n @param request the {@link ACaptureRequest} of interest.\n @param tag the tag value of the camera metadata entry to be set.\n @param count number of elements to be set in data argument\n @param data the entries to be set/change in the capture request.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request is NULL, count is larger than\n             zero while data is NULL, the data type of the tag is not unsigned 8 bits, or\n             the tag is not controllable by application.</li></ul>"]
    pub fn ACaptureRequest_setEntry_u8(
        request: *mut ACaptureRequest,
        tag: u32,
        count: u32,
        data: *const u8,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with signed 32 bits data type.\n\n <p>Set count to 0 and data to NULL to remove a tag from the capture request.</p>\n\n @param request the {@link ACaptureRequest} of interest.\n @param tag the tag value of the camera metadata entry to be set.\n @param count number of elements to be set in data argument\n @param data the entries to be set/change in the capture request.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request is NULL, count is larger than\n             zero while data is NULL, the data type of the tag is not signed 32 bits, or\n             the tag is not controllable by application.</li></ul>"]
    pub fn ACaptureRequest_setEntry_i32(
        request: *mut ACaptureRequest,
        tag: u32,
        count: u32,
        data: *const i32,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with float data type.\n\n <p>Set count to 0 and data to NULL to remove a tag from the capture request.</p>\n\n @param request the {@link ACaptureRequest} of interest.\n @param tag the tag value of the camera metadata entry to be set.\n @param count number of elements to be set in data argument\n @param data the entries to be set/change in the capture request.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request is NULL, count is larger than\n             zero while data is NULL, the data type of the tag is not float, or\n             the tag is not controllable by application.</li></ul>"]
    pub fn ACaptureRequest_setEntry_float(
        request: *mut ACaptureRequest,
        tag: u32,
        count: u32,
        data: *const f32,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with signed 64 bits data type.\n\n <p>Set count to 0 and data to NULL to remove a tag from the capture request.</p>\n\n @param request the {@link ACaptureRequest} of interest.\n @param tag the tag value of the camera metadata entry to be set.\n @param count number of elements to be set in data argument\n @param data the entries to be set/change in the capture request.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request is NULL, count is larger than\n             zero while data is NULL, the data type of the tag is not signed 64 bits, or\n             the tag is not controllable by application.</li></ul>"]
    pub fn ACaptureRequest_setEntry_i64(
        request: *mut ACaptureRequest,
        tag: u32,
        count: u32,
        data: *const i64,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with double data type.\n\n <p>Set count to 0 and data to NULL to remove a tag from the capture request.</p>\n\n @param request the {@link ACaptureRequest} of interest.\n @param tag the tag value of the camera metadata entry to be set.\n @param count number of elements to be set in data argument\n @param data the entries to be set/change in the capture request.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request is NULL, count is larger than\n             zero while data is NULL, the data type of the tag is not double, or\n             the tag is not controllable by application.</li></ul>"]
    pub fn ACaptureRequest_setEntry_double(
        request: *mut ACaptureRequest,
        tag: u32,
        count: u32,
        data: *const f64,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with rational data type.\n\n <p>Set count to 0 and data to NULL to remove a tag from the capture request.</p>\n\n @param request the {@link ACaptureRequest} of interest.\n @param tag the tag value of the camera metadata entry to be set.\n @param count number of elements to be set in data argument\n @param data the entries to be set/change in the capture request.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request is NULL, count is larger than\n             zero while data is NULL, the data type of the tag is not rational, or\n             the tag is not controllable by application.</li></ul>"]
    pub fn ACaptureRequest_setEntry_rational(
        request: *mut ACaptureRequest,
        tag: u32,
        count: u32,
        data: *const ACameraMetadata_rational,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Free a {@link ACaptureRequest} structure.\n\n @param request the {@link ACaptureRequest} to be freed."]
    pub fn ACaptureRequest_free(request: *mut ACaptureRequest);
}
extern "C" {
    #[doc = " Associate an arbitrary user context pointer to the {@link ACaptureRequest}\n\n This method is useful for user to identify the capture request in capture session callbacks.\n The context is NULL for newly created request.\n {@link ACameraOutputTarget_free} will not free the context. Also calling this method twice\n will not cause the previous context be freed.\n Also note that calling this method after the request has been sent to capture session will not\n change the context pointer in the capture callbacks.\n\n @param request the {@link ACaptureRequest} of interest.\n @param context the user context pointer to be associated with this capture request.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request is NULL.</li></ul>"]
    pub fn ACaptureRequest_setUserContext(
        request: *mut ACaptureRequest,
        context: *mut ::std::os::raw::c_void,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Get the user context pointer of the {@link ACaptureRequest}\n\n This method is useful for user to identify the capture request in capture session callbacks.\n The context is NULL for newly created request.\n\n @param request the {@link ACaptureRequest} of interest.\n @param context the user context pointer of this capture request.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request is NULL.</li></ul>"]
    pub fn ACaptureRequest_getUserContext(
        request: *const ACaptureRequest,
        context: *mut *mut ::std::os::raw::c_void,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Create a copy of input {@link ACaptureRequest}.\n\n <p>The returned ACaptureRequest must be freed by the application by {@link ACaptureRequest_free}\n after application is done using it.</p>\n\n @param src the input {@link ACaptureRequest} to be copied.\n\n @return a valid ACaptureRequest pointer or NULL if the input request cannot be copied."]
    pub fn ACaptureRequest_copy(src: *const ACaptureRequest) -> *mut ACaptureRequest;
}
extern "C" {
    #[doc = " Get a metadata entry from input {@link ACaptureRequest} for\n a physical camera backing a logical multi-camera device.\n\n <p>Same as ACaptureRequest_getConstEntry, except that if the key is contained\n in {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, this function\n returns the entry set by ACaptureRequest_setEntry_physicalCamera_* class of\n functions on the particular physical camera.</p>\n\n @param request the {@link ACaptureRequest} of interest created by\n                {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param physicalId one of the physical Ids used when request is created with\n                   {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param tag the capture request metadata tag in\n            {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}\n            that is set by ACaptureRequest_setEntry_physicalCamera_* class of functions.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if metadata, physicalId, or entry is NULL,\n         physicalId is not one of the Ids used in creating the request, or if the capture\n         request is a regular request with no physical Ids at all.</li>\n         <li>{@link ACAMERA_ERROR_METADATA_NOT_FOUND} if the capture request does not contain an\n             entry of input tag value.</li></ul>"]
    pub fn ACaptureRequest_getConstEntry_physicalCamera(
        request: *const ACaptureRequest,
        physicalId: *const ::std::os::raw::c_char,
        tag: u32,
        entry: *mut ACameraMetadata_const_entry,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with unsigned 8 bits data type for\n a physical camera backing a logical multi-camera device.\n\n <p>Same as ACaptureRequest_setEntry_u8, except that if tag is contained\n in {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, this function\n sets the entry for a particular physical sub-camera backing the logical multi-camera.\n If tag is not contained in\n {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, the key will be ignored\n by the camera device.</p>\n\n @param request the {@link ACaptureRequest} of interest created by\n                {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param physicalId one of the physical Ids used when request is created with\n                   {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param tag one of the capture request metadata tags in\n            {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request or physicalId is NULL, count is\n             larger than zero while data is NULL, the data type of the tag is not unsigned 8 bits,\n             the tag is not controllable by application, physicalId is not one of the Ids used\n             in creating the request, or if the capture request is a regular request with no\n             physical Ids at all.</li></ul>"]
    pub fn ACaptureRequest_setEntry_physicalCamera_u8(
        request: *mut ACaptureRequest,
        physicalId: *const ::std::os::raw::c_char,
        tag: u32,
        count: u32,
        data: *const u8,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with signed 32 bits data type for\n a physical camera of a logical multi-camera device.\n\n <p>Same as ACaptureRequest_setEntry_i32, except that if tag is contained\n in {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, this function\n sets the entry for a particular physical sub-camera backing the logical multi-camera.\n If tag is not contained in\n {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, the key will be ignored\n by the camera device.</p>\n\n @param request the {@link ACaptureRequest} of interest created by\n                {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param physicalId one of the physical Ids used when request is created with\n                   {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param tag one of the capture request metadata tags in\n            {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request or physicalId is NULL, count is\n             larger than zero while data is NULL, the data type of the tag is not signed 32 bits,\n             the tag is not controllable by application, physicalId is not one of the Ids used\n             in creating the request, or if the capture request is a regular request with no\n             physical Ids at all.</li></ul>"]
    pub fn ACaptureRequest_setEntry_physicalCamera_i32(
        request: *mut ACaptureRequest,
        physicalId: *const ::std::os::raw::c_char,
        tag: u32,
        count: u32,
        data: *const i32,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with float data type for\n a physical camera of a logical multi-camera device.\n\n <p>Same as ACaptureRequest_setEntry_float, except that if tag is contained\n in {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, this function\n sets the entry for a particular physical sub-camera backing the logical multi-camera.\n If tag is not contained in\n {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, the key will be ignored\n by the camera device.</p>\n\n @param request the {@link ACaptureRequest} of interest created by\n                {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param physicalId one of the physical Ids used when request is created with\n                   {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param tag one of the capture request metadata tags in\n            {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request or physicalId is NULL, count is\n             larger than zero while data is NULL, the data type of the tag is not float,\n             the tag is not controllable by application, physicalId is not one of the Ids used\n             in creating the request, or if the capture request is a regular request with no\n             physical Ids at all.</li></ul>"]
    pub fn ACaptureRequest_setEntry_physicalCamera_float(
        request: *mut ACaptureRequest,
        physicalId: *const ::std::os::raw::c_char,
        tag: u32,
        count: u32,
        data: *const f32,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with signed 64 bits data type for\n a physical camera of a logical multi-camera device.\n\n <p>Same as ACaptureRequest_setEntry_i64, except that if tag is contained\n in {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, this function\n sets the entry for a particular physical sub-camera backing the logical multi-camera.\n If tag is not contained in\n {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, the key will be ignored\n by the camera device.</p>\n\n @param request the {@link ACaptureRequest} of interest created by\n                {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param physicalId one of the physical Ids used when request is created with\n                   {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param tag one of the capture request metadata tags in\n            {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request or physicalId is NULL, count is\n             larger than zero while data is NULL, the data type of the tag is not signed 64 bits,\n             the tag is not controllable by application, physicalId is not one of the Ids used\n             in creating the request, or if the capture request is a regular request with no\n             physical Ids at all.</li></ul>"]
    pub fn ACaptureRequest_setEntry_physicalCamera_i64(
        request: *mut ACaptureRequest,
        physicalId: *const ::std::os::raw::c_char,
        tag: u32,
        count: u32,
        data: *const i64,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with double data type for\n a physical camera of a logical multi-camera device.\n\n <p>Same as ACaptureRequest_setEntry_double, except that if tag is contained\n in {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, this function\n sets the entry for a particular physical sub-camera backing the logical multi-camera.\n If tag is not contained in\n {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, the key will be ignored\n by the camera device.</p>\n\n @param request the {@link ACaptureRequest} of interest created by\n                {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param physicalId one of the physical Ids used when request is created with\n                   {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param tag one of the capture request metadata tags in\n            {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request or physicalId is NULL, count is\n             larger than zero while data is NULL, the data type of the tag is not double,\n             the tag is not controllable by application, physicalId is not one of the Ids used\n             in creating the request, or if the capture request is a regular request with no\n             physical Ids at all.</li></ul>"]
    pub fn ACaptureRequest_setEntry_physicalCamera_double(
        request: *mut ACaptureRequest,
        physicalId: *const ::std::os::raw::c_char,
        tag: u32,
        count: u32,
        data: *const f64,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Set/change a camera capture control entry with rational data type for\n a physical camera of a logical multi-camera device.\n\n <p>Same as ACaptureRequest_setEntry_rational, except that if tag is contained\n in {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, this function\n sets the entry for a particular physical sub-camera backing the logical multi-camera.\n If tag is not contained in\n {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}, the key will be ignored\n by the camera device.</p>\n\n @param request the {@link ACaptureRequest} of interest created by\n                {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param physicalId one of the physical Ids used when request is created with\n                   {@link ACameraDevice_createCaptureRequest_withPhysicalIds}.\n @param tag one of the capture request metadata tags in\n            {@link ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS}.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if request or physicalId is NULL, count is\n             larger than zero while data is NULL, the data type of the tag is not rational,\n             the tag is not controllable by application, physicalId is not one of the Ids used\n             in creating the request, or if the capture request is a regular request with no\n             physical Ids at all.</li></ul>"]
    pub fn ACaptureRequest_setEntry_physicalCamera_rational(
        request: *mut ACaptureRequest,
        physicalId: *const ::std::os::raw::c_char,
        tag: u32,
        count: u32,
        data: *const ACameraMetadata_rational,
    ) -> camera_status_t;
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraCaptureSession {
    _unused: [u8; 0],
}
#[doc = " The definition of camera capture session state callback.\n\n @param context The optional application context provided by user in\n                {@link ACameraCaptureSession_stateCallbacks}.\n @param session The camera capture session whose state is changing."]
pub type ACameraCaptureSession_stateCallback = ::std::option::Option<
    unsafe extern "C" fn(context: *mut ::std::os::raw::c_void, session: *mut ACameraCaptureSession),
>;
#[doc = " Capture session state callbacks used in {@link ACameraDevice_createCaptureSession} and\n {@link ACameraDevice_createCaptureSessionWithSessionParameters}"]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraCaptureSession_stateCallbacks {
    #[doc = " optional application context."]
    pub context: *mut ::std::os::raw::c_void,
    #[doc = " This callback is called when the session is closed and deleted from memory.\n\n <p>A session is closed when {@link ACameraCaptureSession_close} is called, a new session\n is created by the parent camera device,\n or when the parent camera device is closed (either by the user closing the device,\n or due to a camera device disconnection or fatal error).</p>\n\n <p>Once this callback is called, all access to this ACameraCaptureSession object will cause\n a crash.</p>"]
    pub onClosed: ACameraCaptureSession_stateCallback,
    #[doc = " This callback is called every time the session has no more capture requests to process.\n\n <p>This callback will be invoked any time the session finishes processing\n all of its active capture requests, and no repeating request or burst is set up.</p>"]
    pub onReady: ACameraCaptureSession_stateCallback,
    #[doc = " This callback is called when the session starts actively processing capture requests.\n\n <p>If the session runs out of capture requests to process and calls {@link onReady},\n then this callback will be invoked again once new requests are submitted for capture.</p>"]
    pub onActive: ACameraCaptureSession_stateCallback,
}
#[test]
fn bindgen_test_layout_ACameraCaptureSession_stateCallbacks() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraCaptureSession_stateCallbacks> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraCaptureSession_stateCallbacks>(),
        32usize,
        concat!(
            "Size of: ",
            stringify!(ACameraCaptureSession_stateCallbacks)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraCaptureSession_stateCallbacks>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(ACameraCaptureSession_stateCallbacks)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).context) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_stateCallbacks),
            "::",
            stringify!(context)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onClosed) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_stateCallbacks),
            "::",
            stringify!(onClosed)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onReady) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_stateCallbacks),
            "::",
            stringify!(onReady)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onActive) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_stateCallbacks),
            "::",
            stringify!(onActive)
        )
    );
}
#[doc = " The capture session has dropped this frame due to an\n {@link ACameraCaptureSession_abortCaptures} call."]
pub const CAPTURE_FAILURE_REASON_FLUSHED: _bindgen_ty_24 = 0;
#[doc = " The capture session has dropped this frame due to an error in the framework."]
pub const CAPTURE_FAILURE_REASON_ERROR: _bindgen_ty_24 = 1;
#[doc = " Enum for describing error reason in {@link ACameraCaptureFailure}"]
pub type _bindgen_ty_24 = ::std::os::raw::c_uint;
#[doc = " Struct to describe a capture failure"]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraCaptureFailure {
    #[doc = " The frame number associated with this failed capture.\n\n <p>Whenever a request has been processed, regardless of failed capture or success,\n it gets a unique frame number assigned to its future result/failed capture.</p>\n\n <p>This value monotonically increments, starting with 0,\n for every new result or failure; and the scope is the lifetime of the\n {@link ACameraDevice}.</p>"]
    pub frameNumber: i64,
    #[doc = " Determine why the request was dropped, whether due to an error or to a user\n action.\n\n @see CAPTURE_FAILURE_REASON_ERROR\n @see CAPTURE_FAILURE_REASON_FLUSHED"]
    pub reason: ::std::os::raw::c_int,
    #[doc = " The sequence ID for this failed capture that was returned by the\n {@link ACameraCaptureSession_capture} or {@link ACameraCaptureSession_setRepeatingRequest}.\n\n <p>The sequence ID is a unique monotonically increasing value starting from 0,\n incremented every time a new group of requests is submitted to the ACameraDevice.</p>"]
    pub sequenceId: ::std::os::raw::c_int,
    #[doc = " Determine if the image was captured from the camera.\n\n <p>If the image was not captured, no image buffers will be available.\n If the image was captured, then image buffers may be available.</p>\n"]
    pub wasImageCaptured: bool,
}
#[test]
fn bindgen_test_layout_ACameraCaptureFailure() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraCaptureFailure> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraCaptureFailure>(),
        24usize,
        concat!("Size of: ", stringify!(ACameraCaptureFailure))
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraCaptureFailure>(),
        8usize,
        concat!("Alignment of ", stringify!(ACameraCaptureFailure))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).frameNumber) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureFailure),
            "::",
            stringify!(frameNumber)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reason) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureFailure),
            "::",
            stringify!(reason)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).sequenceId) as usize - ptr as usize },
        12usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureFailure),
            "::",
            stringify!(sequenceId)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).wasImageCaptured) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureFailure),
            "::",
            stringify!(wasImageCaptured)
        )
    );
}
#[doc = " The definition of camera capture start callback.\n\n @param context The optional application context provided by user in\n                {@link ACameraCaptureSession_captureCallbacks}.\n @param session The camera capture session of interest.\n @param request The capture request that is starting. Note that this pointer points to a copy of\n                capture request sent by application, so the address is different to what\n                application sent but the content will match. This request will be freed by\n                framework immediately after this callback returns.\n @param timestamp The timestamp when the capture is started. This timestmap will match\n                  {@link ACAMERA_SENSOR_TIMESTAMP} of the {@link ACameraMetadata} in\n                  {@link ACameraCaptureSession_captureCallbacks#onCaptureCompleted} callback."]
pub type ACameraCaptureSession_captureCallback_start = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        session: *mut ACameraCaptureSession,
        request: *const ACaptureRequest,
        timestamp: i64,
    ),
>;
#[doc = " The definition of camera capture progress/result callback.\n\n @param context The optional application context provided by user in\n                {@link ACameraCaptureSession_captureCallbacks}.\n @param session The camera capture session of interest.\n @param request The capture request of interest. Note that this pointer points to a copy of\n                capture request sent by application, so the address is different to what\n                application sent but the content will match. This request will be freed by\n                framework immediately after this callback returns.\n @param result The capture result metadata reported by camera device. The memory is managed by\n                camera framework. Do not access this pointer after this callback returns."]
pub type ACameraCaptureSession_captureCallback_result = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        session: *mut ACameraCaptureSession,
        request: *mut ACaptureRequest,
        result: *const ACameraMetadata,
    ),
>;
#[doc = " The definition of camera capture failure callback.\n\n @param context The optional application context provided by user in\n                {@link ACameraCaptureSession_captureCallbacks}.\n @param session The camera capture session of interest.\n @param request The capture request of interest. Note that this pointer points to a copy of\n                capture request sent by application, so the address is different to what\n                application sent but the content will match. This request will be freed by\n                framework immediately after this callback returns.\n @param failure The {@link ACameraCaptureFailure} desribes the capture failure. The memory is\n                managed by camera framework. Do not access this pointer after this callback\n                returns."]
pub type ACameraCaptureSession_captureCallback_failed = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        session: *mut ACameraCaptureSession,
        request: *mut ACaptureRequest,
        failure: *mut ACameraCaptureFailure,
    ),
>;
#[doc = " The definition of camera sequence end callback.\n\n @param context The optional application context provided by user in\n                {@link ACameraCaptureSession_captureCallbacks}.\n @param session The camera capture session of interest.\n @param sequenceId The capture sequence ID of the finished sequence.\n @param frameNumber The frame number of the last frame of this sequence."]
pub type ACameraCaptureSession_captureCallback_sequenceEnd = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        session: *mut ACameraCaptureSession,
        sequenceId: ::std::os::raw::c_int,
        frameNumber: i64,
    ),
>;
#[doc = " The definition of camera sequence aborted callback.\n\n @param context The optional application context provided by user in\n                {@link ACameraCaptureSession_captureCallbacks}.\n @param session The camera capture session of interest.\n @param sequenceId The capture sequence ID of the aborted sequence."]
pub type ACameraCaptureSession_captureCallback_sequenceAbort = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        session: *mut ACameraCaptureSession,
        sequenceId: ::std::os::raw::c_int,
    ),
>;
#[doc = " The definition of camera buffer lost callback.\n\n @param context The optional application context provided by user in\n                {@link ACameraCaptureSession_captureCallbacks}.\n @param session The camera capture session of interest.\n @param request The capture request of interest. Note that this pointer points to a copy of\n                capture request sent by application, so the address is different to what\n                application sent but the content will match. This request will be freed by\n                framework immediately after this callback returns.\n @param window The {@link ANativeWindow} that the lost buffer would have been sent to.\n @param frameNumber The frame number of the lost buffer."]
pub type ACameraCaptureSession_captureCallback_bufferLost = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        session: *mut ACameraCaptureSession,
        request: *mut ACaptureRequest,
        window: *mut ACameraWindowType,
        frameNumber: i64,
    ),
>;
#[doc = " ACaptureCaptureSession_captureCallbacks structure used in\n {@link ACameraCaptureSession_capture} and {@link ACameraCaptureSession_setRepeatingRequest}."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraCaptureSession_captureCallbacks {
    #[doc = " optional application context."]
    pub context: *mut ::std::os::raw::c_void,
    #[doc = " This callback is called when the camera device has started capturing\n the output image for the request, at the beginning of image exposure.\n\n <p>This callback is invoked right as\n the capture of a frame begins, so it is the most appropriate time\n for playing a shutter sound, or triggering UI indicators of capture.</p>\n\n <p>The request that is being used for this capture is provided, along\n with the actual timestamp for the start of exposure.\n This timestamp matches the timestamps that will be\n included in {@link ACAMERA_SENSOR_TIMESTAMP} of the {@link ACameraMetadata} in\n {@link onCaptureCompleted} callback,\n and in the buffers sent to each output ANativeWindow. These buffer\n timestamps are accessible through, for example,\n {@link AImage_getTimestamp} or\n <a href=\"http://developer.android.com/reference/android/graphics/SurfaceTexture.html#getTimestamp()\">\n android.graphics.SurfaceTexture#getTimestamp()</a>.</p>\n\n <p>Note that the ACaptureRequest pointer in the callback will not match what application has\n submitted, but the contents the ACaptureRequest will match what application submitted.</p>\n"]
    pub onCaptureStarted: ACameraCaptureSession_captureCallback_start,
    #[doc = " This callback is called when an image capture makes partial forward progress; some\n (but not all) results from an image capture are available.\n\n <p>The result provided here will contain some subset of the fields of\n a full result. Multiple {@link onCaptureProgressed} calls may happen per\n capture; a given result field will only be present in one partial\n capture at most. The final {@link onCaptureCompleted} call will always\n contain all the fields (in particular, the union of all the fields of all\n the partial results composing the total result).</p>\n\n <p>For each request, some result data might be available earlier than others. The typical\n delay between each partial result (per request) is a single frame interval.\n For performance-oriented use-cases, applications should query the metadata they need\n to make forward progress from the partial results and avoid waiting for the completed\n result.</p>\n\n <p>For a particular request, {@link onCaptureProgressed} may happen before or after\n {@link onCaptureStarted}.</p>\n\n <p>Each request will generate at least `1` partial results, and at most\n {@link ACAMERA_REQUEST_PARTIAL_RESULT_COUNT} partial results.</p>\n\n <p>Depending on the request settings, the number of partial results per request\n will vary, although typically the partial count could be the same as long as the\n camera device subsystems enabled stay the same.</p>\n\n <p>Note that the ACaptureRequest pointer in the callback will not match what application has\n submitted, but the contents the ACaptureRequest will match what application submitted.</p>"]
    pub onCaptureProgressed: ACameraCaptureSession_captureCallback_result,
    #[doc = " This callback is called when an image capture has fully completed and all the\n result metadata is available.\n\n <p>This callback will always fire after the last {@link onCaptureProgressed};\n in other words, no more partial results will be delivered once the completed result\n is available.</p>\n\n <p>For performance-intensive use-cases where latency is a factor, consider\n using {@link onCaptureProgressed} instead.</p>\n\n <p>Note that the ACaptureRequest pointer in the callback will not match what application has\n submitted, but the contents the ACaptureRequest will match what application submitted.</p>"]
    pub onCaptureCompleted: ACameraCaptureSession_captureCallback_result,
    #[doc = " This callback is called instead of {@link onCaptureCompleted} when the\n camera device failed to produce a capture result for the\n request.\n\n <p>Other requests are unaffected, and some or all image buffers from\n the capture may have been pushed to their respective output\n streams.</p>\n\n <p>Note that the ACaptureRequest pointer in the callback will not match what application has\n submitted, but the contents the ACaptureRequest will match what application submitted.</p>\n\n @see ACameraCaptureFailure"]
    pub onCaptureFailed: ACameraCaptureSession_captureCallback_failed,
    #[doc = " This callback is called independently of the others in {@link ACameraCaptureSession_captureCallbacks},\n when a capture sequence finishes and all capture result\n or capture failure for it have been returned via this {@link ACameraCaptureSession_captureCallbacks}.\n\n <p>In total, there will be at least one result/failure returned by this listener\n before this callback is invoked. If the capture sequence is aborted before any\n requests have been processed, {@link onCaptureSequenceAborted} is invoked instead.</p>"]
    pub onCaptureSequenceCompleted: ACameraCaptureSession_captureCallback_sequenceEnd,
    #[doc = " This callback is called independently of the others in {@link ACameraCaptureSession_captureCallbacks},\n when a capture sequence aborts before any capture result\n or capture failure for it have been returned via this {@link ACameraCaptureSession_captureCallbacks}.\n\n <p>Due to the asynchronous nature of the camera device, not all submitted captures\n are immediately processed. It is possible to clear out the pending requests\n by a variety of operations such as {@link ACameraCaptureSession_stopRepeating} or\n {@link ACameraCaptureSession_abortCaptures}. When such an event happens,\n {@link onCaptureSequenceCompleted} will not be called.</p>"]
    pub onCaptureSequenceAborted: ACameraCaptureSession_captureCallback_sequenceAbort,
    #[doc = " This callback is called if a single buffer for a capture could not be sent to its\n destination ANativeWindow.\n\n <p>If the whole capture failed, then {@link onCaptureFailed} will be called instead. If\n some but not all buffers were captured but the result metadata will not be available,\n then onCaptureFailed will be invoked with {@link ACameraCaptureFailure#wasImageCaptured}\n returning true, along with one or more calls to {@link onCaptureBufferLost} for the\n failed outputs.</p>\n\n <p>Note that the ACaptureRequest pointer in the callback will not match what application has\n submitted, but the contents the ACaptureRequest will match what application submitted.\n The ANativeWindow pointer will always match what application submitted in\n {@link ACameraDevice_createCaptureSession}</p>\n"]
    pub onCaptureBufferLost: ACameraCaptureSession_captureCallback_bufferLost,
}
#[test]
fn bindgen_test_layout_ACameraCaptureSession_captureCallbacks() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraCaptureSession_captureCallbacks> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraCaptureSession_captureCallbacks>(),
        64usize,
        concat!(
            "Size of: ",
            stringify!(ACameraCaptureSession_captureCallbacks)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraCaptureSession_captureCallbacks>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(ACameraCaptureSession_captureCallbacks)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).context) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacks),
            "::",
            stringify!(context)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureStarted) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacks),
            "::",
            stringify!(onCaptureStarted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureProgressed) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacks),
            "::",
            stringify!(onCaptureProgressed)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureCompleted) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacks),
            "::",
            stringify!(onCaptureCompleted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureFailed) as usize - ptr as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacks),
            "::",
            stringify!(onCaptureFailed)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureSequenceCompleted) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacks),
            "::",
            stringify!(onCaptureSequenceCompleted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureSequenceAborted) as usize - ptr as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacks),
            "::",
            stringify!(onCaptureSequenceAborted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureBufferLost) as usize - ptr as usize },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacks),
            "::",
            stringify!(onCaptureBufferLost)
        )
    );
}
pub const CAPTURE_SEQUENCE_ID_NONE: _bindgen_ty_25 = -1;
pub type _bindgen_ty_25 = ::std::os::raw::c_int;
extern "C" {
    #[doc = " Close this capture session.\n\n <p>Closing a session frees up the target output Surfaces of the session for reuse with either\n a new session, or to other APIs that can draw to Surfaces.</p>\n\n <p>Note that creating a new capture session with {@link ACameraDevice_createCaptureSession}\n will close any existing capture session automatically, and call the older session listener's\n {@link ACameraCaptureSession_stateCallbacks#onClosed} callback. Using\n {@link ACameraDevice_createCaptureSession} directly without closing is the recommended approach\n for quickly switching to a new session, since unchanged target outputs can be reused more\n efficiently.</p>\n\n <p>After a session is closed and before {@link ACameraCaptureSession_stateCallbacks#onClosed}\n is called, all methods invoked on the session will return {@link ACAMERA_ERROR_SESSION_CLOSED},\n and any repeating requests are stopped (as if {@link ACameraCaptureSession_stopRepeating} was\n called). However, any in-progress capture requests submitted to the session will be completed as\n normal; once all captures have completed and the session has been torn down,\n {@link ACameraCaptureSession_stateCallbacks#onClosed} callback will be called and the seesion\n will be removed from memory.</p>\n\n <p>Closing a session is idempotent; closing more than once has no effect.</p>\n\n @param session the capture session of interest"]
    pub fn ACameraCaptureSession_close(session: *mut ACameraCaptureSession);
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraDevice {
    _unused: [u8; 0],
}
extern "C" {
    #[doc = " Get the ACameraDevice pointer associated with this capture session in the device argument\n if the method succeeds.\n\n @param session the capture session of interest\n @param device the {@link ACameraDevice} associated with session. Will be set to NULL\n        if the session is closed or this method fails.\n @return <ul><li>\n             {@link ACAMERA_OK} if the method call succeeds. The {@link ACameraDevice}\n                                will be stored in device argument</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if session or device is NULL</li>\n         <li>{@link ACAMERA_ERROR_SESSION_CLOSED} if the capture session has been closed</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons</li></ul>\n"]
    pub fn ACameraCaptureSession_getDevice(
        session: *mut ACameraCaptureSession,
        device: *mut *mut ACameraDevice,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Submit an array of requests to be captured in sequence as a burst in the minimum of time possible.\n\n <p>The burst will be captured in the minimum amount of time possible, and will not be\n interleaved with requests submitted by other capture or repeat calls.</p>\n\n <p>Each capture produces one {@link ACameraMetadata} as a capture result and image buffers for\n one or more target {@link ANativeWindow}s. The target ANativeWindows (set with\n {@link ACaptureRequest_addTarget}) must be a subset of the ANativeWindow provided when\n this capture session was created.</p>\n\n @param session the capture session of interest\n @param callbacks the {@link ACameraCaptureSession_captureCallbacks} to be associated this capture\n        sequence. No capture callback will be fired if this is set to NULL.\n @param numRequests number of requests in requests argument. Must be at least 1.\n @param requests an array of {@link ACaptureRequest} to be captured. Length must be at least\n        numRequests.\n @param captureSequenceId the capture sequence ID associated with this capture method invocation\n        will be stored here if this argument is not NULL and the method call succeeds.\n        When this argument is set to NULL, the capture sequence ID will not be returned.\n\n @return <ul><li>\n             {@link ACAMERA_OK} if the method succeeds. captureSequenceId will be filled\n             if it is not NULL.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if session or requests is NULL, or\n             if numRequests < 1</li>\n         <li>{@link ACAMERA_ERROR_SESSION_CLOSED} if the capture session has been closed</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if the camera device is closed</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DEVICE} if the camera device encounters fatal error</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_SERVICE} if the camera service encounters fatal error</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons</li></ul>"]
    pub fn ACameraCaptureSession_capture(
        session: *mut ACameraCaptureSession,
        callbacks: *mut ACameraCaptureSession_captureCallbacks,
        numRequests: ::std::os::raw::c_int,
        requests: *mut *mut ACaptureRequest,
        captureSequenceId: *mut ::std::os::raw::c_int,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Request endlessly repeating capture of a sequence of images by this capture session.\n\n <p>With this method, the camera device will continually capture images,\n cycling through the settings in the provided list of\n {@link ACaptureRequest}, at the maximum rate possible.</p>\n\n <p>If a request is submitted through {@link ACameraCaptureSession_capture},\n the current repetition of the request list will be\n completed before the higher-priority request is handled. This guarantees\n that the application always receives a complete repeat burst captured in\n minimal time, instead of bursts interleaved with higher-priority\n captures, or incomplete captures.</p>\n\n <p>Repeating burst requests are a simple way for an application to\n maintain a preview or other continuous stream of frames where each\n request is different in a predicatable way, without having to continually\n submit requests through {@link ACameraCaptureSession_capture}.</p>\n\n <p>To stop the repeating capture, call {@link ACameraCaptureSession_stopRepeating}. Any\n ongoing burst will still be completed, however. Calling\n {@link ACameraCaptureSession_abortCaptures} will also clear the request.</p>\n\n <p>Calling this method will replace a previously-set repeating requests\n set up by this method, although any in-progress burst will be completed before the new repeat\n burst will be used.</p>\n\n @param session the capture session of interest\n @param callbacks the {@link ACameraCaptureSession_captureCallbacks} to be associated with this\n        capture sequence. No capture callback will be fired if callbacks is set to NULL.\n @param numRequests number of requests in requests array. Must be at least 1.\n @param requests an array of {@link ACaptureRequest} to be captured. Length must be at least\n        numRequests.\n @param captureSequenceId the capture sequence ID associated with this capture method invocation\n        will be stored here if this argument is not NULL and the method call succeeds.\n        When this argument is set to NULL, the capture sequence ID will not be returned.\n\n @return <ul><li>\n             {@link ACAMERA_OK} if the method succeeds. captureSequenceId will be filled\n             if it is not NULL.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if session or requests is NULL, or\n             if numRequests < 1</li>\n         <li>{@link ACAMERA_ERROR_SESSION_CLOSED} if the capture session has been closed</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if the camera device is closed</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DEVICE} if the camera device encounters fatal error</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_SERVICE} if the camera service encounters fatal error</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for  some other reasons</li></ul>"]
    pub fn ACameraCaptureSession_setRepeatingRequest(
        session: *mut ACameraCaptureSession,
        callbacks: *mut ACameraCaptureSession_captureCallbacks,
        numRequests: ::std::os::raw::c_int,
        requests: *mut *mut ACaptureRequest,
        captureSequenceId: *mut ::std::os::raw::c_int,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Cancel any ongoing repeating capture set by {@link ACameraCaptureSession_setRepeatingRequest}.\n Has no effect on requests submitted through {@link ACameraCaptureSession_capture}.\n\n <p>Any currently in-flight captures will still complete, as will any burst that is\n mid-capture. To ensure that the device has finished processing all of its capture requests\n and is in ready state, wait for the {@link ACameraCaptureSession_stateCallbacks#onReady} callback\n after calling this method.</p>\n\n @param session the capture session of interest\n\n @return <ul><li>\n             {@link ACAMERA_OK} if the method succeeds. captureSequenceId will be filled\n             if it is not NULL.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if session is NULL.</li>\n         <li>{@link ACAMERA_ERROR_SESSION_CLOSED} if the capture session has been closed</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if the camera device is closed</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DEVICE} if the camera device encounters fatal error</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_SERVICE} if the camera service encounters fatal error</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons</li></ul>"]
    pub fn ACameraCaptureSession_stopRepeating(
        session: *mut ACameraCaptureSession,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Discard all captures currently pending and in-progress as fast as possible.\n\n <p>The camera device will discard all of its current work as fast as possible. Some in-flight\n captures may complete successfully and call\n {@link ACameraCaptureSession_captureCallbacks#onCaptureCompleted},\n while others will trigger their {@link ACameraCaptureSession_captureCallbacks#onCaptureFailed}\n callbacks. If a repeating request list is set, it will be cleared.</p>\n\n <p>This method is the fastest way to switch the camera device to a new session with\n {@link ACameraDevice_createCaptureSession}, at the cost of discarding in-progress\n work. It must be called before the new session is created. Once all pending requests are\n either completed or thrown away, the {@link ACameraCaptureSession_stateCallbacks#onReady}\n callback will be called, if the session has not been closed. Otherwise, the\n {@link ACameraCaptureSession_stateCallbacks#onClosed}\n callback will be fired when a new session is created by the camera device and the previous\n session is being removed from memory.</p>\n\n <p>Cancelling will introduce at least a brief pause in the stream of data from the camera\n device, since once the camera device is emptied, the first new request has to make it through\n the entire camera pipeline before new output buffers are produced.</p>\n\n <p>This means that using ACameraCaptureSession_abortCaptures to simply remove pending requests is\n not recommended; it's best used for quickly switching output configurations, or for cancelling\n long in-progress requests (such as a multi-second capture).</p>\n\n @param session the capture session of interest\n\n @return <ul><li>\n             {@link ACAMERA_OK} if the method succeeds. captureSequenceId will be filled\n             if it is not NULL.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if session is NULL.</li>\n         <li>{@link ACAMERA_ERROR_SESSION_CLOSED} if the capture session has been closed</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if the camera device is closed</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DEVICE} if the camera device encounters fatal error</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_SERVICE} if the camera service encounters fatal error</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons</li></ul>"]
    pub fn ACameraCaptureSession_abortCaptures(
        session: *mut ACameraCaptureSession,
    ) -> camera_status_t;
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACaptureSessionOutput {
    _unused: [u8; 0],
}
extern "C" {
    #[doc = " Update shared ACaptureSessionOutput.\n\n <p>A shared ACaptureSessionOutput (see {@link ACaptureSessionSharedOutput_create}) that\n was modified via calls to {@link ACaptureSessionSharedOutput_add} or\n {@link ACaptureSessionSharedOutput_remove} must be updated by calling this method before its\n changes take effect. After the update call returns  with {@link ACAMERA_OK}, any newly added\n native windows can be used as a target in subsequent capture requests.</p>\n\n <p>Native windows that get removed must not be part of any active repeating or single/burst\n request or have any pending results. Consider updating repeating requests via\n {@link ACameraCaptureSession_setRepeatingRequest} and then wait for the last frame number\n when the sequence completes\n {@link ACameraCaptureSession_captureCallbacks#onCaptureSequenceCompleted}.</p>\n\n <p>Native windows that get added must not be part of any other registered ACaptureSessionOutput\n and must be compatible. Compatible windows must have matching format, rotation and\n consumer usage.</p>\n\n <p>A shared ACameraCaptureSession can support up to 4 additional native windows.</p>\n\n @param session the capture session of interest\n @param output the modified output configuration\n\n @return <ul><li>\n             {@link ACAMERA_OK} if the method succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if session or output is NULL; or output\n             contains invalid native windows; or if an attempt was made to add\n             a native window to a different output configuration; or new native window is not\n             compatible; or any removed native window still has pending requests;</li>\n         <li>{@link ACAMERA_ERROR_INVALID_OPERATION} if output configuration is not shared (see\n             {@link ACaptureSessionSharedOutput_create};  or the number of additional\n             native windows goes beyond the supported limit.</li>\n         <li>{@link ACAMERA_ERROR_SESSION_CLOSED} if the capture session has been closed</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if the camera device is closed</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DEVICE} if the camera device encounters fatal error</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_SERVICE} if the camera service encounters fatal\n             error</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons</li></ul>"]
    pub fn ACameraCaptureSession_updateSharedOutput(
        session: *mut ACameraCaptureSession,
        output: *mut ACaptureSessionOutput,
    ) -> camera_status_t;
}
#[doc = " The definition of final capture result callback with logical multi-camera support.\n\n This has the same functionality as final ACameraCaptureSession_captureCallback_result, with\n added ability to return physical camera result metadata within a logical multi-camera.\n\n For a logical multi-camera, this function will be called with the Id and result metadata\n of the underlying physical cameras, which the corresponding capture request contains targets for.\n If the capture request doesn't contain targets specific to any physical camera, or the current\n camera device isn't a logical multi-camera, physicalResultCount will be 0.\n\n @param context The optional application context provided by user in\n                {@link ACameraCaptureSession_captureCallbacks}.\n @param session The camera capture session of interest.\n @param request The capture request of interest. Note that this pointer points to a copy of\n                capture request sent by application, so the address is different to what\n                application sent but the content will match. This request will be freed by\n                framework immediately after this callback returns.\n @param result The capture result metadata reported by camera device. The memory is managed by\n                camera framework. Do not access this pointer after this callback returns.\n @param physicalResultCount The number of physical camera result metadata\n @param physicalCameraIds The array of physical camera IDs on which the\n                physical result metadata are reported.\n @param physicalResults The array of capture result metadata reported by the\n                physical camera devices."]
pub type ACameraCaptureSession_logicalCamera_captureCallback_result = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        session: *mut ACameraCaptureSession,
        request: *mut ACaptureRequest,
        result: *const ACameraMetadata,
        physicalResultCount: usize,
        physicalCameraIds: *mut *const ::std::os::raw::c_char,
        physicalResults: *mut *const ACameraMetadata,
    ),
>;
#[doc = " Struct to describe a logical camera capture failure"]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ALogicalCameraCaptureFailure {
    #[doc = " The {@link ACameraCaptureFailure} contains information about regular logical device capture\n failure."]
    pub captureFailure: ACameraCaptureFailure,
    #[doc = " The physical camera device ID in case the capture failure comes from a capture request\n with configured physical camera streams for a logical camera. physicalCameraId will be set\n to NULL in case the capture request has no associated physical camera device.\n"]
    pub physicalCameraId: *const ::std::os::raw::c_char,
}
#[test]
fn bindgen_test_layout_ALogicalCameraCaptureFailure() {
    const UNINIT: ::std::mem::MaybeUninit<ALogicalCameraCaptureFailure> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ALogicalCameraCaptureFailure>(),
        32usize,
        concat!("Size of: ", stringify!(ALogicalCameraCaptureFailure))
    );
    assert_eq!(
        ::std::mem::align_of::<ALogicalCameraCaptureFailure>(),
        8usize,
        concat!("Alignment of ", stringify!(ALogicalCameraCaptureFailure))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).captureFailure) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ALogicalCameraCaptureFailure),
            "::",
            stringify!(captureFailure)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).physicalCameraId) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ALogicalCameraCaptureFailure),
            "::",
            stringify!(physicalCameraId)
        )
    );
}
#[doc = " The definition of logical camera capture failure callback.\n\n @param context The optional application context provided by user in\n                {@link ACameraCaptureSession_captureCallbacks}.\n @param session The camera capture session of interest.\n @param request The capture request of interest. Note that this pointer points to a copy of\n                capture request sent by application, so the address is different to what\n                application sent but the content will match. This request will be freed by\n                framework immediately after this callback returns.\n @param failure The {@link ALogicalCameraCaptureFailure} desribes the capture failure. The memory\n                is managed by camera framework. Do not access this pointer after this callback\n                returns."]
pub type ACameraCaptureSession_logicalCamera_captureCallback_failed = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        session: *mut ACameraCaptureSession,
        request: *mut ACaptureRequest,
        failure: *mut ALogicalCameraCaptureFailure,
    ),
>;
#[doc = " This has the same functionality as ACameraCaptureSession_captureCallbacks,\n with the exception that an onLogicalCameraCaptureCompleted callback is\n used, instead of onCaptureCompleted, to support logical multi-camera."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraCaptureSession_logicalCamera_captureCallbacks {
    #[doc = " Same as ACameraCaptureSession_captureCallbacks"]
    pub context: *mut ::std::os::raw::c_void,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureStarted}."]
    pub onCaptureStarted: ACameraCaptureSession_captureCallback_start,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureProgressed}."]
    pub onCaptureProgressed: ACameraCaptureSession_captureCallback_result,
    #[doc = " This callback is called when an image capture has fully completed and all the\n result metadata is available. For a logical multi-camera, this callback\n also returns the result metadata for all physical cameras being\n explicitly requested on.\n\n <p>This callback will always fire after the last {@link onCaptureProgressed};\n in other words, no more partial results will be delivered once the completed result\n is available.</p>\n\n <p>For performance-intensive use-cases where latency is a factor, consider\n using {@link onCaptureProgressed} instead.</p>\n\n <p>Note that the ACaptureRequest pointer in the callback will not match what application has\n submitted, but the contents the ACaptureRequest will match what application submitted.</p>"]
    pub onLogicalCameraCaptureCompleted: ACameraCaptureSession_logicalCamera_captureCallback_result,
    #[doc = " This callback is called instead of {@link onLogicalCameraCaptureCompleted} when the\n camera device failed to produce a capture result for the\n request.\n\n <p>Other requests are unaffected, and some or all image buffers from\n the capture may have been pushed to their respective output\n streams.</p>\n\n <p>Note that the ACaptureRequest pointer in the callback will not match what application has\n submitted, but the contents the ACaptureRequest will match what application submitted.</p>\n\n @see ALogicalCameraCaptureFailure"]
    pub onLogicalCameraCaptureFailed: ACameraCaptureSession_logicalCamera_captureCallback_failed,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureSequenceCompleted}."]
    pub onCaptureSequenceCompleted: ACameraCaptureSession_captureCallback_sequenceEnd,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureSequenceAborted}."]
    pub onCaptureSequenceAborted: ACameraCaptureSession_captureCallback_sequenceAbort,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureBufferLost}."]
    pub onCaptureBufferLost: ACameraCaptureSession_captureCallback_bufferLost,
}
#[test]
fn bindgen_test_layout_ACameraCaptureSession_logicalCamera_captureCallbacks() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraCaptureSession_logicalCamera_captureCallbacks> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraCaptureSession_logicalCamera_captureCallbacks>(),
        64usize,
        concat!(
            "Size of: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacks)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraCaptureSession_logicalCamera_captureCallbacks>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacks)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).context) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacks),
            "::",
            stringify!(context)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureStarted) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacks),
            "::",
            stringify!(onCaptureStarted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureProgressed) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacks),
            "::",
            stringify!(onCaptureProgressed)
        )
    );
    assert_eq!(
        unsafe {
            ::std::ptr::addr_of!((*ptr).onLogicalCameraCaptureCompleted) as usize - ptr as usize
        },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacks),
            "::",
            stringify!(onLogicalCameraCaptureCompleted)
        )
    );
    assert_eq!(
        unsafe {
            ::std::ptr::addr_of!((*ptr).onLogicalCameraCaptureFailed) as usize - ptr as usize
        },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacks),
            "::",
            stringify!(onLogicalCameraCaptureFailed)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureSequenceCompleted) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacks),
            "::",
            stringify!(onCaptureSequenceCompleted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureSequenceAborted) as usize - ptr as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacks),
            "::",
            stringify!(onCaptureSequenceAborted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureBufferLost) as usize - ptr as usize },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacks),
            "::",
            stringify!(onCaptureBufferLost)
        )
    );
}
extern "C" {
    #[doc = " This has the same functionality as ACameraCaptureSession_capture, with added\n support for logical multi-camera where the capture callbacks supports result metadata for\n physical cameras."]
    pub fn ACameraCaptureSession_logicalCamera_capture(
        session: *mut ACameraCaptureSession,
        callbacks: *mut ACameraCaptureSession_logicalCamera_captureCallbacks,
        numRequests: ::std::os::raw::c_int,
        requests: *mut *mut ACaptureRequest,
        captureSequenceId: *mut ::std::os::raw::c_int,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " This has the same functionality as ACameraCaptureSession_setRepeatingRequest, with added\n support for logical multi-camera where the capture callbacks supports result metadata for\n physical cameras."]
    pub fn ACameraCaptureSession_logicalCamera_setRepeatingRequest(
        session: *mut ACameraCaptureSession,
        callbacks: *mut ACameraCaptureSession_logicalCamera_captureCallbacks,
        numRequests: ::std::os::raw::c_int,
        requests: *mut *mut ACaptureRequest,
        captureSequenceId: *mut ::std::os::raw::c_int,
    ) -> camera_status_t;
}
#[doc = " The definition of camera capture start callback. The same as\n {@link ACameraCaptureSession_captureCallbacks#onCaptureStarted}, except that\n it has the frame number of the capture as well.\n\n @param context The optional application context provided by user in\n                {@link ACameraCaptureSession_captureCallbacks}.\n @param session The camera capture session of interest.\n @param request The capture request that is starting. Note that this pointer points to a copy of\n                capture request sent by application, so the address is different to what\n                application sent but the content will match. This request will be freed by\n                framework immediately after this callback returns.\n @param timestamp The timestamp when the capture is started. This timestamp will match\n                  {@link ACAMERA_SENSOR_TIMESTAMP} of the {@link ACameraMetadata} in\n                  {@link ACameraCaptureSession_captureCallbacks#onCaptureCompleted} callback.\n @param frameNumber the frame number of the capture started"]
pub type ACameraCaptureSession_captureCallback_startV2 = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        session: *mut ACameraCaptureSession,
        request: *const ACaptureRequest,
        timestamp: i64,
        frameNumber: i64,
    ),
>;
#[doc = " This has the same functionality as ACameraCaptureSession_captureCallbacks,\n with the exception that captureCallback_startV2 callback is\n used, instead of captureCallback_start, to support retrieving the frame number."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraCaptureSession_captureCallbacksV2 {
    #[doc = " Same as ACameraCaptureSession_captureCallbacks"]
    pub context: *mut ::std::os::raw::c_void,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureStarted},\n except that it has the frame number of the capture added in the parameter\n list."]
    pub onCaptureStarted: ACameraCaptureSession_captureCallback_startV2,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureProgressed}."]
    pub onCaptureProgressed: ACameraCaptureSession_captureCallback_result,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureCompleted}."]
    pub onCaptureCompleted: ACameraCaptureSession_captureCallback_result,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureFailed}."]
    pub onCaptureFailed: ACameraCaptureSession_captureCallback_failed,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureSequenceCompleted}."]
    pub onCaptureSequenceCompleted: ACameraCaptureSession_captureCallback_sequenceEnd,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureSequenceAborted}."]
    pub onCaptureSequenceAborted: ACameraCaptureSession_captureCallback_sequenceAbort,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureBufferLost}."]
    pub onCaptureBufferLost: ACameraCaptureSession_captureCallback_bufferLost,
}
#[test]
fn bindgen_test_layout_ACameraCaptureSession_captureCallbacksV2() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraCaptureSession_captureCallbacksV2> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraCaptureSession_captureCallbacksV2>(),
        64usize,
        concat!(
            "Size of: ",
            stringify!(ACameraCaptureSession_captureCallbacksV2)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraCaptureSession_captureCallbacksV2>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(ACameraCaptureSession_captureCallbacksV2)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).context) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacksV2),
            "::",
            stringify!(context)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureStarted) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacksV2),
            "::",
            stringify!(onCaptureStarted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureProgressed) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacksV2),
            "::",
            stringify!(onCaptureProgressed)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureCompleted) as usize - ptr as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacksV2),
            "::",
            stringify!(onCaptureCompleted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureFailed) as usize - ptr as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacksV2),
            "::",
            stringify!(onCaptureFailed)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureSequenceCompleted) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacksV2),
            "::",
            stringify!(onCaptureSequenceCompleted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureSequenceAborted) as usize - ptr as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacksV2),
            "::",
            stringify!(onCaptureSequenceAborted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureBufferLost) as usize - ptr as usize },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_captureCallbacksV2),
            "::",
            stringify!(onCaptureBufferLost)
        )
    );
}
#[doc = " This has the same functionality as ACameraCaptureSession_logicalCamera_captureCallbacks,\n with the exception that an captureCallback_startV2 callback is\n used, instead of captureCallback_start, to support retrieving frame number."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraCaptureSession_logicalCamera_captureCallbacksV2 {
    #[doc = " Same as ACameraCaptureSession_captureCallbacks"]
    pub context: *mut ::std::os::raw::c_void,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureStarted},\n except that it has the frame number of the capture added in the parameter\n list."]
    pub onCaptureStarted: ACameraCaptureSession_captureCallback_startV2,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureProgressed}."]
    pub onCaptureProgressed: ACameraCaptureSession_captureCallback_result,
    #[doc = " Same as\n {@link ACameraCaptureSession_logicalCamera_captureCallbacks#onLogicalCaptureCompleted}."]
    pub onLogicalCameraCaptureCompleted: ACameraCaptureSession_logicalCamera_captureCallback_result,
    #[doc = " This callback is called instead of {@link onLogicalCameraCaptureCompleted} when the\n camera device failed to produce a capture result for the\n request.\n\n <p>Other requests are unaffected, and some or all image buffers from\n the capture may have been pushed to their respective output\n streams.</p>\n\n <p>Note that the ACaptureRequest pointer in the callback will not match what application has\n submitted, but the contents the ACaptureRequest will match what application submitted.</p>\n\n @see ALogicalCameraCaptureFailure"]
    pub onLogicalCameraCaptureFailed: ACameraCaptureSession_logicalCamera_captureCallback_failed,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureSequenceCompleted}."]
    pub onCaptureSequenceCompleted: ACameraCaptureSession_captureCallback_sequenceEnd,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureSequenceAborted}."]
    pub onCaptureSequenceAborted: ACameraCaptureSession_captureCallback_sequenceAbort,
    #[doc = " Same as {@link ACameraCaptureSession_captureCallbacks#onCaptureBufferLost}."]
    pub onCaptureBufferLost: ACameraCaptureSession_captureCallback_bufferLost,
}
#[test]
fn bindgen_test_layout_ACameraCaptureSession_logicalCamera_captureCallbacksV2() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraCaptureSession_logicalCamera_captureCallbacksV2> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraCaptureSession_logicalCamera_captureCallbacksV2>(),
        64usize,
        concat!(
            "Size of: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacksV2)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraCaptureSession_logicalCamera_captureCallbacksV2>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacksV2)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).context) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacksV2),
            "::",
            stringify!(context)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureStarted) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacksV2),
            "::",
            stringify!(onCaptureStarted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureProgressed) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacksV2),
            "::",
            stringify!(onCaptureProgressed)
        )
    );
    assert_eq!(
        unsafe {
            ::std::ptr::addr_of!((*ptr).onLogicalCameraCaptureCompleted) as usize - ptr as usize
        },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacksV2),
            "::",
            stringify!(onLogicalCameraCaptureCompleted)
        )
    );
    assert_eq!(
        unsafe {
            ::std::ptr::addr_of!((*ptr).onLogicalCameraCaptureFailed) as usize - ptr as usize
        },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacksV2),
            "::",
            stringify!(onLogicalCameraCaptureFailed)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureSequenceCompleted) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacksV2),
            "::",
            stringify!(onCaptureSequenceCompleted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureSequenceAborted) as usize - ptr as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacksV2),
            "::",
            stringify!(onCaptureSequenceAborted)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCaptureBufferLost) as usize - ptr as usize },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraCaptureSession_logicalCamera_captureCallbacksV2),
            "::",
            stringify!(onCaptureBufferLost)
        )
    );
}
extern "C" {
    #[doc = " This has the same functionality as ACameraCaptureSession_capture, with added\n support for v2 of camera callbacks, where the onCaptureStarted callback\n adds frame number in its parameter list."]
    pub fn ACameraCaptureSession_captureV2(
        session: *mut ACameraCaptureSession,
        callbacks: *mut ACameraCaptureSession_captureCallbacksV2,
        numRequests: ::std::os::raw::c_int,
        requests: *mut *mut ACaptureRequest,
        captureSequenceId: *mut ::std::os::raw::c_int,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " This has the same functionality as ACameraCaptureSession_logical_setRepeatingRequest, with added\n support for v2 of logical multi-camera callbacks where the onCaptureStarted\n callback adds frame number in its parameter list."]
    pub fn ACameraCaptureSession_setRepeatingRequestV2(
        session: *mut ACameraCaptureSession,
        callbacks: *mut ACameraCaptureSession_captureCallbacksV2,
        numRequests: ::std::os::raw::c_int,
        requests: *mut *mut ACaptureRequest,
        captureSequenceId: *mut ::std::os::raw::c_int,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " This has the same functionality as ACameraCaptureSession_logical_capture, with added\n support for v2 of logical multi-camera  callbacks where the onCaptureStarted callback\n adds frame number in its parameter list."]
    pub fn ACameraCaptureSession_logicalCamera_captureV2(
        session: *mut ACameraCaptureSession,
        callbacks: *mut ACameraCaptureSession_logicalCamera_captureCallbacksV2,
        numRequests: ::std::os::raw::c_int,
        requests: *mut *mut ACaptureRequest,
        captureSequenceId: *mut ::std::os::raw::c_int,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " This has the same functionality as ACameraCaptureSession_logical_setRepeatingRequest, with added\n support for v2 of logical multi-camera callbacks where the onCaptureStarted\n callback adds frame number in its parameter list."]
    pub fn ACameraCaptureSession_logicalCamera_setRepeatingRequestV2(
        session: *mut ACameraCaptureSession,
        callbacks: *mut ACameraCaptureSession_logicalCamera_captureCallbacksV2,
        numRequests: ::std::os::raw::c_int,
        requests: *mut *mut ACaptureRequest,
        captureSequenceId: *mut ::std::os::raw::c_int,
    ) -> camera_status_t;
}
#[doc = " Struct to hold list of camera device Ids. This can refer to either the Ids\n of connected camera devices returned from {@link ACameraManager_getCameraIdList},\n or the physical camera Ids passed into\n {@link ACameraDevice_createCaptureRequest_withPhysicalIds}."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraIdList {
    #[doc = "< Number of camera device Ids"]
    pub numCameras: ::std::os::raw::c_int,
    #[doc = "< list of camera device Ids"]
    pub cameraIds: *mut *const ::std::os::raw::c_char,
}
#[test]
fn bindgen_test_layout_ACameraIdList() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraIdList> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraIdList>(),
        16usize,
        concat!("Size of: ", stringify!(ACameraIdList))
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraIdList>(),
        8usize,
        concat!("Alignment of ", stringify!(ACameraIdList))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).numCameras) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraIdList),
            "::",
            stringify!(numCameras)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).cameraIds) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraIdList),
            "::",
            stringify!(cameraIds)
        )
    );
}
#[doc = " The camera device is in use already."]
pub const ERROR_CAMERA_IN_USE: _bindgen_ty_26 = 1;
#[doc = " The system-wide limit for number of open cameras or camera resources has\n been reached, and more camera devices cannot be opened until previous\n instances are closed."]
pub const ERROR_MAX_CAMERAS_IN_USE: _bindgen_ty_26 = 2;
#[doc = " The camera is disabled due to a device policy, and cannot be opened."]
pub const ERROR_CAMERA_DISABLED: _bindgen_ty_26 = 3;
#[doc = " The camera device has encountered a fatal error.\n <p>The camera device needs to be re-opened to be used again.</p>"]
pub const ERROR_CAMERA_DEVICE: _bindgen_ty_26 = 4;
#[doc = " The camera service has encountered a fatal error.\n <p>The Android device may need to be shut down and restarted to restore\n camera function, or there may be a persistent hardware problem.\n An attempt at recovery may be possible by closing the\n CameraDevice and the CameraManager, and trying to acquire all resources\n again from scratch.</p>"]
pub const ERROR_CAMERA_SERVICE: _bindgen_ty_26 = 5;
#[doc = " Enum for ACameraDevice_ErrorStateCallback error code"]
pub type _bindgen_ty_26 = ::std::os::raw::c_uint;
#[doc = " Camera device state callbacks to be used in {@link ACameraDevice_StateCallbacks}.\n\n @param context The optional context in {@link ACameraDevice_StateCallbacks} will be\n                passed to this callback.\n @param device The {@link ACameraDevice} that is being disconnected."]
pub type ACameraDevice_StateCallback = ::std::option::Option<
    unsafe extern "C" fn(context: *mut ::std::os::raw::c_void, device: *mut ACameraDevice),
>;
#[doc = " Camera device error state callbacks to be used in {@link ACameraDevice_StateCallbacks}.\n\n @param context The optional context in {@link ACameraDevice_StateCallbacks} will be\n                passed to this callback.\n @param device The {@link ACameraDevice} that is being disconnected.\n @param error The error code describes the cause of this error callback. See the folowing\n              links for more detail.\n\n @see ERROR_CAMERA_IN_USE\n @see ERROR_MAX_CAMERAS_IN_USE\n @see ERROR_CAMERA_DISABLED\n @see ERROR_CAMERA_DEVICE\n @see ERROR_CAMERA_SERVICE"]
pub type ACameraDevice_ErrorStateCallback = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        device: *mut ACameraDevice,
        error: ::std::os::raw::c_int,
    ),
>;
#[doc = " Applications' callbacks for camera device state changes, register with\n {@link ACameraManager_openCamera}."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraDevice_StateCallbacks {
    #[doc = " optional application context."]
    pub context: *mut ::std::os::raw::c_void,
    #[doc = " The function is  called when a camera device is no longer available for use.\n\n <p>Any attempt to call API methods on this ACameraDevice will return\n {@link ACAMERA_ERROR_CAMERA_DISCONNECTED}. The disconnection could be due to a\n change in security policy or permissions; the physical disconnection\n of a removable camera device; or the camera being needed for a\n higher-priority camera API client.</p>\n\n <p>Application should clean up the camera with {@link ACameraDevice_close} after\n this happens, as it is not recoverable until the camera can be opened\n again.</p>\n"]
    pub onDisconnected: ACameraDevice_StateCallback,
    #[doc = " The function called when a camera device has encountered a serious error.\n\n <p>This indicates a failure of the camera device or camera service in some way.\n Any attempt to call API methods on this ACameraDevice in the future will return\n {@link ACAMERA_ERROR_CAMERA_DISCONNECTED}.</p>\n\n <p>There may still be capture completion or camera stream callbacks that will be called\n after this error is received.</p>\n\n <p>Application should clean up the camera with {@link ACameraDevice_close} after this\n happens. Further attempts at recovery are error-code specific.</p>\n"]
    pub onError: ACameraDevice_ErrorStateCallback,
}
#[test]
fn bindgen_test_layout_ACameraDevice_StateCallbacks() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraDevice_StateCallbacks> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraDevice_StateCallbacks>(),
        24usize,
        concat!("Size of: ", stringify!(ACameraDevice_StateCallbacks))
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraDevice_StateCallbacks>(),
        8usize,
        concat!("Alignment of ", stringify!(ACameraDevice_StateCallbacks))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).context) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraDevice_StateCallbacks),
            "::",
            stringify!(context)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onDisconnected) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraDevice_StateCallbacks),
            "::",
            stringify!(onDisconnected)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onError) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraDevice_StateCallbacks),
            "::",
            stringify!(onError)
        )
    );
}
#[doc = " For backward compatiblity."]
pub type ACameraDevice_stateCallbacks = ACameraDevice_StateCallbacks;
extern "C" {
    #[doc = " Close the connection and free this ACameraDevice synchronously. Access to the ACameraDevice\n after calling this method will cause a crash.\n\n <p>After this call, all calls to the active ACameraCaptureSession associated to this\n ACameraDevice will return {@link ACAMERA_ERROR_SESSION_CLOSED} except for calls to\n {@link ACameraCaptureSession_close}.</p>\n\n <p>This method will stop all repeating captures sent via\n {@link ACameraCaptureSession_setRepeatingRequest} and block until all capture requests sent via\n {@link ACameraCaptureSession_capture} is complete. Once the method returns, the camera device\n will be removed from memory and access to the closed camera device pointer will cause a crash.</p>\n\n @param device the camera device to be closed\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if device is NULL.</li></ul>"]
    pub fn ACameraDevice_close(device: *mut ACameraDevice) -> camera_status_t;
}
extern "C" {
    #[doc = " Return the camera id associated with this camera device.\n\n @param device the camera device to be closed\n\n @return camera ID string. The returned string is managed by framework and should not be\n delete/free by the application. Also the returned string must not be used after the device\n has been closed."]
    pub fn ACameraDevice_getId(device: *const ACameraDevice) -> *const ::std::os::raw::c_char;
}
#[doc = " Create a request suitable for a camera preview window. Specifically, this\n means that high frame rate is given priority over the highest-quality\n post-processing. These requests would normally be used with the\n {@link ACameraCaptureSession_setRepeatingRequest} method.\n This template is guaranteed to be supported on all camera devices.\n\n @see ACameraDevice_createCaptureRequest"]
pub const ACameraDevice_request_template_TEMPLATE_PREVIEW: ACameraDevice_request_template = 1;
#[doc = " Create a request suitable for still image capture. Specifically, this\n means prioritizing image quality over frame rate. These requests would\n commonly be used with the {@link ACameraCaptureSession_capture} method.\n This template is guaranteed to be supported on all camera devices.\n\n @see ACameraDevice_createCaptureRequest"]
pub const ACameraDevice_request_template_TEMPLATE_STILL_CAPTURE: ACameraDevice_request_template = 2;
#[doc = " Create a request suitable for video recording. Specifically, this means\n that a stable frame rate is used, and post-processing is set for\n recording quality. These requests would commonly be used with the\n {@link ACameraCaptureSession_setRepeatingRequest} method.\n This template is guaranteed to be supported on all camera devices.\n\n @see ACameraDevice_createCaptureRequest"]
pub const ACameraDevice_request_template_TEMPLATE_RECORD: ACameraDevice_request_template = 3;
#[doc = " Create a request suitable for still image capture while recording\n video. Specifically, this means maximizing image quality without\n disrupting the ongoing recording. These requests would commonly be used\n with the {@link ACameraCaptureSession_capture} method while a request based on\n {@link TEMPLATE_RECORD} is is in use with {@link ACameraCaptureSession_setRepeatingRequest}.\n This template is guaranteed to be supported on all camera devices.\n\n @see ACameraDevice_createCaptureRequest"]
pub const ACameraDevice_request_template_TEMPLATE_VIDEO_SNAPSHOT: ACameraDevice_request_template =
    4;
#[doc = " Create a request suitable for zero shutter lag still capture. This means\n means maximizing image quality without compromising preview frame rate.\n AE/AWB/AF should be on auto mode.\n\n @see ACameraDevice_createCaptureRequest"]
pub const ACameraDevice_request_template_TEMPLATE_ZERO_SHUTTER_LAG: ACameraDevice_request_template =
    5;
#[doc = " A basic template for direct application control of capture\n parameters. All automatic control is disabled (auto-exposure, auto-white\n balance, auto-focus), and post-processing parameters are set to preview\n quality. The manual capture parameters (exposure, sensitivity, and so on)\n are set to reasonable defaults, but should be overriden by the\n application depending on the intended use case.\n This template is guaranteed to be supported on camera devices that support the\n {@link ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR} capability.\n\n @see ACameraDevice_createCaptureRequest"]
pub const ACameraDevice_request_template_TEMPLATE_MANUAL: ACameraDevice_request_template = 6;
#[doc = " Capture request pre-defined template types, used in {@link ACameraDevice_createCaptureRequest}\n and {@link ACameraDevice_createCaptureRequest_withPhysicalIds}."]
pub type ACameraDevice_request_template = ::std::os::raw::c_uint;
extern "C" {
    #[doc = " Create a ACaptureRequest for capturing images, initialized with template\n for a target use case.\n\n <p>The settings are chosen to be the best options for this camera device,\n so it is not recommended to reuse the same request for a different camera device.</p>\n\n @param device the camera device of interest\n @param templateId the type of capture request to be created.\n        See {@link ACameraDevice_request_template}.\n @param request the output request will be stored here if the method call succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds. The created capture request will be\n                                filled in request argument.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if device or request is NULL, templateId\n                                is undefined or camera device does not support requested template.\n                                </li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if the camera device is closed.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DEVICE} if the camera device encounters fatal error.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_SERVICE} if the camera service encounters fatal error.</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons.</li></ul>\n\n @see TEMPLATE_PREVIEW\n @see TEMPLATE_RECORD\n @see TEMPLATE_STILL_CAPTURE\n @see TEMPLATE_VIDEO_SNAPSHOT\n @see TEMPLATE_MANUAL"]
    pub fn ACameraDevice_createCaptureRequest(
        device: *const ACameraDevice,
        templateId: ACameraDevice_request_template,
        request: *mut *mut ACaptureRequest,
    ) -> camera_status_t;
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACaptureSessionOutputContainer {
    _unused: [u8; 0],
}
extern "C" {
    #[doc = " Create a capture session output container.\n\n <p>The container is used in {@link ACameraDevice_createCaptureSession} method to create a capture\n session. Use {@link ACaptureSessionOutputContainer_free} to free the container and its memory\n after application no longer needs the ACaptureSessionOutputContainer.</p>\n\n @param container the output {@link ACaptureSessionOutputContainer} will be stored here if the\n                  method call succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds. The created container will be\n                                filled in container argument.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if container is NULL.</li></ul>"]
    pub fn ACaptureSessionOutputContainer_create(
        container: *mut *mut ACaptureSessionOutputContainer,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Free a capture session output container.\n\n @param container the {@link ACaptureSessionOutputContainer} to be freed.\n\n @see ACaptureSessionOutputContainer_create"]
    pub fn ACaptureSessionOutputContainer_free(container: *mut ACaptureSessionOutputContainer);
}
extern "C" {
    #[doc = " Create a ACaptureSessionOutput object.\n\n <p>The ACaptureSessionOutput is used in {@link ACaptureSessionOutputContainer_add} method to add\n an output {@link ANativeWindow} to ACaptureSessionOutputContainer. Use\n {@link ACaptureSessionOutput_free} to free the object and its memory after application no longer\n needs the {@link ACaptureSessionOutput}.</p>\n\n @param anw the {@link ANativeWindow} to be associated with the {@link ACaptureSessionOutput}\n @param output the output {@link ACaptureSessionOutput} will be stored here if the\n                  method call succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds. The created container will be\n                                filled in the output argument.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if anw or output is NULL.</li></ul>\n\n @see ACaptureSessionOutputContainer_add"]
    pub fn ACaptureSessionOutput_create(
        anw: *mut ACameraWindowType,
        output: *mut *mut ACaptureSessionOutput,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Free a ACaptureSessionOutput object.\n\n @param output the {@link ACaptureSessionOutput} to be freed.\n\n @see ACaptureSessionOutput_create"]
    pub fn ACaptureSessionOutput_free(output: *mut ACaptureSessionOutput);
}
extern "C" {
    #[doc = " Add an {@link ACaptureSessionOutput} object to {@link ACaptureSessionOutputContainer}.\n\n @param container the {@link ACaptureSessionOutputContainer} of interest.\n @param output the output {@link ACaptureSessionOutput} to be added to container.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if container or output is NULL.</li></ul>"]
    pub fn ACaptureSessionOutputContainer_add(
        container: *mut ACaptureSessionOutputContainer,
        output: *const ACaptureSessionOutput,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Remove an {@link ACaptureSessionOutput} object from {@link ACaptureSessionOutputContainer}.\n\n <p>This method has no effect if the ACaptureSessionOutput does not exist in\n ACaptureSessionOutputContainer.</p>\n\n @param container the {@link ACaptureSessionOutputContainer} of interest.\n @param output the output {@link ACaptureSessionOutput} to be removed from container.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if container or output is NULL.</li></ul>"]
    pub fn ACaptureSessionOutputContainer_remove(
        container: *mut ACaptureSessionOutputContainer,
        output: *const ACaptureSessionOutput,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Create a new camera capture session by providing the target output set of {@link ANativeWindow}\n to the camera device.\n\n <p>If there is a preexisting session, the previous session will be closed\n automatically. However, app still needs to call {@link ACameraCaptureSession_close} on previous\n session. Otherwise the resources held by previous session will NOT be freed.</p>\n\n <p>The active capture session determines the set of potential output {@link ANativeWindow}s for\n the camera device for each capture request. A given request may use all\n or only some of the outputs. Once the ACameraCaptureSession is created, requests can be\n submitted with {@link ACameraCaptureSession_capture} or\n {@link ACameraCaptureSession_setRepeatingRequest}.</p>\n\n <p>Often the {@link ANativeWindow} used with this method can be obtained from a <a href=\n \"http://developer.android.com/reference/android/view/Surface.html\">Surface</a> java object by\n {@link ANativeWindow_fromSurface} NDK method. Surfaces or ANativeWindow suitable for inclusion as a camera\n output can be created for various use cases and targets:</p>\n\n <ul>\n\n <li>For drawing to a\n   <a href=\"http://developer.android.com/reference/android/view/SurfaceView.html\">SurfaceView</a>:\n   Once the SurfaceView's Surface is created, set the size\n   of the Surface with\n   <a href=\"http://developer.android.com/reference/android/view/SurfaceHolder.html#setFixedSize(int, int)\">\n    android.view.SurfaceHolder\\#setFixedSize</a> to be one of the PRIVATE output sizes\n   returned by {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS}\n   and then obtain the Surface by calling <a href=\n   \"http://developer.android.com/reference/android/view/SurfaceHolder.html#getSurface()\">\n   android.view.SurfaceHolder\\#getSurface</a>. If the size is not set by the application, it will\n   be rounded to the nearest supported size less than 1080p, by the camera device.</li>\n\n <li>For accessing through an OpenGL texture via a <a href=\n   \"http://developer.android.com/reference/android/graphics/SurfaceTexture.html\">SurfaceTexture</a>:\n   Set the size of the SurfaceTexture with <a href=\n   \"http://developer.android.com/reference/android/graphics/SurfaceTexture.html#setDefaultBufferSize(int, int)\">\n   setDefaultBufferSize</a> to be one of the PRIVATE output sizes\n   returned by {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS}\n   before creating a Surface from the SurfaceTexture with <a href=\n   \"http://developer.android.com/reference/android/view/Surface.html#Surface(android.graphics.SurfaceTexture)\">\n   Surface\\#Surface(SurfaceTextrue)</a>. If the size is not set by the application, it will be set to be the\n   smallest supported size less than 1080p, by the camera device.</li>\n\n <li>For recording with <a href=\n     \"http://developer.android.com/reference/android/media/MediaCodec.html\">\n     MediaCodec</a>: Call\n   <a href=\n     \"http://developer.android.com/reference/android/media/MediaCodec.html#createInputSurface()\">\n     android.media.MediaCodec\\#createInputSurface</a> after configuring\n   the media codec to use one of the PRIVATE output sizes\n   returned by {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS}.\n   </li>\n\n <li>For recording with <a href=\n    \"http://developer.android.com/reference/android/media/MediaRecorder.html\">\n    MediaRecorder</a>: Call\n   <a href=\"http://developer.android.com/reference/android/media/MediaRecorder.html#getSurface()\">\n    android.media.MediaRecorder\\#getSurface</a> after configuring the media recorder to use\n   one of the PRIVATE output sizes returned by\n   {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS}, or configuring it to use one of the supported\n   <a href=\"http://developer.android.com/reference/android/media/CamcorderProfile.html\">\n    CamcorderProfiles</a>.</li>\n\n <li>For access to RAW, uncompressed YUV, or compressed JPEG data in the application: Create an\n   {@link AImageReader} object using the {@link AImageReader_new} method with one of the supported\n   output formats given by {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS}. Then obtain a\n   ANativeWindow from it with {@link AImageReader_getWindow}.\n   If the AImageReader size is not set to a supported size, it will be rounded to a supported\n   size less than 1080p by the camera device.\n   </li>\n\n </ul>\n\n <p>The camera device will query each ANativeWindow's size and formats upon this\n call, so they must be set to a valid setting at this time.</p>\n\n <p>It can take several hundred milliseconds for the session's configuration to complete,\n since camera hardware may need to be powered on or reconfigured.</p>\n\n <p>If a prior ACameraCaptureSession already exists when this method is called, the previous\n session will no longer be able to accept new capture requests and will be closed. Any\n in-progress capture requests made on the prior session will be completed before it's closed.\n To minimize the transition time,\n the ACameraCaptureSession_abortCaptures method can be used to discard the remaining\n requests for the prior capture session before a new one is created. Note that once the new\n session is created, the old one can no longer have its captures aborted.</p>\n\n <p>Using larger resolution outputs, or more outputs, can result in slower\n output rate from the device.</p>\n\n <p>Configuring a session with an empty list will close the current session, if\n any. This can be used to release the current session's target surfaces for another use.</p>\n\n <p>While any of the sizes from {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS} can be used when\n a single output stream is configured, a given camera device may not be able to support all\n combination of sizes, formats, and targets when multiple outputs are configured at once.  The\n tables below list the maximum guaranteed resolutions for combinations of streams and targets,\n given the capabilities of the camera device.</p>\n\n <p>If an application tries to create a session using a set of targets that exceed the limits\n described in the below tables, one of three possibilities may occur. First, the session may\n be successfully created and work normally. Second, the session may be successfully created,\n but the camera device won't meet the frame rate guarantees as described in\n {@link ACAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS}. Or third, if the output set\n cannot be used at all, session creation will fail entirely, with\n {@link ACAMERA_ERROR_STREAM_CONFIGURE_FAIL} being returned.</p>\n\n <p>For the type column `PRIV` refers to output format {@link AIMAGE_FORMAT_PRIVATE},\n `YUV` refers to output format {@link AIMAGE_FORMAT_YUV_420_888},\n `JPEG` refers to output format {@link AIMAGE_FORMAT_JPEG},\n and `RAW` refers to output format {@link AIMAGE_FORMAT_RAW16}\n\n\n <p>For the maximum size column, `PREVIEW` refers to the best size match to the\n device's screen resolution, or to 1080p `(1920x1080)`, whichever is\n smaller. `RECORD` refers to the camera device's maximum supported recording resolution,\n as determined by <a href=\"http://developer.android.com/reference/android/media/CamcorderProfile.html\">\n android.media.CamcorderProfiles</a>. And `MAXIMUM` refers to the\n camera device's maximum output resolution for that format or target from\n {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS}.</p>\n\n <p>To use these tables, determine the number and the formats/targets of outputs needed, and\n find the row(s) of the table with those targets. The sizes indicate the maximum set of sizes\n that can be used; it is guaranteed that for those targets, the listed sizes and anything\n smaller from the list given by {@link ACAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS} can be\n successfully used to create a session.  For example, if a row indicates that a 8 megapixel\n (MP) YUV_420_888 output can be used together with a 2 MP `PRIV` output, then a session\n can be created with targets `[8 MP YUV, 2 MP PRIV]` or targets `[2 MP YUV, 2 MP PRIV]`;\n but a session with targets `[8 MP YUV, 4 MP PRIV]`, targets `[4 MP YUV, 4 MP PRIV]`,\n or targets `[8 MP PRIV, 2 MP YUV]` would not be guaranteed to work, unless\n some other row of the table lists such a combination.</p>\n\n <p>Legacy devices ({@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL}\n `== `{@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY LEGACY}) support at\n least the following stream combinations:\n\n <table>\n <tr><th colspan=\"7\">LEGACY-level guaranteed configurations</th></tr>\n <tr> <th colspan=\"2\" id=\"rb\">Target 1</th> <th colspan=\"2\" id=\"rb\">Target 2</th>  <th colspan=\"2\" id=\"rb\">Target 3</th> <th rowspan=\"2\">Sample use case(s)</th> </tr>\n <tr> <th>Type</th><th id=\"rb\">Max size</th> <th>Type</th><th id=\"rb\">Max size</th> <th>Type</th><th id=\"rb\">Max size</th></tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td colspan=\"2\" id=\"rb\"></td> <td>Simple preview, GPU video processing, or no-preview video recording.</td> </tr>\n <tr> <td>`JPEG`</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td colspan=\"2\" id=\"rb\"></td> <td>No-viewfinder still image capture.</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td colspan=\"2\" id=\"rb\"></td> <td>In-application video/image processing.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`JPEG`</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td>Standard still imaging.</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`JPEG`</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td>In-app processing plus still capture.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td colspan=\"2\" id=\"rb\"></td> <td>Standard recording.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td colspan=\"2\" id=\"rb\"></td> <td>Preview plus in-app processing.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`JPEG`</td><td id=\"rb\">`MAXIMUM`</td> <td>Still capture plus in-app processing.</td> </tr>\n </table><br>\n </p>\n\n <p>Limited-level ({@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL}\n `== `{@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED LIMITED}) devices\n support at least the following stream combinations in addition to those for\n {@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY LEGACY} devices:\n\n <table>\n <tr><th colspan=\"7\">LIMITED-level additional guaranteed configurations</th></tr>\n <tr><th colspan=\"2\" id=\"rb\">Target 1</th><th colspan=\"2\" id=\"rb\">Target 2</th><th colspan=\"2\" id=\"rb\">Target 3</th> <th rowspan=\"2\">Sample use case(s)</th> </tr>\n <tr><th>Type</th><th id=\"rb\">Max size</th><th>Type</th><th id=\"rb\">Max size</th><th>Type</th><th id=\"rb\">Max size</th></tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`PRIV`</td><td id=\"rb\">`RECORD `</td> <td colspan=\"2\" id=\"rb\"></td> <td>High-resolution video recording with preview.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`RECORD `</td> <td colspan=\"2\" id=\"rb\"></td> <td>High-resolution in-app video processing with preview.</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`RECORD `</td> <td colspan=\"2\" id=\"rb\"></td> <td>Two-input in-app video processing.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`PRIV`</td><td id=\"rb\">`RECORD `</td> <td>`JPEG`</td><td id=\"rb\">`RECORD `</td> <td>High-resolution recording with video snapshot.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`RECORD `</td> <td>`JPEG`</td><td id=\"rb\">`RECORD `</td> <td>High-resolution in-app processing with video snapshot.</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`JPEG`</td><td id=\"rb\">`MAXIMUM`</td> <td>Two-input in-app processing with still capture.</td> </tr>\n </table><br>\n </p>\n\n <p>FULL-level ({@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL}\n `== `{@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_FULL FULL}) devices\n support at least the following stream combinations in addition to those for\n {@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED LIMITED} devices:\n\n <table>\n <tr><th colspan=\"7\">FULL-level additional guaranteed configurations</th></tr>\n <tr><th colspan=\"2\" id=\"rb\">Target 1</th><th colspan=\"2\" id=\"rb\">Target 2</th><th colspan=\"2\" id=\"rb\">Target 3</th> <th rowspan=\"2\">Sample use case(s)</th> </tr>\n <tr><th>Type</th><th id=\"rb\">Max size</th><th>Type</th><th id=\"rb\">Max size</th><th>Type</th><th id=\"rb\">Max size</th> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`PRIV`</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td>Maximum-resolution GPU processing with preview.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td>Maximum-resolution in-app processing with preview.</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td>Maximum-resolution two-input in-app processsing.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`JPEG`</td><td id=\"rb\">`MAXIMUM`</td> <td>Video recording with maximum-size video snapshot</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`640x480`</td> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`MAXIMUM`</td> <td>Standard video recording plus maximum-resolution in-app processing.</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`640x480`</td> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`MAXIMUM`</td> <td>Preview plus two-input maximum-resolution in-app processing.</td> </tr>\n </table><br>\n </p>\n\n <p>RAW-capability ({@link ACAMERA_REQUEST_AVAILABLE_CAPABILITIES} includes\n {@link ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_RAW RAW}) devices additionally support\n at least the following stream combinations on both\n {@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_FULL FULL} and\n {@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED LIMITED} devices:\n\n <table>\n <tr><th colspan=\"7\">RAW-capability additional guaranteed configurations</th></tr>\n <tr><th colspan=\"2\" id=\"rb\">Target 1</th><th colspan=\"2\" id=\"rb\">Target 2</th><th colspan=\"2\" id=\"rb\">Target 3</th> <th rowspan=\"2\">Sample use case(s)</th> </tr>\n <tr><th>Type</th><th id=\"rb\">Max size</th><th>Type</th><th id=\"rb\">Max size</th><th>Type</th><th id=\"rb\">Max size</th> </tr>\n <tr> <td>`RAW `</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td colspan=\"2\" id=\"rb\"></td> <td>No-preview DNG capture.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`RAW `</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td>Standard DNG capture.</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`RAW `</td><td id=\"rb\">`MAXIMUM`</td> <td colspan=\"2\" id=\"rb\"></td> <td>In-app processing plus DNG capture.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`RAW `</td><td id=\"rb\">`MAXIMUM`</td> <td>Video recording with DNG capture.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`RAW `</td><td id=\"rb\">`MAXIMUM`</td> <td>Preview with in-app processing and DNG capture.</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`RAW `</td><td id=\"rb\">`MAXIMUM`</td> <td>Two-input in-app processing plus DNG capture.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`JPEG`</td><td id=\"rb\">`MAXIMUM`</td> <td>`RAW `</td><td id=\"rb\">`MAXIMUM`</td> <td>Still capture with simultaneous JPEG and DNG.</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`JPEG`</td><td id=\"rb\">`MAXIMUM`</td> <td>`RAW `</td><td id=\"rb\">`MAXIMUM`</td> <td>In-app processing with simultaneous JPEG and DNG.</td> </tr>\n </table><br>\n </p>\n\n <p>BURST-capability ({@link ACAMERA_REQUEST_AVAILABLE_CAPABILITIES} includes\n {@link ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE BURST_CAPTURE}) devices\n support at least the below stream combinations in addition to those for\n {@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED LIMITED} devices. Note that all\n FULL-level devices support the BURST capability, and the below list is a strict subset of the\n list for FULL-level devices, so this table is only relevant for LIMITED-level devices that\n support the BURST_CAPTURE capability.\n\n <table>\n <tr><th colspan=\"5\">BURST-capability additional guaranteed configurations</th></tr>\n <tr><th colspan=\"2\" id=\"rb\">Target 1</th><th colspan=\"2\" id=\"rb\">Target 2</th><th rowspan=\"2\">Sample use case(s)</th> </tr>\n <tr><th>Type</th><th id=\"rb\">Max size</th><th>Type</th><th id=\"rb\">Max size</th> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`PRIV`</td><td id=\"rb\">`MAXIMUM`</td> <td>Maximum-resolution GPU processing with preview.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`MAXIMUM`</td> <td>Maximum-resolution in-app processing with preview.</td> </tr>\n <tr> <td>`YUV `</td><td id=\"rb\">`PREVIEW`</td> <td>`YUV `</td><td id=\"rb\">`MAXIMUM`</td> <td>Maximum-resolution two-input in-app processsing.</td> </tr>\n </table><br>\n </p>\n\n <p>LEVEL-3 ({@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL}\n `== `{@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_3 LEVEL_3})\n support at least the following stream combinations in addition to the combinations for\n {@link ACAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_FULL FULL} and for\n RAW capability ({@link ACAMERA_REQUEST_AVAILABLE_CAPABILITIES} includes\n {@link ACAMERA_REQUEST_AVAILABLE_CAPABILITIES_RAW RAW}):\n\n <table>\n <tr><th colspan=\"11\">LEVEL-3 additional guaranteed configurations</th></tr>\n <tr><th colspan=\"2\" id=\"rb\">Target 1</th><th colspan=\"2\" id=\"rb\">Target 2</th><th colspan=\"2\" id=\"rb\">Target 3</th><th colspan=\"2\" id=\"rb\">Target 4</th><th rowspan=\"2\">Sample use case(s)</th> </tr>\n <tr><th>Type</th><th id=\"rb\">Max size</th><th>Type</th><th id=\"rb\">Max size</th><th>Type</th><th id=\"rb\">Max size</th><th>Type</th><th id=\"rb\">Max size</th> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`PRIV`</td><td id=\"rb\">`640x480`</td> <td>`YUV`</td><td id=\"rb\">`MAXIMUM`</td> <td>`RAW`</td><td id=\"rb\">`MAXIMUM`</td> <td>In-app viewfinder analysis with dynamic selection of output format.</td> </tr>\n <tr> <td>`PRIV`</td><td id=\"rb\">`PREVIEW`</td> <td>`PRIV`</td><td id=\"rb\">`640x480`</td> <td>`JPEG`</td><td id=\"rb\">`MAXIMUM`</td> <td>`RAW`</td><td id=\"rb\">`MAXIMUM`</td> <td>In-app viewfinder analysis with dynamic selection of output format.</td> </tr>\n </table><br>\n </p>\n\n <p>Since the capabilities of camera devices vary greatly, a given camera device may support\n target combinations with sizes outside of these guarantees, but this can only be tested for\n by attempting to create a session with such targets.</p>\n\n <p>Exception on 176x144 (QCIF) resolution:\n Camera devices usually have a fixed capability for downscaling from larger resolution to\n smaller, and the QCIF resolution sometimes cannot be fully supported due to this\n limitation on devices with high-resolution image sensors. Therefore, trying to configure a\n QCIF resolution stream together with any other stream larger than 1920x1080 resolution\n (either width or height) might not be supported, and capture session creation will fail if it\n is not.</p>\n\n @param device the camera device of interest.\n @param outputs the {@link ACaptureSessionOutputContainer} describes all output streams.\n @param callbacks the {@link ACameraCaptureSession_stateCallbacks capture session state callbacks}.\n @param session the created {@link ACameraCaptureSession} will be filled here if the method call\n        succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds. The created capture session will be\n                                filled in session argument.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if any of device, outputs, callbacks or\n                                session is NULL.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if the camera device is closed.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DEVICE} if the camera device encounters fatal error.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_SERVICE} if the camera service encounters fatal error.</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons.</li></ul>"]
    pub fn ACameraDevice_createCaptureSession(
        device: *mut ACameraDevice,
        outputs: *const ACaptureSessionOutputContainer,
        callbacks: *const ACameraCaptureSession_stateCallbacks,
        session: *mut *mut ACameraCaptureSession,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Create a shared ACaptureSessionOutput object.\n\n <p>The ACaptureSessionOutput is used in {@link ACaptureSessionOutputContainer_add} method to add\n an output {@link ANativeWindow} to ACaptureSessionOutputContainer. Use\n {@link ACaptureSessionOutput_free} to free the object and its memory after application no longer\n needs the {@link ACaptureSessionOutput}. A shared ACaptureSessionOutput can be further modified\n via {@link ACaptureSessionSharedOutput_add} or {@link ACaptureSessionSharedOutput_remove} and\n must be updated via {@link ACameraCaptureSession_updateSharedOutput}.</p>\n\n @param anw the {@link ANativeWindow} to be associated with the {@link ACaptureSessionOutput}\n @param output the output {@link ACaptureSessionOutput} will be stored here if the\n                  method call succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds. The created container will be\n                                filled in the output argument.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if anw or output is NULL.</li></ul>\n\n @see ACaptureSessionOutputContainer_add"]
    pub fn ACaptureSessionSharedOutput_create(
        anw: *mut ACameraWindowType,
        output: *mut *mut ACaptureSessionOutput,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Add a native window to shared ACaptureSessionOutput.\n\n The ACaptureSessionOutput must be created via {@link ACaptureSessionSharedOutput_create}.\n\n @param output  the shared ACaptureSessionOutput to be extended.\n @param anw The new native window.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if anw or output is NULL; or output is not\n             shared see {@link ACaptureSessionSharedOutput_create}; or anw matches with the native\n             window associated with ACaptureSessionOutput; or anw is already present inside\n             ACaptureSessionOutput.</li></ul>"]
    pub fn ACaptureSessionSharedOutput_add(
        output: *mut ACaptureSessionOutput,
        anw: *mut ACameraWindowType,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Remove a native window from shared ACaptureSessionOutput.\n\n @param output the {@link ACaptureSessionOutput} to be modified.\n @param anw The native window to be removed.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if anw or output is NULL; or output is not\n             shared see {@link ACaptureSessionSharedOutput_create}; or anw matches with the native\n             window associated with ACaptureSessionOutput; or anw is not present inside\n             ACaptureSessionOutput.</li></ul>"]
    pub fn ACaptureSessionSharedOutput_remove(
        output: *mut ACaptureSessionOutput,
        anw: *mut ACameraWindowType,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Create a new camera capture session similar to {@link ACameraDevice_createCaptureSession}. This\n function allows clients to pass additional session parameters during session initialization. For\n further information about session parameters see {@link ACAMERA_REQUEST_AVAILABLE_SESSION_KEYS}.\n\n @param device the camera device of interest.\n @param outputs the {@link ACaptureSessionOutputContainer} describes all output streams.\n @param sessionParameters An optional capture request that contains the initial values of session\n                          parameters advertised in\n                          {@link ACAMERA_REQUEST_AVAILABLE_SESSION_KEYS}.\n @param callbacks the {@link ACameraCaptureSession_stateCallbacks}\n                  capture session state callbacks.\n @param session the created {@link ACameraCaptureSession} will be filled here if the method call\n                succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds. The created capture session will be\n                                filled in session argument.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if any of device, outputs, callbacks or\n                                session is NULL.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if the camera device is closed.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DEVICE} if the camera device encounters fatal error.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_SERVICE} if the camera service encounters fatal error.\n         </li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons.</li></ul>"]
    pub fn ACameraDevice_createCaptureSessionWithSessionParameters(
        device: *mut ACameraDevice,
        outputs: *const ACaptureSessionOutputContainer,
        sessionParameters: *const ACaptureRequest,
        callbacks: *const ACameraCaptureSession_stateCallbacks,
        session: *mut *mut ACameraCaptureSession,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Create a ACaptureSessionOutput object used for streaming from a physical\n camera as part of a logical camera device.\n\n <p>The ACaptureSessionOutput is used in {@link ACaptureSessionOutputContainer_add} method to add\n an output {@link ANativeWindow} to ACaptureSessionOutputContainer. Use\n {@link ACaptureSessionOutput_free} to free the object and its memory after application no longer\n needs the {@link ACaptureSessionOutput}.</p>\n\n @param anw the {@link ANativeWindow} to be associated with the {@link ACaptureSessionOutput}\n @param physicalId the Id of the physical camera this output is associated\n                  with.\n @param output the output {@link ACaptureSessionOutput} will be stored here if the\n                  method call succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds. The created container will be\n                                filled in the output argument.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if anw, physicalId or output is NULL.</li></ul>\n\n @see ACaptureSessionOutputContainer_add"]
    pub fn ACaptureSessionPhysicalOutput_create(
        anw: *mut ACameraWindowType,
        physicalId: *const ::std::os::raw::c_char,
        output: *mut *mut ACaptureSessionOutput,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Create a logical multi-camera ACaptureRequest for capturing images, initialized with template\n for a target use case, with the ability to specify physical camera settings.\n\n <p>The settings are chosen to be the best options for this camera device,\n so it is not recommended to reuse the same request for a different camera device.</p>\n\n <p>Note that for all keys in physical camera settings, only the keys\n advertised in ACAMERA_REQUEST_AVAILABLE_PHYSICAL_CAMERA_REQUEST_KEYS are\n applicable. All other keys are ignored by the camera device.</p>\n\n @param device the camera device of interest\n @param templateId the type of capture request to be created.\n        See {@link ACameraDevice_request_template}.\n @param physicalIdList The list of physical camera Ids that can be used to\n        customize the request for a specific physical camera.\n @param request the output request will be stored here if the method call succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds. The created capture request will be\n                                filled in request argument.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if device, physicalIdList, or request is\n                                NULL, templateId is undefined or camera device does not support\n                                requested template, or if some Ids in physicalIdList isn't a\n                                valid physical camera backing the current camera device.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if the camera device is closed.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DEVICE} if the camera device encounters fatal error.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_SERVICE} if the camera service encounters fatal error.</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons.</li></ul>\n\n @see TEMPLATE_PREVIEW\n @see TEMPLATE_RECORD\n @see TEMPLATE_STILL_CAPTURE\n @see TEMPLATE_VIDEO_SNAPSHOT\n @see TEMPLATE_MANUAL"]
    pub fn ACameraDevice_createCaptureRequest_withPhysicalIds(
        device: *const ACameraDevice,
        templateId: ACameraDevice_request_template,
        physicalIdList: *const ACameraIdList,
        request: *mut *mut ACaptureRequest,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Check whether a particular {@link ACaptureSessionOutputContainer} is supported by\n the camera device.\n\n <p>This method performs a runtime check of a given {@link\n ACaptureSessionOutputContainer}. The result confirms whether or not the\n passed CaptureSession outputs can be successfully used to create a camera\n capture session using {@link ACameraDevice_createCaptureSession}.</p>\n\n <p>This method can be called at any point before, during and after active\n capture session. It must not impact normal camera behavior in any way and\n must complete significantly faster than creating a capture session.</p>\n\n <p>Although this method is faster than creating a new capture session, it is not intended\n to be used for exploring the entire space of supported stream combinations.</p>\n\n @param device the camera device of interest\n @param sessionOutputContainer the {@link ACaptureSessionOutputContainer} of\n                               interest.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the given {@link ACaptureSessionOutputContainer}\n                                is supported by the camera device.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if device, or sessionOutputContainer\n                                                     is NULL.</li>\n         <li>{@link ACAMERA_ERROR_STREAM_CONFIGURE_FAIL} if the given\n                                                         {@link ACaptureSessionOutputContainer}\n                                                         is not supported by\n                                                         the camera\n                                                         device.</li>\n        <li>{@link ACAMERA_ERROR_UNSUPPORTED_OPERATION} if the query operation is not\n                                                        supported by the camera device.</li>\n        </ul>"]
    pub fn ACameraDevice_isSessionConfigurationSupported(
        device: *const ACameraDevice,
        sessionOutputContainer: *const ACaptureSessionOutputContainer,
    ) -> camera_status_t;
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraManager {
    _unused: [u8; 0],
}
extern "C" {
    #[doc = " Create ACameraManager instance.\n\n <p>The ACameraManager is responsible for\n detecting, characterizing, and connecting to {@link ACameraDevice}s.</p>\n\n <p>The caller must call {@link ACameraManager_delete} to free the resources once it is done\n using the ACameraManager instance.</p>\n\n @return a {@link ACameraManager} instance.\n"]
    pub fn ACameraManager_create() -> *mut ACameraManager;
}
extern "C" {
    #[doc = " <p>Delete the {@link ACameraManager} instance and free its resources. </p>\n\n @param manager the {@link ACameraManager} instance to be deleted."]
    pub fn ACameraManager_delete(manager: *mut ACameraManager);
}
extern "C" {
    #[doc = " Create a list of currently connected camera devices, including\n cameras that may be in use by other camera API clients.\n\n <p>Non-removable cameras use integers starting at 0 for their\n identifiers, while removable cameras have a unique identifier for each\n individual device, even if they are the same model.</p>\n\n <p>ACameraManager_getCameraIdList will allocate and return an {@link ACameraIdList}.\n The caller must call {@link ACameraManager_deleteCameraIdList} to free the memory</p>\n\n <p>Note: the returned camera list might be a subset to the output of <a href=\n \"https://developer.android.com/reference/android/hardware/camera2/CameraManager.html#getCameraIdList()\">\n SDK CameraManager#getCameraIdList API</a> as the NDK API does not support some legacy camera\n hardware.</p>\n\n @param manager the {@link ACameraManager} of interest\n @param cameraIdList the output {@link ACameraIdList} will be filled in here if the method call\n        succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if manager or cameraIdList is NULL.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if connection to camera service fails.</li>\n         <li>{@link ACAMERA_ERROR_NOT_ENOUGH_MEMORY} if allocating memory fails.</li></ul>"]
    pub fn ACameraManager_getCameraIdList(
        manager: *mut ACameraManager,
        cameraIdList: *mut *mut ACameraIdList,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Delete a list of camera devices allocated via {@link ACameraManager_getCameraIdList}.\n\n @param cameraIdList the {@link ACameraIdList} to be deleted."]
    pub fn ACameraManager_deleteCameraIdList(cameraIdList: *mut ACameraIdList);
}
#[doc = " Definition of camera availability callbacks.\n\n @param context The optional application context provided by user in\n                {@link ACameraManager_AvailabilityCallbacks}.\n @param cameraId The ID of the camera device whose availability is changing. The memory of this\n                 argument is owned by camera framework and will become invalid immediately after\n                 this callback returns."]
pub type ACameraManager_AvailabilityCallback = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        cameraId: *const ::std::os::raw::c_char,
    ),
>;
#[doc = " Definition of physical camera availability callbacks.\n\n @param context The optional application context provided by user in\n                {@link ACameraManager_AvailabilityCallbacks}.\n @param cameraId The ID of the logical multi-camera device whose physical camera status is\n                 changing. The memory of this argument is owned by camera framework and will\n                 become invalid immediately after this callback returns.\n @param physicalCameraId The ID of the physical camera device whose status is changing. The\n                 memory of this argument is owned by camera framework and will become invalid\n                 immediately after this callback returns."]
pub type ACameraManager_PhysicalCameraAvailabilityCallback = ::std::option::Option<
    unsafe extern "C" fn(
        context: *mut ::std::os::raw::c_void,
        cameraId: *const ::std::os::raw::c_char,
        physicalCameraId: *const ::std::os::raw::c_char,
    ),
>;
#[doc = " A listener for camera devices becoming available or unavailable to open.\n\n <p>Cameras become available when they are no longer in use, or when a new\n removable camera is connected. They become unavailable when some\n application or service starts using a camera, or when a removable camera\n is disconnected.</p>\n\n @see ACameraManager_registerAvailabilityCallback"]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraManager_AvailabilityListener {
    #[doc = " Optional application context."]
    pub context: *mut ::std::os::raw::c_void,
    #[doc = " Called when a camera becomes available"]
    pub onCameraAvailable: ACameraManager_AvailabilityCallback,
    #[doc = " Called when a camera becomes unavailable"]
    pub onCameraUnavailable: ACameraManager_AvailabilityCallback,
}
#[test]
fn bindgen_test_layout_ACameraManager_AvailabilityListener() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraManager_AvailabilityListener> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraManager_AvailabilityListener>(),
        24usize,
        concat!("Size of: ", stringify!(ACameraManager_AvailabilityListener))
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraManager_AvailabilityListener>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(ACameraManager_AvailabilityListener)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).context) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraManager_AvailabilityListener),
            "::",
            stringify!(context)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCameraAvailable) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraManager_AvailabilityListener),
            "::",
            stringify!(onCameraAvailable)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onCameraUnavailable) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraManager_AvailabilityListener),
            "::",
            stringify!(onCameraUnavailable)
        )
    );
}
#[doc = " A listener for camera devices becoming available or unavailable to open.\n\n <p>Cameras become available when they are no longer in use, or when a new\n removable camera is connected. They become unavailable when some\n application or service starts using a camera, or when a removable camera\n is disconnected.</p>\n\n @see ACameraManager_registerAvailabilityCallback"]
pub type ACameraManager_AvailabilityCallbacks = ACameraManager_AvailabilityListener;
extern "C" {
    #[doc = " Register camera availability callbacks.\n\n <p>onCameraUnavailable will be called whenever a camera device is opened by any camera API client.\n Other camera API clients may still be able to open such a camera device, evicting the existing\n client if they have higher priority than the existing client of a camera device.\n See {@link ACameraManager_openCamera} for more details.</p>\n\n <p>The callbacks will be called on a dedicated thread shared among all ACameraManager\n instances.</p>\n\n <p>Since this callback will be registered with the camera service, remember to unregister it\n once it is no longer needed; otherwise the callback will continue to receive events\n indefinitely and it may prevent other resources from being released. Specifically, the\n callbacks will be invoked independently of the general activity lifecycle and independently\n of the state of individual ACameraManager instances.</p>\n\n @param manager the {@link ACameraManager} of interest.\n @param callback the {@link ACameraManager_AvailabilityCallbacks} to be registered.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if manager or callback is NULL, or\n                  {ACameraManager_AvailabilityCallbacks#onCameraAvailable} or\n                  {ACameraManager_AvailabilityCallbacks#onCameraUnavailable} is NULL.</li></ul>"]
    pub fn ACameraManager_registerAvailabilityCallback(
        manager: *mut ACameraManager,
        callback: *const ACameraManager_AvailabilityCallbacks,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Unregister camera availability callbacks.\n\n <p>Removing a callback that isn't registered has no effect.</p>\n\n <p>This function must not be called with a mutex lock also held by\n the availability callbacks.</p>\n\n @param manager the {@link ACameraManager} of interest.\n @param callback the {@link ACameraManager_AvailabilityCallbacks} to be unregistered.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if callback,\n                  {ACameraManager_AvailabilityCallbacks#onCameraAvailable} or\n                  {ACameraManager_AvailabilityCallbacks#onCameraUnavailable} is NULL.</li></ul>"]
    pub fn ACameraManager_unregisterAvailabilityCallback(
        manager: *mut ACameraManager,
        callback: *const ACameraManager_AvailabilityCallbacks,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Query the capabilities of a camera device. These capabilities are\n immutable for a given camera.\n\n <p>See {@link ACameraMetadata} document and {@link NdkCameraMetadataTags.h} for more details.</p>\n\n <p>The caller must call {@link ACameraMetadata_free} to free the memory of the output\n characteristics.</p>\n\n @param manager the {@link ACameraManager} of interest.\n @param cameraId the ID string of the camera device of interest.\n @param characteristics the output {@link ACameraMetadata} will be filled here if the method call\n        succeeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if manager, cameraId, or characteristics\n                  is NULL, or cameraId does not match any camera devices connected.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if connection to camera service fails.</li>\n         <li>{@link ACAMERA_ERROR_NOT_ENOUGH_MEMORY} if allocating memory fails.</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons.</li></ul>"]
    pub fn ACameraManager_getCameraCharacteristics(
        manager: *mut ACameraManager,
        cameraId: *const ::std::os::raw::c_char,
        characteristics: *mut *mut ACameraMetadata,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Open a connection to a camera with the given ID. The opened camera device will be\n returned in the `device` parameter.\n\n <p>Use {@link ACameraManager_getCameraIdList} to get the list of available camera\n devices. Note that even if an id is listed, open may fail if the device\n is disconnected between the calls to {@link ACameraManager_getCameraIdList} and\n {@link ACameraManager_openCamera}, or if a higher-priority camera API client begins using the\n camera device.</p>\n\n <p>Devices for which the\n {@link ACameraManager_AvailabilityCallbacks#onCameraUnavailable} callback has been called due to\n the device being in use by a lower-priority, background camera API client can still potentially\n be opened by calling this method when the calling camera API client has a higher priority\n than the current camera API client using this device.  In general, if the top, foreground\n activity is running within your application process, your process will be given the highest\n priority when accessing the camera, and this method will succeed even if the camera device is\n in use by another camera API client. Any lower-priority application that loses control of the\n camera in this way will receive an\n {@link ACameraDevice_StateCallbacks#onDisconnected} callback.</p>\n\n <p>Once the camera is successfully opened,the ACameraDevice can then be set up\n for operation by calling {@link ACameraDevice_createCaptureSession} and\n {@link ACameraDevice_createCaptureRequest}.</p>\n\n <p>If the camera becomes disconnected after this function call returns,\n {@link ACameraDevice_StateCallbacks#onDisconnected} with a\n ACameraDevice in the disconnected state will be called.</p>\n\n <p>If the camera runs into error after this function call returns,\n {@link ACameraDevice_StateCallbacks#onError} with a\n ACameraDevice in the error state will be called.</p>\n\n @param manager the {@link ACameraManager} of interest.\n @param cameraId the ID string of the camera device to be opened.\n @param callback the {@link ACameraDevice_StateCallbacks} associated with the opened camera device.\n @param device the opened {@link ACameraDevice} will be filled here if the method call succeeds.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if manager, cameraId, callback, or device\n                  is NULL, or cameraId does not match any camera devices connected.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISCONNECTED} if connection to camera service fails.</li>\n         <li>{@link ACAMERA_ERROR_NOT_ENOUGH_MEMORY} if allocating memory fails.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_IN_USE} if camera device is being used by a higher\n                   priority camera API client.</li>\n         <li>{@link ACAMERA_ERROR_MAX_CAMERA_IN_USE} if the system-wide limit for number of open\n                   cameras or camera resources has been reached, and more camera devices cannot be\n                   opened until previous instances are closed.</li>\n         <li>{@link ACAMERA_ERROR_CAMERA_DISABLED} if the camera is disabled due to a device\n                   policy, and cannot be opened.</li>\n         <li>{@link ACAMERA_ERROR_PERMISSION_DENIED} if the application does not have permission\n                   to open camera.</li>\n         <li>{@link ACAMERA_ERROR_UNKNOWN} if the method fails for some other reasons.</li></ul>"]
    pub fn ACameraManager_openCamera(
        manager: *mut ACameraManager,
        cameraId: *const ::std::os::raw::c_char,
        callback: *mut ACameraDevice_StateCallbacks,
        device: *mut *mut ACameraDevice,
    ) -> camera_status_t;
}
#[doc = " Definition of camera access permission change callback.\n\n <p>Notification that camera access priorities have changed and the camera may\n now be openable. An application that was previously denied camera access due to\n a higher-priority user already using the camera, or that was disconnected from an\n active camera session due to a higher-priority user trying to open the camera,\n should try to open the camera again if it still wants to use it.  Note that\n multiple applications may receive this callback at the same time, and only one of\n them will succeed in opening the camera in practice, depending on exact access\n priority levels and timing. This method is useful in cases where multiple\n applications may be in the resumed state at the same time, and the user switches\n focus between them, or if the current camera-using application moves between\n full-screen and Picture-in-Picture (PiP) states. In such cases, the camera\n available/unavailable callbacks will not be invoked, but another application may\n now have higher priority for camera access than the current camera-using\n application.</p>\n\n @param context The optional application context provided by user in\n                {@link ACameraManager_AvailabilityListener}."]
pub type ACameraManager_AccessPrioritiesChangedCallback =
    ::std::option::Option<unsafe extern "C" fn(context: *mut ::std::os::raw::c_void)>;
#[doc = " A listener for camera devices becoming available/unavailable to open or when\n the camera access permissions change.\n\n <p>Cameras become available when they are no longer in use, or when a new\n removable camera is connected. They become unavailable when some\n application or service starts using a camera, or when a removable camera\n is disconnected.</p>\n\n @see ACameraManager_registerExtendedAvailabilityCallback"]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ACameraManager_ExtendedAvailabilityListener {
    #[doc = " Called when a camera becomes available or unavailable"]
    pub availabilityCallbacks: ACameraManager_AvailabilityCallbacks,
    #[doc = " Called when there is camera access permission change"]
    pub onCameraAccessPrioritiesChanged: ACameraManager_AccessPrioritiesChangedCallback,
    #[doc = " Called when a physical camera becomes available"]
    pub onPhysicalCameraAvailable: ACameraManager_PhysicalCameraAvailabilityCallback,
    #[doc = " Called when a physical camera becomes unavailable"]
    pub onPhysicalCameraUnavailable: ACameraManager_PhysicalCameraAvailabilityCallback,
    #[doc = " Reserved for future use, please ensure that all entries are set to NULL"]
    pub reserved: [*mut ::std::os::raw::c_void; 4usize],
}
#[test]
fn bindgen_test_layout_ACameraManager_ExtendedAvailabilityListener() {
    const UNINIT: ::std::mem::MaybeUninit<ACameraManager_ExtendedAvailabilityListener> =
        ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<ACameraManager_ExtendedAvailabilityListener>(),
        80usize,
        concat!(
            "Size of: ",
            stringify!(ACameraManager_ExtendedAvailabilityListener)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<ACameraManager_ExtendedAvailabilityListener>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(ACameraManager_ExtendedAvailabilityListener)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).availabilityCallbacks) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraManager_ExtendedAvailabilityListener),
            "::",
            stringify!(availabilityCallbacks)
        )
    );
    assert_eq!(
        unsafe {
            ::std::ptr::addr_of!((*ptr).onCameraAccessPrioritiesChanged) as usize - ptr as usize
        },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraManager_ExtendedAvailabilityListener),
            "::",
            stringify!(onCameraAccessPrioritiesChanged)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onPhysicalCameraAvailable) as usize - ptr as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraManager_ExtendedAvailabilityListener),
            "::",
            stringify!(onPhysicalCameraAvailable)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).onPhysicalCameraUnavailable) as usize - ptr as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraManager_ExtendedAvailabilityListener),
            "::",
            stringify!(onPhysicalCameraUnavailable)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reserved) as usize - ptr as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(ACameraManager_ExtendedAvailabilityListener),
            "::",
            stringify!(reserved)
        )
    );
}
#[doc = " A listener for camera devices becoming available/unavailable to open or when\n the camera access permissions change.\n\n <p>Cameras become available when they are no longer in use, or when a new\n removable camera is connected. They become unavailable when some\n application or service starts using a camera, or when a removable camera\n is disconnected.</p>\n\n @see ACameraManager_registerExtendedAvailabilityCallback"]
pub type ACameraManager_ExtendedAvailabilityCallbacks = ACameraManager_ExtendedAvailabilityListener;
extern "C" {
    #[doc = " Register camera extended availability callbacks.\n\n <p>onCameraUnavailable will be called whenever a camera device is opened by any camera API\n client. Other camera API clients may still be able to open such a camera device, evicting the\n existing client if they have higher priority than the existing client of a camera device.\n See {@link ACameraManager_openCamera} for more details.</p>\n\n <p>The callbacks will be called on a dedicated thread shared among all ACameraManager\n instances.</p>\n\n <p>Since this callback will be registered with the camera service, remember to unregister it\n once it is no longer needed; otherwise the callback will continue to receive events\n indefinitely and it may prevent other resources from being released. Specifically, the\n callbacks will be invoked independently of the general activity lifecycle and independently\n of the state of individual ACameraManager instances.</p>\n\n @param manager the {@link ACameraManager} of interest.\n @param callback the {@link ACameraManager_ExtendedAvailabilityCallbacks} to be registered.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if manager or callback is NULL, or\n                  {ACameraManager_ExtendedAvailabilityCallbacks#onCameraAccessPrioritiesChanged}\n                  or {ACameraManager_AvailabilityCallbacks#onCameraAvailable} or\n                  {ACameraManager_AvailabilityCallbacks#onCameraUnavailable} is NULL.</li></ul>"]
    pub fn ACameraManager_registerExtendedAvailabilityCallback(
        manager: *mut ACameraManager,
        callback: *const ACameraManager_ExtendedAvailabilityCallbacks,
    ) -> camera_status_t;
}
extern "C" {
    #[doc = " Unregister camera extended availability callbacks.\n\n <p>Removing a callback that isn't registered has no effect.</p>\n\n <p>This function must not be called with a mutex lock also held by\n the extended availability callbacks.</p>\n\n @param manager the {@link ACameraManager} of interest.\n @param callback the {@link ACameraManager_ExtendedAvailabilityCallbacks} to be unregistered.\n\n @return <ul>\n         <li>{@link ACAMERA_OK} if the method call succeeds.</li>\n         <li>{@link ACAMERA_ERROR_INVALID_PARAMETER} if callback,\n                  {ACameraManager_ExtendedAvailabilityCallbacks#onCameraAccessPrioritiesChanged}\n                  or {ACameraManager_AvailabilityCallbacks#onCameraAvailable} or\n                  {ACameraManager_AvailabilityCallbacks#onCameraUnavailable} is NULL.</li></ul>"]
    pub fn ACameraManager_unregisterExtendedAvailabilityCallback(
        manager: *mut ACameraManager,
        callback: *const ACameraManager_ExtendedAvailabilityCallbacks,
    ) -> camera_status_t;
}
#[doc = " The requested media operation completed successfully."]
pub const media_status_t_AMEDIA_OK: media_status_t = 0;
#[doc = " This indicates required resource was not able to be allocated."]
pub const media_status_t_AMEDIACODEC_ERROR_INSUFFICIENT_RESOURCE: media_status_t = 1100;
#[doc = " This indicates the resource manager reclaimed the media resource used by the codec.\n With this error, the codec must be released, as it has moved to terminal state."]
pub const media_status_t_AMEDIACODEC_ERROR_RECLAIMED: media_status_t = 1101;
#[doc = " This indicates the resource manager reclaimed the media resource used by the codec.\n With this error, the codec must be released, as it has moved to terminal state."]
pub const media_status_t_AMEDIA_ERROR_BASE: media_status_t = -10000;
#[doc = " The called media function failed with an unknown error."]
pub const media_status_t_AMEDIA_ERROR_UNKNOWN: media_status_t = -10000;
#[doc = " The input media data is corrupt or incomplete."]
pub const media_status_t_AMEDIA_ERROR_MALFORMED: media_status_t = -10001;
#[doc = " The required operation or media formats are not supported."]
pub const media_status_t_AMEDIA_ERROR_UNSUPPORTED: media_status_t = -10002;
#[doc = " An invalid (or already closed) object is used in the function call."]
pub const media_status_t_AMEDIA_ERROR_INVALID_OBJECT: media_status_t = -10003;
#[doc = " At least one of the invalid parameters is used."]
pub const media_status_t_AMEDIA_ERROR_INVALID_PARAMETER: media_status_t = -10004;
#[doc = " The media object is not in the right state for the required operation."]
pub const media_status_t_AMEDIA_ERROR_INVALID_OPERATION: media_status_t = -10005;
#[doc = " Media stream ends while processing the requested operation."]
pub const media_status_t_AMEDIA_ERROR_END_OF_STREAM: media_status_t = -10006;
#[doc = " An Error occurred when the Media object is carrying IO operation."]
pub const media_status_t_AMEDIA_ERROR_IO: media_status_t = -10007;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_ERROR_WOULD_BLOCK: media_status_t = -10008;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_DRM_ERROR_BASE: media_status_t = -20000;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_DRM_NOT_PROVISIONED: media_status_t = -20001;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_DRM_RESOURCE_BUSY: media_status_t = -20002;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_DRM_DEVICE_REVOKED: media_status_t = -20003;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_DRM_SHORT_BUFFER: media_status_t = -20004;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_DRM_SESSION_NOT_OPENED: media_status_t = -20005;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_DRM_TAMPER_DETECTED: media_status_t = -20006;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_DRM_VERIFY_FAILED: media_status_t = -20007;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_DRM_NEED_KEY: media_status_t = -20008;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_DRM_LICENSE_EXPIRED: media_status_t = -20009;
#[doc = " The required operation would have to be blocked (on I/O or others),\n   but blocking is not enabled."]
pub const media_status_t_AMEDIA_IMGREADER_ERROR_BASE: media_status_t = -30000;
#[doc = " There are no more image buffers to read/write image data."]
pub const media_status_t_AMEDIA_IMGREADER_NO_BUFFER_AVAILABLE: media_status_t = -30001;
#[doc = " The AImage object has used up the allowed maximum image buffers."]
pub const media_status_t_AMEDIA_IMGREADER_MAX_IMAGES_ACQUIRED: media_status_t = -30002;
#[doc = " The required image buffer could not be locked to read."]
pub const media_status_t_AMEDIA_IMGREADER_CANNOT_LOCK_IMAGE: media_status_t = -30003;
#[doc = " The media data or buffer could not be unlocked."]
pub const media_status_t_AMEDIA_IMGREADER_CANNOT_UNLOCK_IMAGE: media_status_t = -30004;
#[doc = " The media/buffer needs to be locked to perform the required operation."]
pub const media_status_t_AMEDIA_IMGREADER_IMAGE_NOT_LOCKED: media_status_t = -30005;
#[doc = " Media error message types returned from NDK media functions."]
pub type media_status_t = ::std::os::raw::c_int;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AImage {
    _unused: [u8; 0],
}
#[doc = " 32 bits RGBA format, 8 bits for each of the four channels.\n\n <p>\n Corresponding formats:\n <ul>\n <li>AHardwareBuffer: AHARDWAREBUFFER_FORMAT_R8G8B8A8_UNORM</li>\n <li>Vulkan: VK_FORMAT_R8G8B8A8_UNORM</li>\n <li>OpenGL ES: GL_RGBA8</li>\n </ul>\n </p>\n\n @see AImage\n @see AImageReader\n @see AHardwareBuffer"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_RGBA_8888: AIMAGE_FORMATS = 1;
#[doc = " 32 bits RGBX format, 8 bits for each of the four channels.  The values\n of the alpha channel bits are ignored (image is assumed to be opaque).\n\n <p>\n Corresponding formats:\n <ul>\n <li>AHardwareBuffer: AHARDWAREBUFFER_FORMAT_R8G8B8X8_UNORM</li>\n <li>Vulkan: VK_FORMAT_R8G8B8A8_UNORM</li>\n <li>OpenGL ES: GL_RGB8</li>\n </ul>\n </p>\n\n @see AImage\n @see AImageReader\n @see AHardwareBuffer"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_RGBX_8888: AIMAGE_FORMATS = 2;
#[doc = " 24 bits RGB format, 8 bits for each of the three channels.\n\n <p>\n Corresponding formats:\n <ul>\n <li>AHardwareBuffer: AHARDWAREBUFFER_FORMAT_R8G8B8_UNORM</li>\n <li>Vulkan: VK_FORMAT_R8G8B8_UNORM</li>\n <li>OpenGL ES: GL_RGB8</li>\n </ul>\n </p>\n\n @see AImage\n @see AImageReader\n @see AHardwareBuffer"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_RGB_888: AIMAGE_FORMATS = 3;
#[doc = " 16 bits RGB format, 5 bits for Red channel, 6 bits for Green channel,\n and 5 bits for Blue channel.\n\n <p>\n Corresponding formats:\n <ul>\n <li>AHardwareBuffer: AHARDWAREBUFFER_FORMAT_R5G6B5_UNORM</li>\n <li>Vulkan: VK_FORMAT_R5G6B5_UNORM_PACK16</li>\n <li>OpenGL ES: GL_RGB565</li>\n </ul>\n </p>\n\n @see AImage\n @see AImageReader\n @see AHardwareBuffer"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_RGB_565: AIMAGE_FORMATS = 4;
#[doc = " 64 bits RGBA format, 16 bits for each of the four channels.\n\n <p>\n Corresponding formats:\n <ul>\n <li>AHardwareBuffer: AHARDWAREBUFFER_FORMAT_R16G16B16A16_FLOAT</li>\n <li>Vulkan: VK_FORMAT_R16G16B16A16_SFLOAT</li>\n <li>OpenGL ES: GL_RGBA16F</li>\n </ul>\n </p>\n\n @see AImage\n @see AImageReader\n @see AHardwareBuffer"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_RGBA_FP16: AIMAGE_FORMATS = 22;
#[doc = " Multi-plane Android YUV 420 format.\n\n <p>This format is a generic YCbCr format, capable of describing any 4:2:0\n chroma-subsampled planar or semiplanar buffer (but not fully interleaved),\n with 8 bits per color sample.</p>\n\n <p>Images in this format are always represented by three separate buffers\n of data, one for each color plane. Additional information always\n accompanies the buffers, describing the row stride and the pixel stride\n for each plane.</p>\n\n <p>The order of planes is guaranteed such that plane #0 is always Y, plane #1 is always\n U (Cb), and plane #2 is always V (Cr).</p>\n\n <p>The Y-plane is guaranteed not to be interleaved with the U/V planes\n (in particular, pixel stride is always 1 in {@link AImage_getPlanePixelStride}).</p>\n\n <p>The U/V planes are guaranteed to have the same row stride and pixel stride, that is, the\n return value of {@link AImage_getPlaneRowStride} for the U/V plane are guaranteed to be the\n same, and the return value of {@link AImage_getPlanePixelStride} for the U/V plane are also\n guaranteed to be the same.</p>\n\n <p>For example, the {@link AImage} object can provide data\n in this format from a {@link ACameraDevice} through an {@link AImageReader} object.</p>\n\n <p>This format is always supported as an output format for the android Camera2 NDK API.</p>\n\n @see AImage\n @see AImageReader\n @see ACameraDevice"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_YUV_420_888: AIMAGE_FORMATS = 35;
#[doc = " Compressed JPEG format.\n\n <p>This format is always supported as an output format for the android Camera2 NDK API.</p>"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_JPEG: AIMAGE_FORMATS = 256;
#[doc = " 16 bits per pixel raw camera sensor image format, usually representing a single-channel\n Bayer-mosaic image.\n\n <p>The layout of the color mosaic, the maximum and minimum encoding\n values of the raw pixel data, the color space of the image, and all other\n needed information to interpret a raw sensor image must be queried from\n the {@link ACameraDevice} which produced the image.</p>"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_RAW16: AIMAGE_FORMATS = 32;
#[doc = " Private raw camera sensor image format, a single channel image with implementation depedent\n pixel layout.\n\n <p>AIMAGE_FORMAT_RAW_PRIVATE is a format for unprocessed raw image buffers coming from an\n image sensor. The actual structure of buffers of this format is implementation-dependent.</p>\n"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_RAW_PRIVATE: AIMAGE_FORMATS = 36;
#[doc = " Android 10-bit raw format.\n\n <p>\n This is a single-plane, 10-bit per pixel, densely packed (in each row),\n unprocessed format, usually representing raw Bayer-pattern images coming\n from an image sensor.\n </p>\n <p>\n In an image buffer with this format, starting from the first pixel of\n each row, each 4 consecutive pixels are packed into 5 bytes (40 bits).\n Each one of the first 4 bytes contains the top 8 bits of each pixel, The\n fifth byte contains the 2 least significant bits of the 4 pixels, the\n exact layout data for each 4 consecutive pixels is illustrated below\n (Pi[j] stands for the jth bit of the ith pixel):\n </p>\n <table>\n <tr>\n <th align=\"center\"></th>\n <th align=\"center\">bit 7</th>\n <th align=\"center\">bit 6</th>\n <th align=\"center\">bit 5</th>\n <th align=\"center\">bit 4</th>\n <th align=\"center\">bit 3</th>\n <th align=\"center\">bit 2</th>\n <th align=\"center\">bit 1</th>\n <th align=\"center\">bit 0</th>\n </tr>\n <tr>\n <td align=\"center\">Byte 0:</td>\n <td align=\"center\">P0[9]</td>\n <td align=\"center\">P0[8]</td>\n <td align=\"center\">P0[7]</td>\n <td align=\"center\">P0[6]</td>\n <td align=\"center\">P0[5]</td>\n <td align=\"center\">P0[4]</td>\n <td align=\"center\">P0[3]</td>\n <td align=\"center\">P0[2]</td>\n </tr>\n <tr>\n <td align=\"center\">Byte 1:</td>\n <td align=\"center\">P1[9]</td>\n <td align=\"center\">P1[8]</td>\n <td align=\"center\">P1[7]</td>\n <td align=\"center\">P1[6]</td>\n <td align=\"center\">P1[5]</td>\n <td align=\"center\">P1[4]</td>\n <td align=\"center\">P1[3]</td>\n <td align=\"center\">P1[2]</td>\n </tr>\n <tr>\n <td align=\"center\">Byte 2:</td>\n <td align=\"center\">P2[9]</td>\n <td align=\"center\">P2[8]</td>\n <td align=\"center\">P2[7]</td>\n <td align=\"center\">P2[6]</td>\n <td align=\"center\">P2[5]</td>\n <td align=\"center\">P2[4]</td>\n <td align=\"center\">P2[3]</td>\n <td align=\"center\">P2[2]</td>\n </tr>\n <tr>\n <td align=\"center\">Byte 3:</td>\n <td align=\"center\">P3[9]</td>\n <td align=\"center\">P3[8]</td>\n <td align=\"center\">P3[7]</td>\n <td align=\"center\">P3[6]</td>\n <td align=\"center\">P3[5]</td>\n <td align=\"center\">P3[4]</td>\n <td align=\"center\">P3[3]</td>\n <td align=\"center\">P3[2]</td>\n </tr>\n <tr>\n <td align=\"center\">Byte 4:</td>\n <td align=\"center\">P3[1]</td>\n <td align=\"center\">P3[0]</td>\n <td align=\"center\">P2[1]</td>\n <td align=\"center\">P2[0]</td>\n <td align=\"center\">P1[1]</td>\n <td align=\"center\">P1[0]</td>\n <td align=\"center\">P0[1]</td>\n <td align=\"center\">P0[0]</td>\n </tr>\n </table>\n <p>\n This format assumes\n <ul>\n <li>a width multiple of 4 pixels</li>\n <li>an even height</li>\n </ul>\n </p>\n\n <pre>size = row stride * height</pre> where the row stride is in <em>bytes</em>,\n not pixels.\n\n <p>\n Since this is a densely packed format, the pixel stride is always 0. The\n application must use the pixel data layout defined in above table to\n access each row data. When row stride is equal to (width * (10 / 8)), there\n will be no padding bytes at the end of each row, the entire image data is\n densely packed. When stride is larger than (width * (10 / 8)), padding\n bytes will be present at the end of each row.\n </p>\n <p>\n For example, the {@link AImage} object can provide data in this format from a\n {@link ACameraDevice} (if supported) through a {@link AImageReader} object.\n The number of planes returned by {@link AImage_getNumberOfPlanes} will always be 1.\n The pixel stride is undefined ({@link AImage_getPlanePixelStride} will return\n {@link AMEDIA_ERROR_UNSUPPORTED}), and the {@link AImage_getPlaneRowStride} described the\n vertical neighboring pixel distance (in bytes) between adjacent rows.\n </p>\n\n @see AImage\n @see AImageReader\n @see ACameraDevice"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_RAW10: AIMAGE_FORMATS = 37;
#[doc = " Android 12-bit raw format.\n\n <p>\n This is a single-plane, 12-bit per pixel, densely packed (in each row),\n unprocessed format, usually representing raw Bayer-pattern images coming\n from an image sensor.\n </p>\n <p>\n In an image buffer with this format, starting from the first pixel of each\n row, each two consecutive pixels are packed into 3 bytes (24 bits). The first\n and second byte contains the top 8 bits of first and second pixel. The third\n byte contains the 4 least significant bits of the two pixels, the exact layout\n data for each two consecutive pixels is illustrated below (Pi[j] stands for\n the jth bit of the ith pixel):\n </p>\n <table>\n <tr>\n <th align=\"center\"></th>\n <th align=\"center\">bit 7</th>\n <th align=\"center\">bit 6</th>\n <th align=\"center\">bit 5</th>\n <th align=\"center\">bit 4</th>\n <th align=\"center\">bit 3</th>\n <th align=\"center\">bit 2</th>\n <th align=\"center\">bit 1</th>\n <th align=\"center\">bit 0</th>\n </tr>\n <tr>\n <td align=\"center\">Byte 0:</td>\n <td align=\"center\">P0[11]</td>\n <td align=\"center\">P0[10]</td>\n <td align=\"center\">P0[ 9]</td>\n <td align=\"center\">P0[ 8]</td>\n <td align=\"center\">P0[ 7]</td>\n <td align=\"center\">P0[ 6]</td>\n <td align=\"center\">P0[ 5]</td>\n <td align=\"center\">P0[ 4]</td>\n </tr>\n <tr>\n <td align=\"center\">Byte 1:</td>\n <td align=\"center\">P1[11]</td>\n <td align=\"center\">P1[10]</td>\n <td align=\"center\">P1[ 9]</td>\n <td align=\"center\">P1[ 8]</td>\n <td align=\"center\">P1[ 7]</td>\n <td align=\"center\">P1[ 6]</td>\n <td align=\"center\">P1[ 5]</td>\n <td align=\"center\">P1[ 4]</td>\n </tr>\n <tr>\n <td align=\"center\">Byte 2:</td>\n <td align=\"center\">P1[ 3]</td>\n <td align=\"center\">P1[ 2]</td>\n <td align=\"center\">P1[ 1]</td>\n <td align=\"center\">P1[ 0]</td>\n <td align=\"center\">P0[ 3]</td>\n <td align=\"center\">P0[ 2]</td>\n <td align=\"center\">P0[ 1]</td>\n <td align=\"center\">P0[ 0]</td>\n </tr>\n </table>\n <p>\n This format assumes\n <ul>\n <li>a width multiple of 4 pixels</li>\n <li>an even height</li>\n </ul>\n </p>\n\n <pre>size = row stride * height</pre> where the row stride is in <em>bytes</em>,\n not pixels.\n\n <p>\n Since this is a densely packed format, the pixel stride is always 0. The\n application must use the pixel data layout defined in above table to\n access each row data. When row stride is equal to (width * (12 / 8)), there\n will be no padding bytes at the end of each row, the entire image data is\n densely packed. When stride is larger than (width * (12 / 8)), padding\n bytes will be present at the end of each row.\n </p>\n <p>\n For example, the {@link AImage} object can provide data in this format from a\n {@link ACameraDevice} (if supported) through a {@link AImageReader} object.\n The number of planes returned by {@link AImage_getNumberOfPlanes} will always be 1.\n The pixel stride is undefined ({@link AImage_getPlanePixelStride} will return\n {@link AMEDIA_ERROR_UNSUPPORTED}), and the {@link AImage_getPlaneRowStride} described the\n vertical neighboring pixel distance (in bytes) between adjacent rows.\n </p>\n\n @see AImage\n @see AImageReader\n @see ACameraDevice"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_RAW12: AIMAGE_FORMATS = 38;
#[doc = " Android dense depth image format.\n\n <p>Each pixel is 16 bits, representing a depth ranging measurement from a depth camera or\n similar sensor. The 16-bit sample consists of a confidence value and the actual ranging\n measurement.</p>\n\n <p>The confidence value is an estimate of correctness for this sample.  It is encoded in the\n 3 most significant bits of the sample, with a value of 0 representing 100% confidence, a\n value of 1 representing 0% confidence, a value of 2 representing 1/7, a value of 3\n representing 2/7, and so on.</p>\n\n <p>As an example, the following sample extracts the range and confidence from the first pixel\n of a DEPTH16-format {@link AImage}, and converts the confidence to a floating-point value\n between 0 and 1.f inclusive, with 1.f representing maximum confidence:\n\n <pre>\n    uint16_t* data;\n    int dataLength;\n    AImage_getPlaneData(image, 0, (uint8_t**)&data, &dataLength);\n    uint16_t depthSample = data[0];\n    uint16_t depthRange = (depthSample & 0x1FFF);\n    uint16_t depthConfidence = ((depthSample >> 13) & 0x7);\n    float depthPercentage = depthConfidence == 0 ? 1.f : (depthConfidence - 1) / 7.f;\n </pre>\n </p>\n\n <p>This format assumes\n <ul>\n <li>an even width</li>\n <li>an even height</li>\n <li>a horizontal stride multiple of 16 pixels</li>\n </ul>\n </p>\n\n <pre> y_size = stride * height </pre>\n\n When produced by a camera, the units for the range are millimeters."]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_DEPTH16: AIMAGE_FORMATS = 1144402265;
#[doc = " Android sparse depth point cloud format.\n\n <p>A variable-length list of 3D points plus a confidence value, with each point represented\n by four floats; first the X, Y, Z position coordinates, and then the confidence value.</p>\n\n <p>The number of points is ((size of the buffer in bytes) / 16).\n\n <p>The coordinate system and units of the position values depend on the source of the point\n cloud data. The confidence value is between 0.f and 1.f, inclusive, with 0 representing 0%\n confidence and 1.f representing 100% confidence in the measured position values.</p>\n\n <p>As an example, the following code extracts the first depth point in a DEPTH_POINT_CLOUD\n format {@link AImage}:\n <pre>\n    float* data;\n    int dataLength;\n    AImage_getPlaneData(image, 0, (uint8_t**)&data, &dataLength);\n    float x = data[0];\n    float y = data[1];\n    float z = data[2];\n    float confidence = data[3];\n </pre>\n"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_DEPTH_POINT_CLOUD: AIMAGE_FORMATS = 257;
#[doc = " Android private opaque image format.\n\n <p>The choices of the actual format and pixel data layout are entirely up to the\n device-specific and framework internal implementations, and may vary depending on use cases\n even for the same device. Also note that the contents of these buffers are not directly\n accessible to the application.</p>\n\n <p>When an {@link AImage} of this format is obtained from an {@link AImageReader} or\n {@link AImage_getNumberOfPlanes()} method will return zero.</p>"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_PRIVATE: AIMAGE_FORMATS = 34;
#[doc = " Android Y8 format.\n\n <p>Y8 is a planar format comprised of a WxH Y plane only, with each pixel\n being represented by 8 bits.</p>\n\n <p>This format assumes\n <ul>\n <li>an even width</li>\n <li>an even height</li>\n <li>a horizontal stride multiple of 16 pixels</li>\n </ul>\n </p>\n\n <pre> size = stride * height </pre>\n\n <p>For example, the {@link AImage} object can provide data\n in this format from a {@link ACameraDevice} (if supported) through a\n {@link AImageReader} object. The number of planes returned by\n {@link AImage_getNumberOfPlanes} will always be 1. The pixel stride returned by\n {@link AImage_getPlanePixelStride} will always be 1, and the\n {@link AImage_getPlaneRowStride} described the vertical neighboring pixel distance\n (in bytes) between adjacent rows.</p>\n"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_Y8: AIMAGE_FORMATS = 538982489;
#[doc = " Compressed HEIC format.\n\n <p>This format defines the HEIC brand of High Efficiency Image File\n Format as described in ISO/IEC 23008-12.</p>"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_HEIC: AIMAGE_FORMATS = 1212500294;
#[doc = " Depth augmented compressed JPEG format.\n\n <p>JPEG compressed main image along with XMP embedded depth metadata\n following ISO 16684-1:2011(E).</p>"]
pub const AIMAGE_FORMATS_AIMAGE_FORMAT_DEPTH_JPEG: AIMAGE_FORMATS = 1768253795;
#[doc = " AImage supported formats: AImageReader only guarantees the support for the formats\n listed here."]
pub type AIMAGE_FORMATS = ::std::os::raw::c_uint;
#[doc = " Data type describing an cropped rectangle returned by {@link AImage_getCropRect}.\n\n <p>Note that the right and bottom coordinates are exclusive, so the width of the rectangle is\n (right - left) and the height of the rectangle is (bottom - top).</p>"]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct AImageCropRect {
    pub left: i32,
    pub top: i32,
    pub right: i32,
    pub bottom: i32,
}
#[test]
fn bindgen_test_layout_AImageCropRect() {
    const UNINIT: ::std::mem::MaybeUninit<AImageCropRect> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<AImageCropRect>(),
        16usize,
        concat!("Size of: ", stringify!(AImageCropRect))
    );
    assert_eq!(
        ::std::mem::align_of::<AImageCropRect>(),
        4usize,
        concat!("Alignment of ", stringify!(AImageCropRect))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).left) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(AImageCropRect),
            "::",
            stringify!(left)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).top) as usize - ptr as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(AImageCropRect),
            "::",
            stringify!(top)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).right) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(AImageCropRect),
            "::",
            stringify!(right)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).bottom) as usize - ptr as usize },
        12usize,
        concat!(
            "Offset of field: ",
            stringify!(AImageCropRect),
            "::",
            stringify!(bottom)
        )
    );
}
extern "C" {
    #[doc = " Return the image back the the system and delete the AImage object from memory.\n\n <p>Do NOT use the image pointer after this method returns.\n Note that if the parent {@link AImageReader} is closed, all the {@link AImage} objects acquired\n from the parent reader will be returned to system. All AImage_* methods except this method will\n return {@link AMEDIA_ERROR_INVALID_OBJECT}. Application still needs to call this method on those\n {@link AImage} objects to fully delete the {@link AImage} object from memory.</p>\n\n Available since API level 24.\n\n @param image The {@link AImage} to be deleted."]
    pub fn AImage_delete(image: *mut AImage);
}
extern "C" {
    #[doc = " Query the width of the input {@link AImage}.\n\n Available since API level 24.\n\n @param image the {@link AImage} of interest.\n @param width the width of the image will be filled here if the method call succeeeds.\n\n @return <ul>\n         <li>{@link AMEDIA_OK} if the method call succeeds.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_PARAMETER} if image or width is NULL.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_OBJECT} if the {@link AImageReader} generated this\n                 image has been deleted.</li></ul>"]
    pub fn AImage_getWidth(image: *const AImage, width: *mut i32) -> media_status_t;
}
extern "C" {
    #[doc = " Query the height of the input {@link AImage}.\n\n Available since API level 24.\n\n @param image the {@link AImage} of interest.\n @param height the height of the image will be filled here if the method call succeeeds.\n\n @return <ul>\n         <li>{@link AMEDIA_OK} if the method call succeeds.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_PARAMETER} if image or height is NULL.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_OBJECT} if the {@link AImageReader} generated this\n                 image has been deleted.</li></ul>"]
    pub fn AImage_getHeight(image: *const AImage, height: *mut i32) -> media_status_t;
}
extern "C" {
    #[doc = " Query the format of the input {@link AImage}.\n\n <p>The format value will be one of AIMAGE_FORMAT_* enum value.</p>\n\n Available since API level 24.\n\n @param image the {@link AImage} of interest.\n @param format the format of the image will be filled here if the method call succeeeds.\n\n @return <ul>\n         <li>{@link AMEDIA_OK} if the method call succeeds.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_PARAMETER} if image or format is NULL.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_OBJECT} if the {@link AImageReader} generated this\n                 image has been deleted.</li></ul>"]
    pub fn AImage_getFormat(image: *const AImage, format: *mut i32) -> media_status_t;
}
extern "C" {
    #[doc = " Query the cropped rectangle of the input {@link AImage}.\n\n <p>The crop rectangle specifies the region of valid pixels in the image, using coordinates in the\n largest-resolution plane.</p>\n\n Available since API level 24.\n\n @param image the {@link AImage} of interest.\n @param rect the cropped rectangle of the image will be filled here if the method call succeeeds.\n\n @return <ul>\n         <li>{@link AMEDIA_OK} if the method call succeeds.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_PARAMETER} if image or rect is NULL.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_OBJECT} if the {@link AImageReader} generated this\n                 image has been deleted.</li></ul>"]
    pub fn AImage_getCropRect(image: *const AImage, rect: *mut AImageCropRect) -> media_status_t;
}
extern "C" {
    #[doc = " Query the timestamp of the input {@link AImage}.\n\n <p>\n The timestamp is measured in nanoseconds, and is normally monotonically increasing. The\n timestamps for the images from different sources may have different timebases therefore may not\n be comparable. The specific meaning and timebase of the timestamp depend on the source providing\n images. For images generated by camera, the timestamp value will match\n {@link ACAMERA_SENSOR_TIMESTAMP} of the {@link ACameraMetadata} in\n {@link ACameraCaptureSession_captureCallbacks#onCaptureStarted} and\n {@link ACameraCaptureSession_captureCallbacks#onCaptureCompleted} callback.\n </p>\n\n Available since API level 24.\n\n @param image the {@link AImage} of interest.\n @param timestampNs the timestamp of the image will be filled here if the method call succeeeds.\n\n @return <ul>\n         <li>{@link AMEDIA_OK} if the method call succeeds.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_PARAMETER} if image or timestampNs is NULL.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_OBJECT} if the {@link AImageReader} generated this\n                 image has been deleted.</li></ul>"]
    pub fn AImage_getTimestamp(image: *const AImage, timestampNs: *mut i64) -> media_status_t;
}
extern "C" {
    #[doc = " Query the number of planes of the input {@link AImage}.\n\n <p>The number of plane of an {@link AImage} is determined by its format, which can be queried by\n {@link AImage_getFormat} method.</p>\n\n Available since API level 24.\n\n @param image the {@link AImage} of interest.\n @param numPlanes the number of planes of the image will be filled here if the method call\n         succeeeds.\n\n @return <ul>\n         <li>{@link AMEDIA_OK} if the method call succeeds.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_PARAMETER} if image or numPlanes is NULL.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_OBJECT} if the {@link AImageReader} generated this\n                 image has been deleted.</li></ul>"]
    pub fn AImage_getNumberOfPlanes(image: *const AImage, numPlanes: *mut i32) -> media_status_t;
}
extern "C" {
    #[doc = " Query the pixel stride of the input {@link AImage}.\n\n <p>This is the distance between two consecutive pixel values in a row of pixels. It may be\n larger than the size of a single pixel to account for interleaved image data or padded formats.\n Note that pixel stride is undefined for some formats such as {@link AIMAGE_FORMAT_RAW_PRIVATE},\n and calling this method on images of these formats will cause {@link AMEDIA_ERROR_UNSUPPORTED}\n being returned.\n For formats where pixel stride is well defined, the pixel stride is always greater than 0.</p>\n\n Available since API level 24.\n\n @param image the {@link AImage} of interest.\n @param planeIdx the index of the plane. Must be less than the number of planes of input image.\n @param pixelStride the pixel stride of the image will be filled here if the method call succeeeds.\n\n @return <ul>\n         <li>{@link AMEDIA_OK} if the method call succeeds.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_PARAMETER} if image or pixelStride is NULL, or planeIdx\n                 is out of the range of [0, numOfPlanes - 1].</li>\n         <li>{@link AMEDIA_ERROR_UNSUPPORTED} if pixel stride is undefined for the format of input\n                 image.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_OBJECT} if the {@link AImageReader} generated this\n                 image has been deleted.</li>\n         <li>{@link AMEDIA_IMGREADER_CANNOT_LOCK_IMAGE} if the {@link AImage} cannot be locked\n                 for CPU access.</li></ul>"]
    pub fn AImage_getPlanePixelStride(
        image: *const AImage,
        planeIdx: ::std::os::raw::c_int,
        pixelStride: *mut i32,
    ) -> media_status_t;
}
extern "C" {
    #[doc = " Query the row stride of the input {@link AImage}.\n\n <p>This is the distance between the start of two consecutive rows of pixels in the image. Note\n that row stried is undefined for some formats such as {@link AIMAGE_FORMAT_RAW_PRIVATE}, and\n calling this method on images of these formats will cause {@link AMEDIA_ERROR_UNSUPPORTED}\n being returned.\n For formats where row stride is well defined, the row stride is always greater than 0.</p>\n\n Available since API level 24.\n\n @param image the {@link AImage} of interest.\n @param planeIdx the index of the plane. Must be less than the number of planes of input image.\n @param rowStride the row stride of the image will be filled here if the method call succeeeds.\n\n @return <ul>\n         <li>{@link AMEDIA_OK} if the method call succeeds.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_PARAMETER} if image or rowStride is NULL, or planeIdx\n                 is out of the range of [0, numOfPlanes - 1].</li>\n         <li>{@link AMEDIA_ERROR_UNSUPPORTED} if row stride is undefined for the format of input\n                 image.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_OBJECT} if the {@link AImageReader} generated this\n                 image has been deleted.</li>\n         <li>{@link AMEDIA_IMGREADER_CANNOT_LOCK_IMAGE} if the {@link AImage} cannot be locked\n                 for CPU access.</li></ul>"]
    pub fn AImage_getPlaneRowStride(
        image: *const AImage,
        planeIdx: ::std::os::raw::c_int,
        rowStride: *mut i32,
    ) -> media_status_t;
}
extern "C" {
    #[doc = " Get the data pointer of the input image for direct application access.\n\n <p>Note that once the {@link AImage} or the parent {@link AImageReader} is deleted, the data\n pointer from previous AImage_getPlaneData call becomes invalid. Do NOT use it after the\n {@link AImage} or the parent {@link AImageReader} is deleted.</p>\n\n Available since API level 24.\n\n @param image the {@link AImage} of interest.\n @param planeIdx the index of the plane. Must be less than the number of planes of input image.\n @param data the data pointer of the image will be filled here if the method call succeeeds.\n @param dataLength the valid length of data will be filled here if the method call succeeeds.\n\n @return <ul>\n         <li>{@link AMEDIA_OK} if the method call succeeds.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_PARAMETER} if image, data or dataLength is NULL, or\n                 planeIdx is out of the range of [0, numOfPlanes - 1].</li>\n         <li>{@link AMEDIA_ERROR_INVALID_OBJECT} if the {@link AImageReader} generated this\n                 image has been deleted.</li>\n         <li>{@link AMEDIA_IMGREADER_CANNOT_LOCK_IMAGE} if the {@link AImage} cannot be locked\n                 for CPU access.</li></ul>"]
    pub fn AImage_getPlaneData(
        image: *const AImage,
        planeIdx: ::std::os::raw::c_int,
        data: *mut *mut u8,
        dataLength: *mut ::std::os::raw::c_int,
    ) -> media_status_t;
}
extern "C" {
    #[doc = " Return the image back the the system and delete the AImage object from memory asynchronously.\n\n <p>Similar to {@link AImage_delete}, do NOT use the image pointer after this method returns.\n However, the caller can still hold on to the {@link AHardwareBuffer} returned from this image and\n signal the release of the hardware buffer back to the {@link AImageReader}'s queue using\n releaseFenceFd.</p>\n\n Available since API level 26.\n\n @param image The {@link AImage} to be deleted.\n @param releaseFenceFd A sync fence fd defined in {@link sync.h}, which signals the release of\n         underlying {@link AHardwareBuffer}.\n\n @see sync.h"]
    pub fn AImage_deleteAsync(image: *mut AImage, releaseFenceFd: ::std::os::raw::c_int);
}
extern "C" {
    #[doc = " Get the hardware buffer handle of the input image intended for GPU and/or hardware access.\n\n <p>Note that no reference on the returned {@link AHardwareBuffer} handle is acquired\n automatically. Once the {@link AImage} or the parent {@link AImageReader} is deleted, the\n {@link AHardwareBuffer} handle from previous {@link AImage_getHardwareBuffer} becomes\n invalid.</p>\n\n <p>If the caller ever needs to hold on a reference to the {@link AHardwareBuffer} handle after\n the {@link AImage} or the parent {@link AImageReader} is deleted, it must call {@link\n AHardwareBuffer_acquire} to acquire an extra reference, and call {@link AHardwareBuffer_release}\n once it has finished using it in order to properly deallocate the underlying memory managed by\n {@link AHardwareBuffer}. If the caller has acquired extra reference on an {@link AHardwareBuffer}\n returned from this function, it must also register a listener using the function\n {@link AImageReader_setBufferRemovedListener} to be notified when the buffer is no longer used\n by {@link AImageReader}.</p>\n\n Available since API level 26.\n\n @param image the {@link AImage} of interest.\n @param buffer The memory area pointed to by buffer will contain the acquired AHardwareBuffer\n         handle.\n @return <ul>\n         <li>{@link AMEDIA_OK} if the method call succeeds.</li>\n         <li>{@link AMEDIA_ERROR_INVALID_PARAMETER} if image or buffer is NULL</li></ul>\n\n @see AImageReader_ImageCallback"]
    pub fn AImage_getHardwareBuffer(
        image: *const AImage,
        buffer: *mut *mut AHardwareBuffer,
    ) -> media_status_t;
}
pub type __builtin_va_list = [__va_list_tag; 1usize];
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct __va_list_tag {
    pub gp_offset: ::std::os::raw::c_uint,
    pub fp_offset: ::std::os::raw::c_uint,
    pub overflow_arg_area: *mut ::std::os::raw::c_void,
    pub reg_save_area: *mut ::std::os::raw::c_void,
}
#[test]
fn bindgen_test_layout___va_list_tag() {
    const UNINIT: ::std::mem::MaybeUninit<__va_list_tag> = ::std::mem::MaybeUninit::uninit();
    let ptr = UNINIT.as_ptr();
    assert_eq!(
        ::std::mem::size_of::<__va_list_tag>(),
        24usize,
        concat!("Size of: ", stringify!(__va_list_tag))
    );
    assert_eq!(
        ::std::mem::align_of::<__va_list_tag>(),
        8usize,
        concat!("Alignment of ", stringify!(__va_list_tag))
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).gp_offset) as usize - ptr as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(__va_list_tag),
            "::",
            stringify!(gp_offset)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).fp_offset) as usize - ptr as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(__va_list_tag),
            "::",
            stringify!(fp_offset)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).overflow_arg_area) as usize - ptr as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(__va_list_tag),
            "::",
            stringify!(overflow_arg_area)
        )
    );
    assert_eq!(
        unsafe { ::std::ptr::addr_of!((*ptr).reg_save_area) as usize - ptr as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(__va_list_tag),
            "::",
            stringify!(reg_save_area)
        )
    );
}
